{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple implementation of MNIST with TAP-SGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt \n",
    "from models.unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FFJORD things\n",
    "import lib.layers as layers\n",
    "import lib.utils as utils\n",
    "import lib.odenvp as odenvp\n",
    "import lib.multiscale_parallel as multiscale_parallel\n",
    "\n",
    "from train_misc import standard_normal_logprob\n",
    "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
    "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
    "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffjord_create_model(args, data_shape, regularization_fns):\n",
    "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
    "    strides = tuple(map(int, args.strides.split(\",\")))\n",
    "\n",
    "    if args.multiscale:\n",
    "        model = odenvp.ODENVP(\n",
    "            (args.batch_size, *data_shape),\n",
    "            n_blocks=args.num_blocks,\n",
    "            intermediate_dims=hidden_dims,\n",
    "            nonlinearity=args.nonlinearity,\n",
    "            alpha=args.alpha,\n",
    "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns},\n",
    "        )\n",
    "    elif args.parallel:\n",
    "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
    "            (args.batch_size, *data_shape),\n",
    "            n_blocks=args.num_blocks,\n",
    "            intermediate_dims=hidden_dims,\n",
    "            alpha=args.alpha,\n",
    "            time_length=args.time_length,\n",
    "        )\n",
    "    else:\n",
    "        if args.autoencode:\n",
    "\n",
    "            def build_cnf():\n",
    "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
    "                    hidden_dims=hidden_dims,\n",
    "                    input_shape=data_shape,\n",
    "                    strides=strides,\n",
    "                    conv=args.conv,\n",
    "                    layer_type=args.layer_type,\n",
    "                    nonlinearity=args.nonlinearity,\n",
    "                )\n",
    "                odefunc = layers.AutoencoderODEfunc(\n",
    "                    autoencoder_diffeq=autoencoder_diffeq,\n",
    "                    divergence_fn=args.divergence_fn,\n",
    "                    residual=args.residual,\n",
    "                    rademacher=args.rademacher,\n",
    "                )\n",
    "                cnf = layers.CNF(\n",
    "                    odefunc=odefunc,\n",
    "                    T=args.time_length,\n",
    "                    regularization_fns=regularization_fns,\n",
    "                    solver=args.solver,\n",
    "                )\n",
    "                return cnf\n",
    "        else:\n",
    "\n",
    "            def build_cnf():\n",
    "                diffeq = layers.ODEnet(\n",
    "                    hidden_dims=hidden_dims,\n",
    "                    input_shape=data_shape,\n",
    "                    strides=strides,\n",
    "                    conv=args.conv,\n",
    "                    layer_type=args.layer_type,\n",
    "                    nonlinearity=args.nonlinearity,\n",
    "                )\n",
    "                odefunc = layers.ODEfunc(\n",
    "                    diffeq=diffeq,\n",
    "                    divergence_fn=args.divergence_fn,\n",
    "                    residual=args.residual,\n",
    "                    rademacher=args.rademacher,\n",
    "                )\n",
    "                cnf = layers.CNF(\n",
    "                    odefunc=odefunc,\n",
    "                    T=args.time_length,\n",
    "                    train_T=args.train_T,\n",
    "                    regularization_fns=regularization_fns,\n",
    "                    solver=args.solver,\n",
    "                )\n",
    "                return cnf\n",
    "\n",
    "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
    "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
    "        if args.batch_norm:\n",
    "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
    "        model = layers.SequentialFlow(chain)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MNIST samples\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = torchvision.datasets.MNIST('mnist_data',train = True,download = True, transform = transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 4\n",
    "ncols = 4\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "i = 1\n",
    "for a in range(nrows):\n",
    "    for b in range(ncols):\n",
    "        fig.add_subplot(nrows,ncols,i)\n",
    "        plt.imshow(dataset[i][0][0],cmap = 'gray')\n",
    "        i = i+1\n",
    "        plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load preconditioning map\n",
    "\n",
    "inputchannels = 1\n",
    "inputheight = 28\n",
    "dimx = inputchannels * inputheight ** 2\n",
    "\n",
    "checkpoint = torch.load('ffjord_mnist_bad_checkpt.pth',map_location = torch.device('cpu'))\n",
    "preconditionerargs = checkpoint['args']\n",
    "\n",
    "data_shape = (inputchannels, inputheight, inputheight)\n",
    "\n",
    "regularization_fns, regularization_coeffs = create_regularization_fns(preconditionerargs)\n",
    "model = ffjord_create_model(preconditionerargs,data_shape,regularization_fns)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scorenet is a U-net, whatever that is. \n",
    "\n",
    "## FIGURE OUT SCORENET NEXT\n",
    "\n",
    "## THEN FIGURE OUT THE PRETRAINED THING\n",
    "\n",
    "##LOOK AT 2D MARGINALS OF THE BAD FFJORDMAP\n",
    "\n",
    "## Define scorenetwork \n",
    "\n",
    "inputchannels = 1\n",
    "inputheight = 28\n",
    "dimx = inputchannels * inputheight ** 2\n",
    "\n",
    "\n",
    "# scorenet = UNet(input_channels = inputchannels,\n",
    "#                 input_height = inputheight,\n",
    "#                 ch = 32,\n",
    "#                 ch_mult = (1,2,2),\n",
    "#                 num_res_blocks=2,\n",
    "#                 attn_resolutions=(16,),\n",
    "#                 resamp_with_conv=True,\n",
    "#                 )\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# scorenet = scorenet.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 2\n",
    "hidden_units = dimx * 2 \n",
    "\n",
    "\n",
    "\n",
    "def construct_score_model(depth,hidden_units):\n",
    "    chain = []\n",
    "    chain.append(nn.Linear(int(dimx)+1,int(hidden_units),bias =True))\n",
    "    chain.append(nn.GELU())\n",
    "\n",
    "    for ii in range(depth-1):\n",
    "        chain.append(nn.Linear(int(hidden_units),int(hidden_units),bias = True))\n",
    "        chain.append(nn.GELU())\n",
    "    chain.append(nn.Linear(int(hidden_units),dimx,bias = True))    \n",
    "\n",
    "    return nn.Sequential(*chain)\n",
    "\n",
    "\n",
    "scorenet =  construct_score_model(depth,hidden_units)\n",
    "scorenet = scorenet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the scorenet\n",
    "\n",
    "def calc_loss(score_network: torch.nn.Module, x: torch.Tensor,Tmin,Tmax,eps) -> torch.Tensor:\n",
    "    # x: (batch_size, nch) is the training data\n",
    "    \n",
    "    # sample the time\n",
    "    t = torch.rand([x.size(0) ]) * (Tmax-Tmin+eps) #+ [1 for _ in range(x.ndim - 1)]).to(x) * (Tmax-Tmin+eps)\n",
    "\n",
    "    # calculate the terms for the posterior log distribution\n",
    "\n",
    "    sigmas = torch.sqrt(1 - torch.exp(-t))\n",
    "    sigmas = sigmas.view(x.shape[0],*([1]*len(x.shape[1:])))\n",
    "    noise = torch.randn_like(x) * sigmas\n",
    "\n",
    "    tenlarge = t.repeat(784,1).T\n",
    "    perturbed_samples = x*torch.exp(-0.5 * tenlarge) + noise\n",
    "\n",
    "    target = - 1/(sigmas ** 2) * noise\n",
    "    score_eval_samples = torch.cat((t.reshape(-1,1),perturbed_samples),1)\n",
    "\n",
    "    scores = -perturbed_samples + score_network(score_eval_samples)\n",
    "\n",
    "    target = target.view(target.shape[0],-1)\n",
    "    scores = scores.view(scores.shape[0],-1)\n",
    "    loss = 0.5 * ((scores-target)**2).sum(dim = -1)\n",
    "\n",
    "    return loss.mean(dim = 0)\n",
    "\n",
    "    # int_beta = (0.1 + 0.5 * (20 - 0.1) * t) * t  # integral of beta\n",
    "    # mu_t = x * torch.exp(-0.5 * int_beta)\n",
    "    # var_t = -torch.expm1(-int_beta)\n",
    "    # x_t = torch.randn_like(x) * var_t ** 0.5 + mu_t\n",
    "    # grad_log_p = -(x_t - mu_t) / var_t  # (batch_size, nch)\n",
    "\n",
    "    # # calculate the score function\n",
    "    # score = score_network(x_t, t)  # score: (batch_size, nch)\n",
    "\n",
    "    # # calculate the loss function\n",
    "    # loss = (score - grad_log_p) ** 2\n",
    "    # lmbda_t = var_t\n",
    "    # weighted_loss = lmbda_t * loss\n",
    "    # return torch.mean(weighted_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.randn_like(data)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training\n",
    "\n",
    "loaded = torch.load('pushforward_samples')\n",
    "\n",
    "scorenet.train()\n",
    "opt = torch.optim.Adam(scorenet.parameters(),lr = 0.001)\n",
    "\n",
    "epochs = 100000\n",
    "for step in range(epochs):\n",
    "\n",
    "    opt.zero_grad()\n",
    "    randind = torch.randint(0,59999,(64,))\n",
    "    data = torch.tensor(loaded[randind,:,:,:])\n",
    "    data = data.reshape(data.shape[0],-1).to(device)\n",
    "    # training step\n",
    "    loss = calc_loss(scorenet, data,0,1,1e-4)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if not step%100:\n",
    "        print(loss,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ou_process(x,T):\n",
    "\n",
    "\n",
    "    sigmas = torch.sqrt(1 - torch.exp(-torch.tensor(T)))\n",
    "    noise = torch.randn_like(x) * sigmas\n",
    "\n",
    "    perturbed_samples = x*torch.exp(-0.5 * T) + noise\n",
    "    return perturbed_samples\n",
    "\n",
    "\n",
    "def generate_samples(score_network: torch.nn.Module, nsamples: int,loaded) -> torch.Tensor:\n",
    "    device = next(score_network.parameters()).device\n",
    "    x_t = torch.randn_like(loaded[0:16,:,:,:], device=device)  # (nsamples, nch)\n",
    "    time_pts = torch.linspace(2, 0, 1000, device=device)  # (ntime_pts,)\n",
    "    # beta = lambda t: 0.1 + (20 - 0.1) * t\n",
    "    for i in range(len(time_pts) - 1):\n",
    "        t = time_pts[i]\n",
    "        dt = time_pts[i + 1] - t\n",
    "\n",
    "        # calculate the drift and diffusion terms\n",
    "        fxt = -0.5  * x_t\n",
    "        gt = 1\n",
    "        score = -x_t + score_network(x_t, t.expand(x_t.shape[0], 1).squeeze()).detach()\n",
    "        drift = fxt - gt * gt * score\n",
    "        diffusion = gt\n",
    "\n",
    "        # euler-maruyama step\n",
    "        x_t = x_t - drift * dt + diffusion * torch.randn_like(x_t) * torch.abs(dt) ** 0.5\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = torch.load('pushforward_samples')\n",
    "loaded = loaded.detach().numpy()\n",
    "\n",
    "evolve = ou_process(torch.tensor(loaded),torch.tensor(2))\n",
    "\n",
    "\n",
    "plt.scatter(evolve[0:10000,0,9,1],evolve[0:10000,0,1,23],s = 0.2)\n",
    "plt.axis('square')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samps = generate_samples(scorenet_trained,2,torch.tensor(loaded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model(generated_samps[:,:,:,:],reverse = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_samps = generated_samps.detach().numpy()\n",
    "plt.scatter(preprocessed[:,0,4,1],preprocessed[:,0,2,2])\n",
    "plt.scatter(generated_samps[:,0,4,1],generated_samps[:,0,2,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(test[13,0,:,:].detach().numpy(),cmap = 'gray')\n",
    "\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "i = 0\n",
    "for a in range(nrows):\n",
    "    for b in range(ncols):\n",
    "        fig.add_subplot(nrows,ncols,i+1)\n",
    "        plt.imshow(test[i,0,:,:].detach().numpy(),cmap = 'gray')\n",
    "        i = i+1\n",
    "        plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(loaded )\n",
    "t = torch.rand([x.size(0), ] + [1 for _ in range(x.ndim - 1)]).to(x) * (2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = torch.load('pushforward_samples').detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(preprocessed[:,0,19,21],preprocessed[:,0,5,10],s = 0.1)\n",
    "plt.axis('square')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "particlesystems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcff6d82c76e93f8bf1a2a5a5959262afe2e0fbcc55797eb73e33c922868f9ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
