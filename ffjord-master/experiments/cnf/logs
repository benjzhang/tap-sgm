/Users/bjzhang/Dropbox (Personal)/icml2023_project/code/ffjord-master/train_discrete_toy.py
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import argparse
import os
import time

import torch
import torch.optim as optim

import lib.layers as layers
import lib.toy_data as toy_data
import lib.utils as utils
from lib.visualize_flow import visualize_transform

from train_misc import standard_normal_logprob
from train_misc import count_parameters

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'fixed_adams']
parser = argparse.ArgumentParser('Continuous Normalizing Flow')
parser.add_argument(
    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='pinwheel'
)

parser.add_argument('--depth', help='number of coupling layers', type=int, default=10)
parser.add_argument('--glow', type=eval, choices=[True, False], default=False)
parser.add_argument('--nf', type=eval, choices=[True, False], default=False)

parser.add_argument('--niters', type=int, default=100001)
parser.add_argument('--batch_size', type=int, default=100)
parser.add_argument('--test_batch_size', type=int, default=1000)
parser.add_argument('--lr', type=float, default=1e-4)
parser.add_argument('--weight_decay', type=float, default=0)

# Track quantities
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument('--save', type=str, default='experiments/cnf')
parser.add_argument('--viz_freq', type=int, default=1000)
parser.add_argument('--val_freq', type=int, default=1000)
parser.add_argument('--log_freq', type=int, default=100)
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')


def construct_model():

    if args.nf:
        chain = []
        for i in range(args.depth):
            chain.append(layers.PlanarFlow(2))
        return layers.SequentialFlow(chain)
    else:
        chain = []
        for i in range(args.depth):
            if args.glow: chain.append(layers.BruteForceLayer(2))
            chain.append(layers.CouplingLayer(2, swap=i % 2 == 0))
        return layers.SequentialFlow(chain)


def get_transforms(model):

    if args.nf:
        sample_fn = None
    else:

        def sample_fn(z, logpz=None):
            if logpz is not None:
                return model(z, logpz, reverse=True)
            else:
                return model(z, reverse=True)

    def density_fn(x, logpx=None):
        if logpx is not None:
            return model(x, logpx, reverse=False)
        else:
            return model(x, reverse=False)

    return sample_fn, density_fn


def compute_loss(args, model, batch_size=None):
    if batch_size is None: batch_size = args.batch_size

    # load data
    x = toy_data.inf_train_gen(args.data, batch_size=batch_size)
    x = torch.from_numpy(x).type(torch.float32).to(device)
    zero = torch.zeros(x.shape[0], 1).to(x)

    # transform to z
    z, delta_logp = model(x, zero)

    # compute log q(z)
    logpz = standard_normal_logprob(z).sum(1, keepdim=True)

    logpx = logpz - delta_logp
    loss = -torch.mean(logpx)
    return loss


if __name__ == '__main__':

    model = construct_model().to(device)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    optimizer = optim.Adamax(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    time_meter = utils.RunningAverageMeter(0.98)
    loss_meter = utils.RunningAverageMeter(0.98)

    end = time.time()
    best_loss = float('inf')
    model.train()
    for itr in range(1, args.niters + 1):
        optimizer.zero_grad()

        loss = compute_loss(args, model)
        loss_meter.update(loss.item())

        loss.backward()
        optimizer.step()

        time_meter.update(time.time() - end)

        if itr % args.log_freq == 0:
            log_message = (
                'Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f})'.format(
                    itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg
                )
            )
            logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                model.eval()
                test_loss = compute_loss(args, model, batch_size=args.test_batch_size)
                log_message = '[TEST] Iter {:04d} | Test Loss {:.6f}'.format(itr, test_loss)
                logger.info(log_message)

                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    utils.makedirs(args.save)
                    torch.save({
                        'args': args,
                        'state_dict': model.state_dict(),
                    }, os.path.join(args.save, 'checkpt.pth'))
                model.train()

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                model.eval()
                p_samples = toy_data.inf_train_gen(args.data, batch_size=2000)

                sample_fn, density_fn = get_transforms(model)

                plt.figure(figsize=(9, 3))
                visualize_transform(
                    p_samples, torch.randn, standard_normal_logprob, transform=sample_fn, inverse_transform=density_fn,
                    samples=True, npts=800, device=device
                )
                fig_filename = os.path.join(args.save, 'figs', '{:04d}.jpg'.format(itr))
                utils.makedirs(os.path.dirname(fig_filename))
                plt.savefig(fig_filename)
                plt.close()
                model.train()

        end = time.time()

    logger.info('Training has finished.')

Namespace(data='8gaussians', depth=10, glow=True, nf=False, niters=100001, batch_size=100, test_batch_size=1000, lr=0.0001, weight_decay=0, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='experiments/cnf', viz_freq=1000, val_freq=1000, log_freq=100, gpu=0)
SequentialFlow(
  (chain): ModuleList(
    (0): BruteForceLayer()
    (1): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (2): BruteForceLayer()
    (3): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (4): BruteForceLayer()
    (5): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (6): BruteForceLayer()
    (7): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (8): BruteForceLayer()
    (9): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (10): BruteForceLayer()
    (11): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (12): BruteForceLayer()
    (13): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (14): BruteForceLayer()
    (15): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (16): BruteForceLayer()
    (17): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (18): BruteForceLayer()
    (19): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
  )
)
Number of trainable parameters: 44220
Iter 0100 | Time 0.0145(0.0313) | Loss 4.252824(4.248180)
Iter 0200 | Time 0.0141(0.0178) | Loss 4.181940(4.200923)
Iter 0300 | Time 0.0157(0.0147) | Loss 4.160287(4.169384)
Iter 0400 | Time 0.0145(0.0145) | Loss 4.134821(4.140947)
Iter 0500 | Time 0.0157(0.0172) | Loss 4.100995(4.117517)
Iter 0600 | Time 0.0132(0.0177) | Loss 4.086251(4.088128)
Iter 0700 | Time 0.0185(0.0168) | Loss 4.061831(4.066441)
Iter 0800 | Time 0.0132(0.0147) | Loss 4.036528(4.040595)
Iter 0900 | Time 0.0130(0.0142) | Loss 4.006174(4.008577)
Iter 1000 | Time 0.0132(0.0143) | Loss 3.958692(3.977778)
[TEST] Iter 1000 | Test Loss 3.971936
Iter 1100 | Time 0.0157(0.0253) | Loss 3.927547(3.944851)
Iter 1200 | Time 0.0156(0.0220) | Loss 3.906774(3.909724)
Iter 1300 | Time 0.0139(0.0179) | Loss 3.823327(3.872668)
Iter 1400 | Time 0.0157(0.0268) | Loss 3.857170(3.842147)
Iter 1500 | Time 0.0217(0.0250) | Loss 3.751931(3.803449)
Iter 1600 | Time 0.0266(0.0211) | Loss 3.788106(3.775714)
Iter 1700 | Time 0.0161(0.0207) | Loss 3.739270(3.738989)
Iter 1800 | Time 0.0223(0.0206) | Loss 3.655657(3.705780)
Iter 1900 | Time 0.0134(0.0204) | Loss 3.639678(3.669242)
Iter 2000 | Time 0.0155(0.0162) | Loss 3.602762(3.634780)
[TEST] Iter 2000 | Test Loss 3.602671
Iter 2100 | Time 0.0174(0.0218) | Loss 3.599586(3.604947)
Iter 2200 | Time 0.0177(0.0178) | Loss 3.524652(3.572239)
Iter 2300 | Time 0.0142(0.0172) | Loss 3.568583(3.541983)
Iter 2400 | Time 0.0147(0.0162) | Loss 3.436161(3.509734)
Iter 2500 | Time 0.0187(0.0165) | Loss 3.449093(3.482565)
Iter 2600 | Time 0.0233(0.0160) | Loss 3.441599(3.465185)
Iter 2700 | Time 0.0145(0.0154) | Loss 3.523856(3.446321)
Iter 2800 | Time 0.0151(0.0149) | Loss 3.350865(3.426391)
Iter 2900 | Time 0.0138(0.0152) | Loss 3.383886(3.404348)
Iter 3000 | Time 0.0144(0.0147) | Loss 3.322025(3.378746)
[TEST] Iter 3000 | Test Loss 3.352461
Iter 3100 | Time 0.0146(0.0144) | Loss 3.337513(3.369348)
Iter 3200 | Time 0.0136(0.0142) | Loss 3.404921(3.349895)
Iter 3300 | Time 0.0142(0.0147) | Loss 3.295879(3.336485)
Iter 3400 | Time 0.0145(0.0144) | Loss 3.182040(3.315464)
Iter 3500 | Time 0.0147(0.0144) | Loss 3.217549(3.303546)
Iter 3600 | Time 0.0123(0.0147) | Loss 3.244813(3.304482)
Iter 3700 | Time 0.0146(0.0145) | Loss 3.290303(3.289396)
Iter 3800 | Time 0.0167(0.0142) | Loss 3.340685(3.269992)
Iter 3900 | Time 0.0134(0.0160) | Loss 3.355787(3.267238)
Iter 4000 | Time 0.0145(0.0157) | Loss 3.305679(3.265400)
[TEST] Iter 4000 | Test Loss 3.223599
Iter 4100 | Time 0.0132(0.0155) | Loss 3.343773(3.250542)
Iter 4200 | Time 0.0179(0.0158) | Loss 3.229124(3.231982)
Iter 4300 | Time 0.0166(0.0169) | Loss 3.278635(3.228575)
Iter 4400 | Time 0.0139(0.0176) | Loss 3.319071(3.236722)
Iter 4500 | Time 0.0129(0.0199) | Loss 3.175461(3.221813)
Iter 4600 | Time 0.0144(0.0160) | Loss 3.384193(3.224217)
Iter 4700 | Time 0.0142(0.0145) | Loss 3.173496(3.220462)
Iter 4800 | Time 0.0133(0.0164) | Loss 3.200171(3.200194)
Iter 4900 | Time 0.0150(0.0168) | Loss 3.173581(3.180237)
Iter 5000 | Time 0.0144(0.0155) | Loss 3.301547(3.188190)
[TEST] Iter 5000 | Test Loss 3.240261
Iter 5100 | Time 0.0148(0.0148) | Loss 3.171021(3.199618)
Iter 5200 | Time 0.0175(0.0157) | Loss 3.154400(3.169672)
Iter 5300 | Time 0.0136(0.0164) | Loss 3.142489(3.171982)
Iter 5400 | Time 0.0136(0.0146) | Loss 3.177855(3.176158)
Iter 5500 | Time 0.0121(0.0148) | Loss 3.113980(3.155879)
Iter 5600 | Time 0.0134(0.0162) | Loss 3.090590(3.164282)
Iter 5700 | Time 0.0148(0.0157) | Loss 3.210772(3.169369)
Iter 5800 | Time 0.0134(0.0174) | Loss 3.185100(3.152676)
Iter 5900 | Time 0.0192(0.0234) | Loss 3.226079(3.147579)
Iter 6000 | Time 0.0150(0.0198) | Loss 3.145329(3.146238)
[TEST] Iter 6000 | Test Loss 3.134835
Iter 6100 | Time 0.0134(0.0182) | Loss 3.018651(3.153821)
Iter 6200 | Time 0.0138(0.0177) | Loss 3.108890(3.141321)
Iter 6300 | Time 0.0191(0.0212) | Loss 3.209109(3.151689)
Iter 6400 | Time 0.0270(0.0207) | Loss 2.990521(3.151083)
Iter 6500 | Time 0.0330(0.0203) | Loss 3.289956(3.148787)
Iter 6600 | Time 0.0133(0.0311) | Loss 3.118570(3.129902)
Iter 6700 | Time 0.0282(0.0360) | Loss 3.070171(3.122296)
Iter 6800 | Time 0.0292(0.0269) | Loss 3.121192(3.127358)
Iter 6900 | Time 0.0156(0.0295) | Loss 3.012558(3.123877)
Iter 7000 | Time 0.0216(0.0255) | Loss 3.144315(3.122743)
[TEST] Iter 7000 | Test Loss 3.133906
Iter 7100 | Time 0.0379(0.0225) | Loss 3.256103(3.132779)
Iter 7200 | Time 0.0186(0.0240) | Loss 3.016099(3.118241)
Iter 7300 | Time 0.0318(0.0332) | Loss 3.058696(3.117227)
Iter 7400 | Time 0.0360(0.0346) | Loss 3.069686(3.107556)
Iter 7500 | Time 0.0380(0.0357) | Loss 3.051038(3.106960)
Iter 7600 | Time 0.0281(0.0364) | Loss 2.993936(3.112415)
Iter 7700 | Time 0.0292(0.0331) | Loss 3.154880(3.094492)
Iter 7800 | Time 0.0151(0.0272) | Loss 3.025192(3.099563)
Iter 7900 | Time 0.0351(0.0288) | Loss 3.059645(3.099043)
Iter 8000 | Time 0.0342(0.0286) | Loss 3.050090(3.099146)
[TEST] Iter 8000 | Test Loss 3.038982
Iter 8100 | Time 0.0163(0.0181) | Loss 3.148585(3.094710)
Iter 8200 | Time 0.0346(0.0354) | Loss 2.955593(3.094093)
Iter 8300 | Time 0.0241(0.0349) | Loss 3.175521(3.101842)
Iter 8400 | Time 0.0451(0.0359) | Loss 3.023748(3.093583)
Iter 8500 | Time 0.0225(0.0286) | Loss 3.185827(3.096634)
Iter 8600 | Time 0.0318(0.0319) | Loss 3.150861(3.099158)
Iter 8700 | Time 0.0466(0.0304) | Loss 3.002649(3.083766)
Iter 8800 | Time 0.0192(0.0317) | Loss 3.044245(3.069967)
Iter 8900 | Time 0.0224(0.0196) | Loss 3.138337(3.079439)
Iter 9000 | Time 0.0237(0.0231) | Loss 3.014544(3.084102)
[TEST] Iter 9000 | Test Loss 3.050989
Iter 9100 | Time 0.0166(0.0275) | Loss 3.023470(3.083309)
Iter 9200 | Time 0.0201(0.0227) | Loss 2.927360(3.073935)
Iter 9300 | Time 0.0229(0.0225) | Loss 3.019462(3.089401)
Iter 9400 | Time 0.0422(0.0227) | Loss 2.992757(3.078523)
Iter 9500 | Time 0.0225(0.0249) | Loss 3.032935(3.061773)
Iter 9600 | Time 0.0210(0.0242) | Loss 3.146694(3.085779)
Iter 9700 | Time 0.0297(0.0223) | Loss 2.929006(3.077016)
Iter 9800 | Time 0.0182(0.0241) | Loss 3.074185(3.075886)
Iter 9900 | Time 0.0253(0.0234) | Loss 3.248491(3.081998)
Iter 10000 | Time 0.0248(0.0233) | Loss 3.190454(3.065886)
[TEST] Iter 10000 | Test Loss 2.980799
Iter 10100 | Time 0.0117(0.0162) | Loss 3.086148(3.075997)
Iter 10200 | Time 0.0148(0.0160) | Loss 3.234281(3.068835)
Iter 10300 | Time 0.0142(0.0150) | Loss 3.011755(3.051460)
Iter 10400 | Time 0.0125(0.0153) | Loss 3.078754(3.072336)
Iter 10500 | Time 0.0131(0.0174) | Loss 2.997368(3.061613)
Iter 10600 | Time 0.0160(0.0156) | Loss 3.040380(3.068130)
Iter 10700 | Time 0.0383(0.0161) | Loss 3.191094(3.068643)
Iter 10800 | Time 0.0165(0.0155) | Loss 3.029803(3.069458)
Iter 10900 | Time 0.0142(0.0180) | Loss 2.996211(3.073167)
Iter 11000 | Time 0.0137(0.0152) | Loss 2.887979(3.056104)
[TEST] Iter 11000 | Test Loss 3.067110
Iter 11100 | Time 0.0156(0.0333) | Loss 2.937519(3.070751)
Iter 11200 | Time 0.0148(0.0228) | Loss 3.087721(3.065723)
Iter 11300 | Time 0.0161(0.0224) | Loss 3.018423(3.050949)
Iter 11400 | Time 0.0285(0.0191) | Loss 3.089771(3.054317)
Iter 11500 | Time 0.0143(0.0196) | Loss 3.195477(3.060554)
Iter 11600 | Time 0.0222(0.0263) | Loss 3.095725(3.062954)
Iter 11700 | Time 0.0214(0.0241) | Loss 3.146262(3.044931)
Iter 11800 | Time 0.0224(0.0266) | Loss 3.169652(3.038212)
Iter 11900 | Time 0.0205(0.0269) | Loss 3.031350(3.053730)
Iter 12000 | Time 0.0149(0.0190) | Loss 2.966974(3.051205)
[TEST] Iter 12000 | Test Loss 3.084479
Iter 12100 | Time 0.0191(0.0167) | Loss 3.029343(3.045095)
Iter 12200 | Time 0.0157(0.0151) | Loss 3.027032(3.050203)
Iter 12300 | Time 0.0132(0.0175) | Loss 3.137111(3.064924)
Iter 12400 | Time 0.0128(0.0154) | Loss 3.071673(3.054421)
Iter 12500 | Time 0.0150(0.0152) | Loss 3.063286(3.049350)
Iter 12600 | Time 0.0169(0.0152) | Loss 3.043372(3.046971)
Iter 12700 | Time 0.0140(0.0148) | Loss 3.122154(3.034040)
Iter 12800 | Time 0.0139(0.0145) | Loss 3.097806(3.032759)
Iter 12900 | Time 0.0196(0.0156) | Loss 2.880623(3.046883)
Iter 13000 | Time 0.0120(0.0152) | Loss 3.200897(3.037280)
[TEST] Iter 13000 | Test Loss 3.010043
Iter 13100 | Time 0.0145(0.0150) | Loss 2.919425(3.046601)
Iter 13200 | Time 0.0146(0.0146) | Loss 3.006335(3.050208)
Iter 13300 | Time 0.0168(0.0171) | Loss 2.983888(3.057050)
Iter 13400 | Time 0.0176(0.0181) | Loss 3.077509(3.040784)
Iter 13500 | Time 0.0145(0.0191) | Loss 3.067251(3.043474)
Iter 13600 | Time 0.0126(0.0170) | Loss 2.952181(3.016826)
Iter 13700 | Time 0.0135(0.0166) | Loss 3.083375(3.043369)
Iter 13800 | Time 0.0143(0.0160) | Loss 2.982592(3.027517)
Iter 13900 | Time 0.0136(0.0162) | Loss 2.975070(3.028875)
Iter 14000 | Time 0.0157(0.0148) | Loss 3.156403(3.033834)
[TEST] Iter 14000 | Test Loss 3.050190
Iter 14100 | Time 0.0141(0.0157) | Loss 3.077931(3.042231)
Iter 14200 | Time 0.0151(0.0159) | Loss 3.071716(3.029280)
Iter 14300 | Time 0.0155(0.0164) | Loss 2.911395(3.053858)
Iter 14400 | Time 0.0149(0.0150) | Loss 3.101907(3.045168)
Iter 14500 | Time 0.0139(0.0150) | Loss 3.045330(3.038665)
Iter 14600 | Time 0.0156(0.0147) | Loss 3.010392(3.036676)
Iter 14700 | Time 0.0142(0.0150) | Loss 3.166364(3.018471)
Iter 14800 | Time 0.0141(0.0160) | Loss 2.965108(3.020785)
Iter 14900 | Time 0.0164(0.0190) | Loss 2.835969(3.022149)
Iter 15000 | Time 0.0209(0.0245) | Loss 3.111636(3.027309)
[TEST] Iter 15000 | Test Loss 2.977141
Iter 15100 | Time 0.0125(0.0232) | Loss 3.228494(3.037596)
Iter 15200 | Time 0.0151(0.0240) | Loss 3.100567(3.026837)
Iter 15300 | Time 0.0150(0.0183) | Loss 3.103103(3.026801)
Iter 15400 | Time 0.0156(0.0160) | Loss 3.062709(3.017789)
Iter 15500 | Time 0.0124(0.0150) | Loss 2.972526(3.016658)
Iter 15600 | Time 0.0172(0.0155) | Loss 3.303849(3.029050)
Iter 15700 | Time 0.0134(0.0201) | Loss 2.890948(3.020236)
Iter 15800 | Time 0.0254(0.0180) | Loss 3.167366(3.031853)
Iter 15900 | Time 0.0135(0.0164) | Loss 3.082167(3.021501)
Iter 16000 | Time 0.0144(0.0154) | Loss 3.083770(3.026210)
[TEST] Iter 16000 | Test Loss 3.002810
Iter 16100 | Time 0.0141(0.0162) | Loss 3.182891(3.013542)
Iter 16200 | Time 0.0149(0.0166) | Loss 2.918850(3.022805)
Iter 16300 | Time 0.0167(0.0153) | Loss 3.183358(3.012553)
Iter 16400 | Time 0.0137(0.0155) | Loss 3.151467(3.029394)
Iter 16500 | Time 0.0126(0.0160) | Loss 3.060701(3.021407)
Iter 16600 | Time 0.0128(0.0163) | Loss 2.937614(3.003249)
Iter 16700 | Time 0.0130(0.0158) | Loss 3.281940(3.014006)
Iter 16800 | Time 0.0145(0.0158) | Loss 3.178781(3.017794)
Iter 16900 | Time 0.0144(0.0164) | Loss 3.065340(3.031203)
Iter 17000 | Time 0.0212(0.0158) | Loss 2.980558(3.005198)
[TEST] Iter 17000 | Test Loss 3.049087
Iter 17100 | Time 0.0133(0.0155) | Loss 2.984550(3.029447)
Iter 17200 | Time 0.0145(0.0155) | Loss 2.939546(3.020211)
Iter 17300 | Time 0.0141(0.0156) | Loss 2.895311(3.030302)
Iter 17400 | Time 0.0151(0.0170) | Loss 3.076373(3.017406)
Iter 17500 | Time 0.0145(0.0178) | Loss 2.943325(3.015448)
Iter 17600 | Time 0.0133(0.0169) | Loss 2.999692(3.010108)
Iter 17700 | Time 0.0171(0.0159) | Loss 2.886475(3.017132)
Iter 17800 | Time 0.0191(0.0163) | Loss 2.953449(3.005866)
Iter 17900 | Time 0.0139(0.0157) | Loss 2.751276(3.004420)
Iter 18000 | Time 0.0153(0.0161) | Loss 3.170529(3.014095)
[TEST] Iter 18000 | Test Loss 3.025492
Iter 18100 | Time 0.0144(0.0220) | Loss 3.070004(3.013727)
Iter 18200 | Time 0.0166(0.0227) | Loss 2.857958(3.016026)
Iter 18300 | Time 0.0224(0.0198) | Loss 3.148110(3.018616)
Iter 18400 | Time 0.0130(0.0200) | Loss 3.064963(3.023156)
Iter 18500 | Time 0.0255(0.0219) | Loss 2.932773(3.026422)
Iter 18600 | Time 0.0149(0.0196) | Loss 3.019441(3.017467)
Iter 18700 | Time 0.0133(0.0193) | Loss 3.051023(3.019434)
Iter 18800 | Time 0.0131(0.0172) | Loss 2.962893(2.997002)
Iter 18900 | Time 0.0168(0.0199) | Loss 3.012375(3.019911)
Iter 19000 | Time 0.0135(0.0167) | Loss 2.939473(3.025327)
[TEST] Iter 19000 | Test Loss 2.971290
Iter 19100 | Time 0.0233(0.0204) | Loss 2.969689(3.011311)
Iter 19200 | Time 0.0212(0.0323) | Loss 3.063931(3.010891)
Iter 19300 | Time 0.0265(0.0285) | Loss 2.961311(3.003384)
Iter 19400 | Time 0.0139(0.0207) | Loss 3.065257(2.996310)
Iter 19500 | Time 0.0160(0.0164) | Loss 2.944291(3.011577)
Iter 19600 | Time 0.0435(0.0164) | Loss 3.011726(3.025143)
Iter 19700 | Time 0.0181(0.0187) | Loss 3.094168(3.026042)
Iter 19800 | Time 0.0182(0.0208) | Loss 2.989354(3.018364)
Iter 19900 | Time 0.0185(0.0181) | Loss 3.019460(2.997668)
Iter 20000 | Time 0.0154(0.0180) | Loss 3.205812(3.013200)
[TEST] Iter 20000 | Test Loss 3.030168
Iter 20100 | Time 0.0154(0.0168) | Loss 2.938896(3.000798)
Iter 20200 | Time 0.0149(0.0168) | Loss 2.949604(3.013524)
Iter 20300 | Time 0.0164(0.0158) | Loss 2.972281(2.984567)
Iter 20400 | Time 0.0229(0.0172) | Loss 2.922031(2.990095)
Iter 20500 | Time 0.0133(0.0169) | Loss 3.012571(2.987072)
Iter 20600 | Time 0.0139(0.0150) | Loss 3.008801(3.012886)
Iter 20700 | Time 0.0174(0.0154) | Loss 2.964390(2.995211)
Iter 20800 | Time 0.0140(0.0148) | Loss 3.096584(3.005169)
Iter 20900 | Time 0.0145(0.0155) | Loss 3.103539(2.975679)
Iter 21000 | Time 0.0149(0.0152) | Loss 3.244659(3.016886)
[TEST] Iter 21000 | Test Loss 2.992754
Iter 21100 | Time 0.0216(0.0219) | Loss 2.892262(3.013370)
Iter 21200 | Time 0.0154(0.0187) | Loss 3.174297(3.004782)
Iter 21300 | Time 0.0126(0.0174) | Loss 3.085639(3.023607)
Iter 21400 | Time 0.0145(0.0216) | Loss 2.949283(2.999671)
Iter 21500 | Time 0.0194(0.0193) | Loss 2.975126(2.996870)
Iter 21600 | Time 0.0150(0.0165) | Loss 3.068375(3.008077)
Iter 21700 | Time 0.0190(0.0161) | Loss 3.032440(2.997348)
Iter 21800 | Time 0.0724(0.0391) | Loss 2.934987(2.991057)
Iter 21900 | Time 0.0160(0.0241) | Loss 2.940115(2.970004)
Iter 22000 | Time 0.0153(0.0174) | Loss 2.817603(3.015303)
[TEST] Iter 22000 | Test Loss 2.944659
Iter 22100 | Time 0.0164(0.0175) | Loss 2.851501(2.994405)
Iter 22200 | Time 0.0158(0.0172) | Loss 2.997030(2.986713)
Iter 22300 | Time 0.0176(0.0166) | Loss 2.982459(2.990686)
Iter 22400 | Time 0.0184(0.0177) | Loss 3.040259(2.999292)
Iter 22500 | Time 0.0196(0.0175) | Loss 2.831097(2.998579)
Iter 22600 | Time 0.0139(0.0163) | Loss 2.810616(2.996073)
Iter 22700 | Time 0.0142(0.0225) | Loss 3.003687(2.991012)
Iter 22800 | Time 0.0146(0.0189) | Loss 2.803472(2.988738)
Iter 22900 | Time 0.0190(0.0163) | Loss 3.093928(2.991458)
Iter 23000 | Time 0.0148(0.0185) | Loss 3.105254(2.991967)
[TEST] Iter 23000 | Test Loss 2.988146
Iter 23100 | Time 0.2162(0.1731) | Loss 2.855831(2.997575)
Iter 23200 | Time 0.0157(0.0398) | Loss 2.754916(2.993830)
Iter 23300 | Time 0.0156(0.0189) | Loss 2.950391(2.979642)
Iter 23400 | Time 0.0143(0.0162) | Loss 2.810451(2.982712)
Iter 23500 | Time 0.0138(0.0176) | Loss 3.050579(2.988834)
Iter 23600 | Time 0.0127(0.0163) | Loss 3.080288(2.994486)
Iter 23700 | Time 0.0153(0.0161) | Loss 3.099504(2.981763)
Iter 23800 | Time 0.0131(0.0157) | Loss 3.110916(2.995168)
Iter 23900 | Time 0.0148(0.0158) | Loss 3.117587(2.990278)
Iter 24000 | Time 0.0208(0.0157) | Loss 3.022139(2.983854)
[TEST] Iter 24000 | Test Loss 2.921281
Iter 24100 | Time 0.0136(0.0153) | Loss 3.202885(2.987726)
Iter 24200 | Time 0.0138(0.0152) | Loss 3.047517(2.974395)
Iter 24300 | Time 0.0163(0.0157) | Loss 3.137309(2.984841)
Iter 24400 | Time 0.0133(0.0158) | Loss 2.967711(3.000152)
Iter 24500 | Time 0.0158(0.0174) | Loss 2.931812(2.992504)
Iter 24600 | Time 0.0154(0.0158) | Loss 2.930771(2.987229)
Iter 24700 | Time 0.0143(0.0151) | Loss 3.090019(2.990775)
Iter 24800 | Time 0.0154(0.0154) | Loss 2.844444(3.004268)
Iter 24900 | Time 0.0140(0.0153) | Loss 2.865800(2.981094)
Iter 25000 | Time 0.0131(0.0161) | Loss 3.301193(2.999190)
[TEST] Iter 25000 | Test Loss 2.978580
Iter 25100 | Time 0.0131(0.0158) | Loss 3.114531(2.976180)
Iter 25200 | Time 0.0126(0.0148) | Loss 2.968436(2.986101)
Iter 25300 | Time 0.0138(0.0148) | Loss 3.003857(2.983900)
Iter 25400 | Time 0.0179(0.0146) | Loss 3.058456(2.974208)
Iter 25500 | Time 0.0121(0.0152) | Loss 2.792845(2.993252)
Iter 25600 | Time 0.0138(0.0148) | Loss 3.046121(2.989123)
Iter 25700 | Time 0.0144(0.0150) | Loss 2.913690(2.991601)
Iter 25800 | Time 0.0150(0.0150) | Loss 2.817183(2.987377)
Iter 25900 | Time 0.0154(0.0159) | Loss 3.110929(2.985886)
Iter 26000 | Time 0.0144(0.0148) | Loss 3.243409(2.979864)
[TEST] Iter 26000 | Test Loss 2.993246
Iter 26100 | Time 0.0137(0.0148) | Loss 2.859144(2.964842)
Iter 26200 | Time 0.0138(0.0153) | Loss 3.005196(2.990202)
Iter 26300 | Time 0.0138(0.0148) | Loss 2.908125(2.966278)
Iter 26400 | Time 0.0135(0.0147) | Loss 2.844337(2.962880)
Iter 26500 | Time 0.0130(0.0150) | Loss 2.991264(2.994967)
Iter 26600 | Time 0.0136(0.0149) | Loss 3.074037(2.982580)
Iter 26700 | Time 0.0143(0.0148) | Loss 2.945035(2.992853)
Iter 26800 | Time 0.0149(0.0146) | Loss 2.965544(2.983194)
Iter 26900 | Time 0.0139(0.0153) | Loss 2.879037(2.985627)
Iter 27000 | Time 0.0153(0.0158) | Loss 3.042991(2.981816)
[TEST] Iter 27000 | Test Loss 2.992406
Iter 27100 | Time 0.0177(0.0179) | Loss 2.947565(2.969456)
Iter 27200 | Time 0.0144(0.0157) | Loss 2.879345(2.975192)
Iter 27300 | Time 0.0135(0.0152) | Loss 2.840840(2.974006)
Iter 27400 | Time 0.0180(0.0152) | Loss 2.971287(2.994228)
Iter 27500 | Time 0.0136(0.0150) | Loss 3.010891(2.978938)
Iter 27600 | Time 0.0147(0.0162) | Loss 3.051583(2.994693)
Iter 27700 | Time 0.0159(0.0178) | Loss 3.202351(2.976581)
Iter 27800 | Time 0.0204(0.0288) | Loss 2.986169(2.993412)
Iter 27900 | Time 0.0143(0.0197) | Loss 2.915705(2.990753)
Iter 28000 | Time 0.0132(0.0156) | Loss 2.996659(2.979248)
[TEST] Iter 28000 | Test Loss 2.995595
Iter 28100 | Time 0.0169(0.0165) | Loss 2.836632(2.965569)
Iter 28200 | Time 0.0176(0.0159) | Loss 3.034538(2.974160)
Iter 28300 | Time 0.0135(0.0148) | Loss 3.231358(2.981871)
Iter 28400 | Time 0.0148(0.0155) | Loss 2.983022(2.982221)
Iter 28500 | Time 0.0133(0.0163) | Loss 2.984394(2.968480)
Iter 28600 | Time 0.0148(0.0161) | Loss 2.956425(2.964149)
Iter 28700 | Time 0.0138(0.0231) | Loss 3.088873(2.978252)
Iter 28800 | Time 0.0645(0.0263) | Loss 3.027201(2.971979)
Iter 28900 | Time 0.0174(0.0175) | Loss 2.778724(2.971961)
Iter 29000 | Time 0.0139(0.0184) | Loss 2.818577(2.974685)
[TEST] Iter 29000 | Test Loss 2.947844
Iter 29100 | Time 0.0159(0.0203) | Loss 3.054912(2.977066)
Iter 29200 | Time 0.0129(0.0177) | Loss 2.938426(2.971653)
Iter 29300 | Time 0.0851(0.0231) | Loss 2.856724(2.970916)
Iter 29400 | Time 0.0159(0.0227) | Loss 2.930077(2.962324)
Iter 29500 | Time 0.0122(0.0169) | Loss 2.997731(2.971920)
Iter 29600 | Time 0.0162(0.0166) | Loss 2.849316(2.950682)
Iter 29700 | Time 0.0133(0.0173) | Loss 3.048265(2.977205)
Iter 29800 | Time 0.0165(0.0219) | Loss 3.166140(2.970960)
Iter 29900 | Time 0.0170(0.0178) | Loss 2.919262(2.974395)
Iter 30000 | Time 0.0191(0.0188) | Loss 3.086837(2.987921)
[TEST] Iter 30000 | Test Loss 2.983350
Iter 30100 | Time 0.0149(0.0150) | Loss 3.071171(2.970121)
Iter 30200 | Time 0.0147(0.0156) | Loss 3.022977(2.971901)
Iter 30300 | Time 0.0119(0.0149) | Loss 3.163999(2.997880)
Iter 30400 | Time 0.0157(0.0158) | Loss 2.925488(2.975972)
Iter 30500 | Time 0.0191(0.0190) | Loss 2.817060(2.964614)
Iter 30600 | Time 0.0136(0.0189) | Loss 2.901065(2.953798)
Iter 30700 | Time 0.0161(0.0163) | Loss 3.322501(2.969619)
Iter 30800 | Time 0.0167(0.0192) | Loss 2.970054(2.984493)
Iter 30900 | Time 0.0155(0.0165) | Loss 2.932135(2.966462)
Iter 31000 | Time 0.0168(0.0155) | Loss 2.908481(2.975763)
[TEST] Iter 31000 | Test Loss 2.932933
Iter 31100 | Time 0.0185(0.0149) | Loss 3.037554(2.962421)
Iter 31200 | Time 0.0171(0.0149) | Loss 2.829136(2.953119)
Iter 31300 | Time 0.0168(0.0148) | Loss 3.153898(2.950445)
Iter 31400 | Time 0.0172(0.0144) | Loss 2.951971(2.966300)
Iter 31500 | Time 0.0151(0.0149) | Loss 2.853526(2.958115)
Iter 31600 | Time 0.0184(0.0162) | Loss 3.052024(2.949975)
Iter 31700 | Time 0.0192(0.0163) | Loss 2.841404(2.975806)
Iter 31800 | Time 0.0144(0.0179) | Loss 3.038304(2.978393)
Iter 31900 | Time 0.0171(0.0167) | Loss 2.995322(2.964794)
Iter 32000 | Time 0.0159(0.0176) | Loss 2.922723(2.963918)
[TEST] Iter 32000 | Test Loss 3.011179
Iter 32100 | Time 0.0133(0.0153) | Loss 3.181924(2.967804)
Iter 32200 | Time 0.0153(0.0150) | Loss 3.060255(2.964457)
Iter 32300 | Time 0.0231(0.0185) | Loss 2.946080(2.963569)
Iter 32400 | Time 0.0153(0.0169) | Loss 2.929088(2.961201)
Iter 32500 | Time 0.0142(0.0162) | Loss 3.016744(2.977611)
Iter 32600 | Time 0.0142(0.0151) | Loss 2.970628(2.966644)
Iter 32700 | Time 0.0177(0.0150) | Loss 2.994205(2.976671)
Iter 32800 | Time 0.0145(0.0147) | Loss 3.049563(2.969368)
Iter 32900 | Time 0.0161(0.0164) | Loss 2.993012(2.953767)
Iter 33000 | Time 0.0211(0.0206) | Loss 2.840312(2.963600)
[TEST] Iter 33000 | Test Loss 2.949138
Iter 33100 | Time 0.0141(0.0174) | Loss 3.039211(2.957699)
Iter 33200 | Time 0.0136(0.0159) | Loss 2.934984(2.954917)
Iter 33300 | Time 0.0155(0.0153) | Loss 2.929399(2.947832)
Iter 33400 | Time 0.0175(0.0157) | Loss 3.011973(2.978971)
Iter 33500 | Time 0.0154(0.0160) | Loss 2.946981(2.965284)
Iter 33600 | Time 0.0129(0.0153) | Loss 3.002886(2.965009)
Iter 33700 | Time 0.0157(0.0151) | Loss 2.970559(2.979006)
Iter 33800 | Time 0.0142(0.0152) | Loss 3.092759(2.961955)
Iter 33900 | Time 0.0163(0.0152) | Loss 2.966529(2.956588)
Iter 34000 | Time 0.0168(0.0169) | Loss 3.078546(2.968447)
[TEST] Iter 34000 | Test Loss 2.914819
Iter 34100 | Time 0.0137(0.0151) | Loss 2.879646(2.955647)
Iter 34200 | Time 0.0146(0.0162) | Loss 2.941873(2.954168)
Iter 34300 | Time 0.0155(0.0168) | Loss 3.004993(2.953865)
Iter 34400 | Time 0.0157(0.0165) | Loss 2.806366(2.949666)
Iter 34500 | Time 0.0184(0.0173) | Loss 2.987504(2.965268)
Iter 34600 | Time 0.0144(0.0165) | Loss 2.980582(2.981372)
Iter 34700 | Time 0.0134(0.0166) | Loss 2.911336(2.961485)
Iter 34800 | Time 0.0141(0.0151) | Loss 3.016886(2.963982)
Iter 34900 | Time 0.0133(0.0146) | Loss 3.016733(2.964490)
Iter 35000 | Time 0.0135(0.0146) | Loss 3.108047(2.963766)
[TEST] Iter 35000 | Test Loss 3.006241
Iter 35100 | Time 0.0134(0.0147) | Loss 3.085430(2.963464)
Iter 35200 | Time 0.0142(0.0150) | Loss 2.942213(2.963113)
Iter 35300 | Time 0.0171(0.0147) | Loss 3.017555(2.955156)
Iter 35400 | Time 0.0167(0.0148) | Loss 3.256039(2.957726)
Iter 35500 | Time 0.0145(0.0148) | Loss 2.742277(2.963483)
Iter 35600 | Time 0.0130(0.0147) | Loss 2.807851(2.946005)
Iter 35700 | Time 0.0184(0.0147) | Loss 3.010755(2.977071)
Iter 35800 | Time 0.0141(0.0148) | Loss 2.973197(2.960992)
Iter 35900 | Time 0.0138(0.0150) | Loss 3.140829(2.955439)
Iter 36000 | Time 0.0174(0.0152) | Loss 2.819413(2.951277)
[TEST] Iter 36000 | Test Loss 2.976232
Iter 36100 | Time 0.0156(0.0175) | Loss 2.794600(2.966375)
Iter 36200 | Time 0.0123(0.0156) | Loss 2.964889(2.954034)
Iter 36300 | Time 0.0149(0.0147) | Loss 2.969257(2.962574)
Iter 36400 | Time 0.0138(0.0147) | Loss 3.069121(2.955920)
Iter 36500 | Time 0.0155(0.0147) | Loss 2.777278(2.960286)
Iter 36600 | Time 0.0152(0.0146) | Loss 2.968361(2.960231)
Iter 36700 | Time 0.0164(0.0148) | Loss 2.926075(2.958949)
Iter 36800 | Time 0.0154(0.0151) | Loss 2.827856(2.953381)
Iter 36900 | Time 0.0144(0.0149) | Loss 2.894925(2.965090)
Iter 37000 | Time 0.0145(0.0148) | Loss 3.006721(2.931453)
[TEST] Iter 37000 | Test Loss 2.944618
Iter 37100 | Time 0.0150(0.0148) | Loss 2.780583(2.945053)
Iter 37200 | Time 0.0145(0.0147) | Loss 2.952223(2.954348)
Iter 37300 | Time 0.0135(0.0148) | Loss 2.914239(2.949672)
Iter 37400 | Time 0.0136(0.0145) | Loss 2.949367(2.962508)
Iter 37500 | Time 0.0131(0.0143) | Loss 2.844594(2.952077)
Iter 37600 | Time 0.0308(0.0168) | Loss 3.009916(2.955038)
Iter 37700 | Time 0.0149(0.0150) | Loss 3.007015(2.954657)
Iter 37800 | Time 0.0144(0.0165) | Loss 2.930283(2.945659)
Iter 37900 | Time 0.0133(0.0154) | Loss 3.164908(2.967716)
Iter 38000 | Time 0.0140(0.0157) | Loss 2.775154(2.941834)
[TEST] Iter 38000 | Test Loss 2.918588
Iter 38100 | Time 0.0127(0.0153) | Loss 3.175288(2.971675)
Iter 38200 | Time 0.0142(0.0144) | Loss 3.037066(2.944497)
Iter 38300 | Time 0.0129(0.0145) | Loss 2.926373(2.958934)
Iter 38400 | Time 0.0160(0.0146) | Loss 2.835755(2.941464)
Iter 38500 | Time 0.0163(0.0148) | Loss 2.717949(2.955223)
Iter 38600 | Time 0.0156(0.0150) | Loss 2.953140(2.955801)
Iter 38700 | Time 0.0150(0.0151) | Loss 3.056616(2.954445)
Iter 38800 | Time 0.0151(0.0148) | Loss 2.809052(2.951910)
Iter 38900 | Time 0.0185(0.0154) | Loss 2.865890(2.963004)
Iter 39000 | Time 0.0133(0.0160) | Loss 3.130060(2.956960)
[TEST] Iter 39000 | Test Loss 2.932786
Iter 39100 | Time 0.0153(0.0152) | Loss 3.018112(2.948176)
Iter 39200 | Time 0.0158(0.0155) | Loss 3.135998(2.962449)
Iter 39300 | Time 0.0157(0.0167) | Loss 2.886135(2.956767)
Iter 39400 | Time 0.0146(0.0153) | Loss 2.994013(2.960388)
Iter 39500 | Time 0.0152(0.0147) | Loss 3.171500(2.954165)
Iter 39600 | Time 0.0154(0.0179) | Loss 2.800597(2.948264)
Iter 39700 | Time 0.0144(0.0173) | Loss 2.824405(2.937462)
Iter 39800 | Time 0.0133(0.0153) | Loss 2.974518(2.944596)
Iter 39900 | Time 0.0137(0.0145) | Loss 3.046442(2.956136)
Iter 40000 | Time 0.0136(0.0144) | Loss 2.916132(2.947757)
[TEST] Iter 40000 | Test Loss 3.023343
Iter 40100 | Time 0.0144(0.0151) | Loss 3.127371(2.942492)
Iter 40200 | Time 0.0129(0.0148) | Loss 3.017912(2.954839)
Iter 40300 | Time 0.0141(0.0143) | Loss 2.755865(2.940442)
Iter 40400 | Time 0.0150(0.0155) | Loss 2.945850(2.927409)
Iter 40500 | Time 0.0138(0.0188) | Loss 3.160945(2.939415)
Iter 40600 | Time 0.0141(0.0156) | Loss 2.840073(2.942525)
Iter 40700 | Time 0.0160(0.0150) | Loss 2.889812(2.941861)
Iter 40800 | Time 0.0144(0.0146) | Loss 2.950609(2.948894)
Iter 40900 | Time 0.0176(0.0151) | Loss 3.108976(2.954256)
Iter 41000 | Time 0.0173(0.0164) | Loss 2.963844(2.943060)
[TEST] Iter 41000 | Test Loss 2.972131
Iter 41100 | Time 0.0134(0.0166) | Loss 3.010485(2.948858)
Iter 41200 | Time 0.0148(0.0150) | Loss 2.927299(2.960666)
Iter 41300 | Time 0.0141(0.0147) | Loss 3.005358(2.933896)
Iter 41400 | Time 0.0143(0.0145) | Loss 3.108320(2.952597)
Iter 41500 | Time 0.0139(0.0143) | Loss 2.853628(2.944773)
Iter 41600 | Time 0.0149(0.0148) | Loss 3.068743(2.958086)
Iter 41700 | Time 0.0209(0.0148) | Loss 2.992236(2.950432)
Iter 41800 | Time 0.0152(0.0145) | Loss 3.030724(2.939712)
Iter 41900 | Time 0.0130(0.0170) | Loss 2.964693(2.937296)
Iter 42000 | Time 0.0139(0.0145) | Loss 2.786798(2.947105)
[TEST] Iter 42000 | Test Loss 2.972551
Iter 42100 | Time 0.0135(0.0145) | Loss 3.177478(2.954099)
Iter 42200 | Time 0.0148(0.0146) | Loss 2.868276(2.940888)
Iter 42300 | Time 0.0139(0.0148) | Loss 3.027882(2.940054)
Iter 42400 | Time 0.0150(0.0146) | Loss 2.938444(2.930152)
Iter 42500 | Time 0.0144(0.0151) | Loss 2.935119(2.964630)
Iter 42600 | Time 0.0153(0.0150) | Loss 2.961508(2.932449)
Iter 42700 | Time 0.0135(0.0144) | Loss 2.930384(2.935515)
Iter 42800 | Time 0.0140(0.0145) | Loss 2.826812(2.954342)
Iter 42900 | Time 0.0147(0.0160) | Loss 2.966689(2.925732)
Iter 43000 | Time 0.0165(0.0155) | Loss 2.877027(2.929734)
[TEST] Iter 43000 | Test Loss 2.949056
Iter 43100 | Time 0.0137(0.0198) | Loss 2.740434(2.923326)
Iter 43200 | Time 0.0136(0.0166) | Loss 2.998992(2.946001)
Iter 43300 | Time 0.0138(0.0193) | Loss 2.973463(2.940587)
Iter 43400 | Time 0.0130(0.0162) | Loss 3.048754(2.945614)
Iter 43500 | Time 0.0144(0.0155) | Loss 2.871336(2.951443)
Iter 43600 | Time 0.0184(0.0161) | Loss 2.976432(2.928413)
Iter 43700 | Time 0.0154(0.0212) | Loss 2.894653(2.928280)
Iter 43800 | Time 0.0155(0.0194) | Loss 2.960405(2.948862)
Iter 43900 | Time 0.0147(0.0173) | Loss 2.772324(2.941315)
Iter 44000 | Time 0.0164(0.0178) | Loss 3.013520(2.953572)
[TEST] Iter 44000 | Test Loss 2.928558
Iter 44100 | Time 0.0151(0.0167) | Loss 2.832465(2.929095)
Iter 44200 | Time 0.0153(0.0157) | Loss 2.931733(2.933639)
Iter 44300 | Time 0.0158(0.0171) | Loss 2.831510(2.926206)
Iter 44400 | Time 0.0152(0.0164) | Loss 2.885571(2.934285)
Iter 44500 | Time 0.0141(0.0174) | Loss 2.846555(2.937179)
Iter 44600 | Time 0.0151(0.0167) | Loss 3.123225(2.953505)
Iter 44700 | Time 0.0143(0.0158) | Loss 2.895756(2.928913)
Iter 44800 | Time 0.0144(0.0152) | Loss 2.925833(2.945464)
Iter 44900 | Time 0.0159(0.0156) | Loss 3.147625(2.943676)
Iter 45000 | Time 0.0153(0.0152) | Loss 3.044195(2.924850)
[TEST] Iter 45000 | Test Loss 2.940468
Iter 45100 | Time 0.0148(0.0161) | Loss 2.891413(2.944512)
Iter 45200 | Time 0.0130(0.0148) | Loss 2.913762(2.938704)
Iter 45300 | Time 0.0160(0.0152) | Loss 2.968045(2.944994)
Iter 45400 | Time 0.0148(0.0150) | Loss 3.031611(2.946253)
Iter 45500 | Time 0.0165(0.0152) | Loss 2.974562(2.944952)
Iter 45600 | Time 0.0150(0.0166) | Loss 3.012579(2.929205)
Iter 45700 | Time 0.0155(0.0149) | Loss 3.100744(2.944527)
Iter 45800 | Time 0.0198(0.0149) | Loss 3.092727(2.939040)
Iter 45900 | Time 0.0148(0.0147) | Loss 2.932572(2.949216)
Iter 46000 | Time 0.0118(0.0149) | Loss 2.793709(2.920299)
[TEST] Iter 46000 | Test Loss 2.930758
Iter 46100 | Time 0.0131(0.0145) | Loss 2.887046(2.940991)
Iter 46200 | Time 0.0144(0.0149) | Loss 2.985084(2.937848)
Iter 46300 | Time 0.0137(0.0148) | Loss 2.830137(2.927711)
Iter 46400 | Time 0.0147(0.0146) | Loss 2.957059(2.928540)
Iter 46500 | Time 0.0164(0.0143) | Loss 2.746141(2.934084)
Iter 46600 | Time 0.0181(0.0162) | Loss 3.105214(2.946445)
Iter 46700 | Time 0.0322(0.0203) | Loss 2.896882(2.940196)
Iter 46800 | Time 0.0141(0.0179) | Loss 2.870522(2.949349)
Iter 46900 | Time 0.0132(0.0158) | Loss 2.822644(2.921172)
Iter 47000 | Time 0.0141(0.0149) | Loss 2.829177(2.926564)
[TEST] Iter 47000 | Test Loss 2.930463
Iter 47100 | Time 0.0138(0.0155) | Loss 2.979454(2.946937)
Iter 47200 | Time 0.0147(0.0149) | Loss 2.923935(2.938310)
Iter 47300 | Time 0.0141(0.0147) | Loss 2.985071(2.948401)
Iter 47400 | Time 0.0163(0.0148) | Loss 3.191200(2.942716)
Iter 47500 | Time 0.0160(0.0149) | Loss 2.986520(2.931641)
Iter 47600 | Time 0.0144(0.0145) | Loss 2.799091(2.928762)
Iter 47700 | Time 0.0151(0.0152) | Loss 3.082195(2.960845)
Iter 47800 | Time 0.0135(0.0146) | Loss 2.974169(2.931874)
Iter 47900 | Time 0.0130(0.0149) | Loss 2.883725(2.937529)
Iter 48000 | Time 0.0128(0.0146) | Loss 2.800667(2.918236)
[TEST] Iter 48000 | Test Loss 2.953170
Iter 48100 | Time 0.0162(0.0184) | Loss 2.895074(2.951057)
Iter 48200 | Time 0.0146(0.0151) | Loss 3.009291(2.932286)
Iter 48300 | Time 0.0165(0.0180) | Loss 2.828325(2.932198)
Iter 48400 | Time 0.0139(0.0159) | Loss 2.997409(2.939976)
Iter 48500 | Time 0.0227(0.0179) | Loss 2.867254(2.937552)
Iter 48600 | Time 0.0149(0.0167) | Loss 2.841722(2.964691)
Iter 48700 | Time 0.0166(0.0193) | Loss 2.859610(2.933605)
Iter 48800 | Time 0.0161(0.0179) | Loss 2.842642(2.936968)
Iter 48900 | Time 0.0150(0.0161) | Loss 2.836827(2.948204)
Iter 49000 | Time 0.0156(0.0154) | Loss 2.965227(2.941770)
[TEST] Iter 49000 | Test Loss 2.980233
Iter 49100 | Time 0.0154(0.0183) | Loss 2.848367(2.939590)
Iter 49200 | Time 0.0138(0.0149) | Loss 2.873281(2.920772)
Iter 49300 | Time 0.0143(0.0149) | Loss 3.183768(2.939314)
Iter 49400 | Time 0.0144(0.0150) | Loss 2.986969(2.940656)
Iter 49500 | Time 0.0142(0.0160) | Loss 2.847548(2.938368)
Iter 49600 | Time 0.0117(0.0147) | Loss 2.943469(2.945756)
Iter 49700 | Time 0.0138(0.0152) | Loss 2.884881(2.938001)
Iter 49800 | Time 0.0130(0.0153) | Loss 2.852571(2.937791)
Iter 49900 | Time 0.0132(0.0154) | Loss 2.862139(2.938739)
Iter 50000 | Time 0.0127(0.0144) | Loss 3.009507(2.923474)
[TEST] Iter 50000 | Test Loss 2.939368
Iter 50100 | Time 0.0146(0.0143) | Loss 2.928511(2.917270)
Iter 50200 | Time 0.0141(0.0145) | Loss 2.934239(2.940753)
Iter 50300 | Time 0.0130(0.0145) | Loss 2.871707(2.923821)
Iter 50400 | Time 0.0139(0.0143) | Loss 2.840752(2.925480)
Iter 50500 | Time 0.0134(0.0146) | Loss 2.850062(2.936945)
Iter 50600 | Time 0.0145(0.0142) | Loss 3.057171(2.947772)
Iter 50700 | Time 0.0142(0.0144) | Loss 2.970253(2.931013)
Iter 50800 | Time 0.0156(0.0145) | Loss 2.817924(2.941001)
Iter 50900 | Time 0.0165(0.0145) | Loss 2.889031(2.926961)
Iter 51000 | Time 0.0139(0.0145) | Loss 2.856915(2.929989)
[TEST] Iter 51000 | Test Loss 2.914503
Iter 51100 | Time 0.0191(0.0182) | Loss 2.689864(2.937421)
Iter 51200 | Time 0.0168(0.0168) | Loss 3.060061(2.951708)
Iter 51300 | Time 0.0272(0.0184) | Loss 2.891778(2.943813)
Iter 51400 | Time 0.0335(0.0171) | Loss 3.015425(2.929410)
Iter 51500 | Time 0.0310(0.0198) | Loss 2.857954(2.935128)
Iter 51600 | Time 0.0147(0.0153) | Loss 2.811180(2.926636)
Iter 51700 | Time 0.0180(0.0153) | Loss 3.144861(2.934380)
Iter 51800 | Time 0.0130(0.0179) | Loss 3.058869(2.929133)
Iter 51900 | Time 0.0158(0.0163) | Loss 2.931004(2.951337)
Iter 52000 | Time 0.0203(0.0170) | Loss 2.872547(2.924419)
[TEST] Iter 52000 | Test Loss 2.897407
Iter 52100 | Time 0.0175(0.0172) | Loss 2.904662(2.951986)
Iter 52200 | Time 0.0156(0.0169) | Loss 2.918530(2.947933)
Iter 52300 | Time 0.0142(0.0150) | Loss 2.869839(2.932669)
Iter 52400 | Time 0.0130(0.0150) | Loss 2.790450(2.924565)
Iter 52500 | Time 0.0134(0.0153) | Loss 2.837848(2.940620)
Iter 52600 | Time 0.0192(0.0148) | Loss 3.002702(2.934447)
Iter 52700 | Time 0.0159(0.0145) | Loss 3.024871(2.933011)
Iter 52800 | Time 0.0275(0.0167) | Loss 2.967046(2.915334)
Iter 52900 | Time 0.0133(0.0180) | Loss 2.839105(2.931383)
Iter 53000 | Time 0.0142(0.0177) | Loss 2.830733(2.934147)
[TEST] Iter 53000 | Test Loss 2.880249
Iter 53100 | Time 0.0192(0.0186) | Loss 2.976458(2.950793)
Iter 53200 | Time 0.0159(0.0175) | Loss 2.981871(2.934825)
Iter 53300 | Time 0.0189(0.0160) | Loss 2.893090(2.936103)
Iter 53400 | Time 0.0163(0.0174) | Loss 2.870937(2.925439)
Iter 53500 | Time 0.0149(0.0156) | Loss 3.011791(2.932204)
Iter 53600 | Time 0.0145(0.0151) | Loss 3.091045(2.940903)
Iter 53700 | Time 0.0143(0.0162) | Loss 2.928525(2.943194)
Iter 53800 | Time 0.0177(0.0197) | Loss 2.944329(2.931062)
Iter 53900 | Time 0.0158(0.0156) | Loss 2.897370(2.935928)
Iter 54000 | Time 0.0148(0.0159) | Loss 2.931649(2.934810)
[TEST] Iter 54000 | Test Loss 2.972565
Iter 54100 | Time 0.0143(0.0162) | Loss 3.003374(2.938887)
Iter 54200 | Time 0.0166(0.0150) | Loss 2.825021(2.936551)
Iter 54300 | Time 0.0157(0.0150) | Loss 2.953022(2.942933)
Iter 54400 | Time 0.0141(0.0152) | Loss 2.961135(2.942777)
Iter 54500 | Time 0.0161(0.0150) | Loss 3.014194(2.934936)
Iter 54600 | Time 0.0156(0.0149) | Loss 2.933686(2.918477)
Iter 54700 | Time 0.0136(0.0148) | Loss 2.976355(2.947970)
Iter 54800 | Time 0.0160(0.0152) | Loss 2.751872(2.928719)
Iter 54900 | Time 0.0158(0.0149) | Loss 2.978123(2.925027)
Iter 55000 | Time 0.0163(0.0148) | Loss 3.026286(2.933681)
[TEST] Iter 55000 | Test Loss 2.908039
Iter 55100 | Time 0.0143(0.0145) | Loss 3.042075(2.926653)
Iter 55200 | Time 0.0137(0.0148) | Loss 2.928821(2.935890)
Iter 55300 | Time 0.0168(0.0154) | Loss 2.916273(2.932729)
Iter 55400 | Time 0.0130(0.0177) | Loss 2.728686(2.933779)
Iter 55500 | Time 0.0133(0.0168) | Loss 2.964178(2.930572)
Iter 55600 | Time 0.0143(0.0154) | Loss 3.131157(2.938019)
Iter 55700 | Time 0.0156(0.0226) | Loss 2.835696(2.916715)
Iter 55800 | Time 0.0183(0.0182) | Loss 2.936584(2.921534)
Iter 55900 | Time 0.0219(0.0175) | Loss 2.843735(2.903429)
Iter 56000 | Time 0.0222(0.0187) | Loss 3.100272(2.927761)
[TEST] Iter 56000 | Test Loss 2.928885
Iter 56100 | Time 0.0167(0.0172) | Loss 2.808761(2.929585)
Iter 56200 | Time 0.0145(0.0189) | Loss 2.928749(2.946943)
Iter 56300 | Time 0.0212(0.0388) | Loss 3.112528(2.944533)
Iter 56400 | Time 0.0312(0.0229) | Loss 2.865226(2.918275)
Iter 56500 | Time 0.0129(0.0172) | Loss 2.864512(2.906195)
Iter 56600 | Time 0.0203(0.0187) | Loss 3.047554(2.931143)
Iter 56700 | Time 0.0165(0.0175) | Loss 3.108119(2.932950)
Iter 56800 | Time 0.0153(0.0186) | Loss 2.897511(2.928538)
Iter 56900 | Time 0.0142(0.0160) | Loss 2.900393(2.926291)
Iter 57000 | Time 0.0150(0.0152) | Loss 2.914634(2.908322)
[TEST] Iter 57000 | Test Loss 2.980951
Iter 57100 | Time 0.0134(0.0188) | Loss 2.868598(2.921869)
Iter 57200 | Time 0.0181(0.0184) | Loss 2.713274(2.939573)
Iter 57300 | Time 0.0163(0.0281) | Loss 2.872905(2.915007)
Iter 57400 | Time 0.0126(0.0185) | Loss 2.940438(2.934899)
Iter 57500 | Time 0.0159(0.0207) | Loss 2.975995(2.934364)
Iter 57600 | Time 0.0135(0.0211) | Loss 3.054463(2.929112)
Iter 57700 | Time 0.0127(0.0220) | Loss 2.903522(2.940060)
Iter 57800 | Time 0.0151(0.0193) | Loss 2.956913(2.929381)
Iter 57900 | Time 0.0387(0.0200) | Loss 2.860288(2.937574)
Iter 58000 | Time 0.0136(0.0206) | Loss 2.854084(2.927630)
[TEST] Iter 58000 | Test Loss 2.953950
Iter 58100 | Time 0.0147(0.0201) | Loss 2.897331(2.941017)
Iter 58200 | Time 0.0162(0.0164) | Loss 2.874939(2.950359)
Iter 58300 | Time 0.0145(0.0209) | Loss 2.895566(2.933161)
Iter 58400 | Time 0.0197(0.0172) | Loss 2.739359(2.923972)
Iter 58500 | Time 0.0129(0.0153) | Loss 3.009310(2.930345)
Iter 58600 | Time 0.0136(0.0149) | Loss 2.886191(2.940963)
Iter 58700 | Time 0.0136(0.0165) | Loss 2.853997(2.944850)
Iter 58800 | Time 0.0160(0.0153) | Loss 2.999741(2.948835)
Iter 58900 | Time 0.0153(0.0150) | Loss 2.845130(2.931985)
Iter 59000 | Time 0.0140(0.0154) | Loss 3.089531(2.926661)
[TEST] Iter 59000 | Test Loss 2.952277
Iter 59100 | Time 0.0206(0.0183) | Loss 2.884755(2.945179)
Iter 59200 | Time 0.0248(0.0217) | Loss 3.060017(2.927578)
Iter 59300 | Time 0.0172(0.0177) | Loss 3.013228(2.928036)
Iter 59400 | Time 0.0147(0.0155) | Loss 2.972222(2.952208)
Iter 59500 | Time 0.0129(0.0169) | Loss 2.993661(2.937659)
Iter 59600 | Time 0.0182(0.0173) | Loss 3.068574(2.920397)
Iter 59700 | Time 0.0154(0.0155) | Loss 2.985248(2.936306)
Iter 59800 | Time 0.0159(0.0150) | Loss 2.906441(2.928293)
Iter 59900 | Time 0.0143(0.0151) | Loss 2.902573(2.936346)
Iter 60000 | Time 0.0134(0.0179) | Loss 2.987641(2.930051)
[TEST] Iter 60000 | Test Loss 2.893775
Iter 60100 | Time 0.0142(0.0167) | Loss 2.982642(2.943210)
Iter 60200 | Time 0.0153(0.0152) | Loss 3.065368(2.936251)
Iter 60300 | Time 0.0143(0.0148) | Loss 2.855345(2.931889)
Iter 60400 | Time 0.0164(0.0148) | Loss 3.129298(2.943441)
Iter 60500 | Time 0.0161(0.0151) | Loss 2.832327(2.929121)
Iter 60600 | Time 0.0183(0.0152) | Loss 2.802075(2.920148)
Iter 60700 | Time 0.0165(0.0150) | Loss 2.836766(2.924265)
Iter 60800 | Time 0.0131(0.0150) | Loss 2.969907(2.916008)
Iter 60900 | Time 0.0135(0.0151) | Loss 2.909399(2.914281)
Iter 61000 | Time 0.0148(0.0156) | Loss 2.994172(2.919685)
[TEST] Iter 61000 | Test Loss 2.918630
Iter 61100 | Time 0.0234(0.0172) | Loss 2.970422(2.930500)
Iter 61200 | Time 0.0171(0.0207) | Loss 2.964048(2.930517)
Iter 61300 | Time 0.0140(0.0173) | Loss 3.085847(2.949357)
Iter 61400 | Time 0.0190(0.0180) | Loss 3.057340(2.925450)
Iter 61500 | Time 0.0157(0.0188) | Loss 2.892439(2.948254)
Iter 61600 | Time 0.0134(0.0184) | Loss 2.928803(2.946087)
Iter 61700 | Time 0.0151(0.0158) | Loss 2.887892(2.923118)
Iter 61800 | Time 0.0158(0.0154) | Loss 2.773731(2.937516)
Iter 61900 | Time 0.0132(0.0154) | Loss 2.765205(2.937334)
Iter 62000 | Time 0.0141(0.0153) | Loss 2.844830(2.935869)
[TEST] Iter 62000 | Test Loss 2.964612
Iter 62100 | Time 0.0159(0.0189) | Loss 2.988764(2.939256)
Iter 62200 | Time 0.0130(0.0173) | Loss 3.119365(2.953866)
Iter 62300 | Time 0.0176(0.0168) | Loss 2.742675(2.917067)
Iter 62400 | Time 0.0218(0.0165) | Loss 2.922451(2.912977)
Iter 62500 | Time 0.0210(0.0193) | Loss 2.924000(2.922449)
Iter 62600 | Time 0.0160(0.0165) | Loss 2.832671(2.915561)
Iter 62700 | Time 0.0133(0.0152) | Loss 2.912004(2.920783)
Iter 62800 | Time 0.0153(0.0151) | Loss 2.986518(2.912534)
Iter 62900 | Time 0.0244(0.0159) | Loss 3.065783(2.907624)
Iter 63000 | Time 0.0158(0.0146) | Loss 2.885567(2.944204)
[TEST] Iter 63000 | Test Loss 2.923358
Iter 63100 | Time 0.0152(0.0151) | Loss 2.841887(2.940159)
Iter 63200 | Time 0.0142(0.0154) | Loss 2.999166(2.927998)
Iter 63300 | Time 0.0173(0.0175) | Loss 3.001091(2.931714)
Iter 63400 | Time 0.0126(0.0177) | Loss 2.890146(2.932875)
Iter 63500 | Time 0.0138(0.0166) | Loss 2.911870(2.914510)
Iter 63600 | Time 0.0176(0.0213) | Loss 2.754775(2.928083)
Iter 63700 | Time 0.0189(0.0189) | Loss 2.893579(2.925087)
Iter 63800 | Time 0.0142(0.0160) | Loss 2.915467(2.922325)
Iter 63900 | Time 0.0175(0.0161) | Loss 2.832733(2.919758)
Iter 64000 | Time 0.0157(0.0161) | Loss 3.065796(2.917788)
[TEST] Iter 64000 | Test Loss 2.916105
Iter 64100 | Time 0.0173(0.0159) | Loss 3.052112(2.922216)
Iter 64200 | Time 0.0153(0.0203) | Loss 3.181445(2.929916)
Iter 64300 | Time 0.0153(0.0155) | Loss 2.795218(2.938082)
Iter 64400 | Time 0.0158(0.0149) | Loss 3.020809(2.939260)
Iter 64500 | Time 0.0139(0.0145) | Loss 3.150809(2.932384)
Iter 64600 | Time 0.0133(0.0150) | Loss 3.085095(2.924556)
Iter 64700 | Time 0.0129(0.0154) | Loss 2.802599(2.927805)
Iter 64800 | Time 0.0150(0.0146) | Loss 2.850902(2.909540)
Iter 64900 | Time 0.0146(0.0147) | Loss 2.941358(2.904973)
Iter 65000 | Time 0.0134(0.0157) | Loss 3.015490(2.934325)
[TEST] Iter 65000 | Test Loss 2.912509
Iter 65100 | Time 0.0136(0.0150) | Loss 2.955228(2.915392)
Iter 65200 | Time 0.0133(0.0148) | Loss 3.047339(2.927115)
Iter 65300 | Time 0.0147(0.0146) | Loss 2.993999(2.940186)
Iter 65400 | Time 0.0164(0.0154) | Loss 2.969219(2.932964)
Iter 65500 | Time 0.0146(0.0156) | Loss 3.021851(2.925578)
Iter 65600 | Time 0.0130(0.0150) | Loss 2.753345(2.917503)
Iter 65700 | Time 0.0146(0.0146) | Loss 2.828647(2.918322)
Iter 65800 | Time 0.0143(0.0174) | Loss 3.004310(2.932880)
Iter 65900 | Time 0.0144(0.0154) | Loss 3.015573(2.944710)
Iter 66000 | Time 0.0136(0.0159) | Loss 2.889657(2.932457)
[TEST] Iter 66000 | Test Loss 2.856009
Iter 66100 | Time 0.0194(0.0244) | Loss 2.864070(2.923132)
Iter 66200 | Time 0.0156(0.0216) | Loss 2.871716(2.920526)
Iter 66300 | Time 0.0139(0.0192) | Loss 2.902099(2.901881)
Iter 66400 | Time 0.0145(0.0173) | Loss 2.991287(2.919287)
Iter 66500 | Time 0.0182(0.0189) | Loss 2.867638(2.935432)
Iter 66600 | Time 0.0152(0.0183) | Loss 2.889914(2.923069)
Iter 66700 | Time 0.0179(0.0160) | Loss 2.775823(2.924588)
Iter 66800 | Time 0.0131(0.0162) | Loss 3.002662(2.931513)
Iter 66900 | Time 0.0166(0.0158) | Loss 3.016321(2.931835)
Iter 67000 | Time 0.0185(0.0157) | Loss 2.911751(2.934465)
[TEST] Iter 67000 | Test Loss 2.876973
Iter 67100 | Time 0.0158(0.0153) | Loss 2.948930(2.935377)
Iter 67200 | Time 0.0143(0.0144) | Loss 2.999441(2.912455)
Iter 67300 | Time 0.0143(0.0145) | Loss 2.829113(2.915117)
Iter 67400 | Time 0.0132(0.0144) | Loss 3.055820(2.928759)
Iter 67500 | Time 0.0175(0.0146) | Loss 2.792031(2.922945)
Iter 67600 | Time 0.0135(0.0148) | Loss 3.053084(2.924608)
Iter 67700 | Time 0.0143(0.0145) | Loss 2.939165(2.940289)
Iter 67800 | Time 0.0137(0.0144) | Loss 3.006143(2.919585)
Iter 67900 | Time 0.0174(0.0161) | Loss 2.975879(2.917957)
Iter 68000 | Time 0.0140(0.0160) | Loss 3.034203(2.917243)
[TEST] Iter 68000 | Test Loss 2.902893
Iter 68100 | Time 0.0144(0.0146) | Loss 2.899160(2.917262)
Iter 68200 | Time 0.0135(0.0146) | Loss 2.956019(2.930175)
Iter 68300 | Time 0.0141(0.0155) | Loss 2.958633(2.909894)
Iter 68400 | Time 0.0135(0.0149) | Loss 2.818124(2.925157)
Iter 68500 | Time 0.0142(0.0146) | Loss 2.969823(2.926930)
Iter 68600 | Time 0.0137(0.0148) | Loss 2.725823(2.932723)
Iter 68700 | Time 0.0149(0.0145) | Loss 2.960266(2.910726)
Iter 68800 | Time 0.0163(0.0153) | Loss 2.977782(2.913388)
Iter 68900 | Time 0.0127(0.0154) | Loss 2.984208(2.924301)
Iter 69000 | Time 0.0143(0.0154) | Loss 2.839527(2.946073)
[TEST] Iter 69000 | Test Loss 2.855761
Iter 69100 | Time 0.0134(0.0170) | Loss 2.830238(2.922978)
Iter 69200 | Time 0.0149(0.0176) | Loss 3.000432(2.901313)
Iter 69300 | Time 0.0185(0.0185) | Loss 2.891581(2.926247)
Iter 69400 | Time 0.0171(0.0173) | Loss 2.915909(2.914600)
Iter 69500 | Time 0.0222(0.0175) | Loss 2.911933(2.907656)
Iter 69600 | Time 0.0148(0.0170) | Loss 3.029359(2.934301)
Iter 69700 | Time 0.0167(0.0200) | Loss 2.958504(2.901777)
Iter 69800 | Time 0.0141(0.0178) | Loss 3.091964(2.918834)
Iter 69900 | Time 0.0142(0.0161) | Loss 2.792029(2.915345)
Iter 70000 | Time 0.0167(0.0154) | Loss 2.874450(2.923575)
[TEST] Iter 70000 | Test Loss 2.906207
Iter 70100 | Time 0.0159(0.0157) | Loss 2.904020(2.911972)
Iter 70200 | Time 0.0184(0.0154) | Loss 2.908777(2.907753)
Iter 70300 | Time 0.0502(0.0172) | Loss 2.868613(2.914010)
Iter 70400 | Time 0.0147(0.0158) | Loss 2.926947(2.918056)
Iter 70500 | Time 0.0136(0.0149) | Loss 3.021588(2.922399)
Iter 70600 | Time 0.0138(0.0151) | Loss 2.829997(2.922024)
Iter 70700 | Time 0.0135(0.0145) | Loss 2.729768(2.936535)
Iter 70800 | Time 0.0152(0.0149) | Loss 2.964911(2.918353)
Iter 70900 | Time 0.0154(0.0150) | Loss 2.774805(2.911635)
Iter 71000 | Time 0.0153(0.0161) | Loss 2.933319(2.925622)
[TEST] Iter 71000 | Test Loss 2.944936
Iter 71100 | Time 0.0205(0.0169) | Loss 2.947934(2.923396)
Iter 71200 | Time 0.0154(0.0203) | Loss 2.663064(2.902831)
Iter 71300 | Time 0.0199(0.0191) | Loss 2.712458(2.915102)
Iter 71400 | Time 0.0177(0.0232) | Loss 2.936929(2.916513)
Iter 71500 | Time 0.0237(0.0250) | Loss 3.230819(2.905270)
Iter 71600 | Time 0.0420(0.0287) | Loss 2.780757(2.922504)
Iter 71700 | Time 0.0185(0.0198) | Loss 2.945584(2.918058)
Iter 71800 | Time 0.0293(0.0237) | Loss 2.800760(2.928293)
Iter 71900 | Time 0.0133(0.0354) | Loss 2.874689(2.914686)
Iter 72000 | Time 0.0155(0.0197) | Loss 3.052781(2.925830)
[TEST] Iter 72000 | Test Loss 2.986562
Iter 72100 | Time 0.0170(0.0178) | Loss 2.934372(2.906119)
Iter 72200 | Time 0.0161(0.0170) | Loss 2.925738(2.925962)
Iter 72300 | Time 0.0136(0.0171) | Loss 2.942404(2.921730)
Iter 72400 | Time 0.0267(0.0199) | Loss 2.876938(2.898997)
Iter 72500 | Time 0.0146(0.0233) | Loss 2.893905(2.938433)
Iter 72600 | Time 0.0273(0.0208) | Loss 2.949377(2.928446)
Iter 72700 | Time 0.0149(0.0170) | Loss 2.873851(2.918058)
Iter 72800 | Time 0.0250(0.0200) | Loss 2.883810(2.906828)
Iter 72900 | Time 0.0239(0.0201) | Loss 3.072649(2.917793)
Iter 73000 | Time 0.0193(0.0200) | Loss 2.806722(2.922498)
[TEST] Iter 73000 | Test Loss 2.941532
Iter 73100 | Time 0.0158(0.0176) | Loss 2.965576(2.917576)
Iter 73200 | Time 0.0178(0.0167) | Loss 2.977194(2.925884)
Iter 73300 | Time 0.0155(0.0197) | Loss 3.002093(2.914053)
Iter 73400 | Time 0.0166(0.0184) | Loss 2.803095(2.897522)
Iter 73500 | Time 0.0190(0.0179) | Loss 2.918964(2.893544)
Iter 73600 | Time 0.0163(0.0179) | Loss 2.782771(2.917541)
Iter 73700 | Time 0.0160(0.0200) | Loss 3.090190(2.936348)
Iter 73800 | Time 0.0285(0.0214) | Loss 2.815369(2.912551)
Iter 73900 | Time 0.0162(0.0177) | Loss 2.889513(2.923651)
Iter 74000 | Time 0.0171(0.0161) | Loss 2.996166(2.934254)
[TEST] Iter 74000 | Test Loss 2.963227
Iter 74100 | Time 0.0140(0.0160) | Loss 3.230075(2.940559)
Iter 74200 | Time 0.0182(0.0165) | Loss 3.040630(2.934331)
Iter 74300 | Time 0.0148(0.0169) | Loss 2.956745(2.908039)
Iter 74400 | Time 0.0162(0.0165) | Loss 2.962811(2.921646)
Iter 74500 | Time 0.0139(0.0165) | Loss 2.912107(2.913912)
Iter 74600 | Time 0.0166(0.0171) | Loss 3.001147(2.921925)
Iter 74700 | Time 0.0169(0.0175) | Loss 2.829470(2.907562)
Iter 74800 | Time 0.0177(0.0171) | Loss 2.780058(2.903268)
Iter 74900 | Time 0.0178(0.0167) | Loss 3.025400(2.928531)
Iter 75000 | Time 0.0134(0.0165) | Loss 2.933892(2.928119)
[TEST] Iter 75000 | Test Loss 2.916182
Iter 75100 | Time 0.0163(0.0159) | Loss 2.945852(2.920982)
Iter 75200 | Time 0.0128(0.0161) | Loss 2.784976(2.923162)
Iter 75300 | Time 0.0128(0.0163) | Loss 2.919895(2.895608)
Iter 75400 | Time 0.0138(0.0163) | Loss 2.820899(2.922110)
Iter 75500 | Time 0.0158(0.0200) | Loss 2.874977(2.905443)
Iter 75600 | Time 0.0163(0.0190) | Loss 2.956917(2.913697)
Iter 75700 | Time 0.0142(0.0164) | Loss 2.876032(2.912752)
Iter 75800 | Time 0.0168(0.0181) | Loss 2.955087(2.907869)
Iter 75900 | Time 0.0145(0.0180) | Loss 2.876438(2.915860)
Iter 76000 | Time 0.0157(0.0179) | Loss 2.927047(2.912091)
[TEST] Iter 76000 | Test Loss 2.899318
Iter 76100 | Time 0.0149(0.0172) | Loss 2.842428(2.909690)
Iter 76200 | Time 0.0147(0.0165) | Loss 3.120443(2.915349)
Iter 76300 | Time 0.0160(0.0298) | Loss 2.928616(2.925848)
Iter 76400 | Time 0.0148(0.0247) | Loss 2.918417(2.919284)
Iter 76500 | Time 0.0165(0.0174) | Loss 2.992592(2.925355)
Iter 76600 | Time 0.0187(0.0160) | Loss 2.937375(2.910870)
Iter 76700 | Time 0.0158(0.0171) | Loss 2.926035(2.916258)
Iter 76800 | Time 0.0138(0.0165) | Loss 2.873522(2.927687)
Iter 76900 | Time 0.0157(0.0162) | Loss 2.912249(2.916972)
Iter 77000 | Time 0.0148(0.0165) | Loss 2.987012(2.929735)
[TEST] Iter 77000 | Test Loss 2.940589
Iter 77100 | Time 0.1116(0.0318) | Loss 2.933146(2.936858)
Iter 77200 | Time 0.0163(0.0192) | Loss 2.866714(2.936203)
Iter 77300 | Time 0.0139(0.0599) | Loss 2.973822(2.909203)
Iter 77400 | Time 0.0156(0.0228) | Loss 2.859160(2.919657)
Iter 77500 | Time 0.0141(0.0169) | Loss 2.890430(2.926022)
Iter 77600 | Time 0.0178(0.0160) | Loss 2.838359(2.918797)
Iter 77700 | Time 0.0157(0.0311) | Loss 2.853550(2.904382)
Iter 77800 | Time 0.0166(0.0250) | Loss 2.823829(2.921426)
Iter 77900 | Time 0.0176(0.0312) | Loss 2.745690(2.896513)
Iter 78000 | Time 0.0151(0.0343) | Loss 3.027732(2.920498)
[TEST] Iter 78000 | Test Loss 2.937007
Iter 78100 | Time 0.0300(0.0446) | Loss 3.028492(2.923924)
Iter 78200 | Time 0.0482(0.0498) | Loss 3.003082(2.931168)
Iter 78300 | Time 0.0171(0.0217) | Loss 3.108570(2.913121)
Iter 78400 | Time 0.0153(0.0297) | Loss 2.807280(2.914172)
Iter 78500 | Time 0.0143(0.0179) | Loss 2.940100(2.912885)
Iter 78600 | Time 0.0143(0.0157) | Loss 3.006972(2.938227)
Iter 78700 | Time 0.0161(0.0152) | Loss 3.011649(2.933229)
Iter 78800 | Time 0.0145(0.0156) | Loss 2.847679(2.928728)
Iter 78900 | Time 0.0148(0.0156) | Loss 3.240952(2.936115)
Iter 79000 | Time 0.0178(0.0154) | Loss 2.951771(2.915560)
[TEST] Iter 79000 | Test Loss 2.943205
Iter 79100 | Time 0.0160(0.0283) | Loss 2.887122(2.917510)
Iter 79200 | Time 0.0165(0.0177) | Loss 2.789750(2.913814)
Iter 79300 | Time 0.0150(0.0153) | Loss 2.940923(2.938505)
Iter 79400 | Time 0.0147(0.0190) | Loss 2.886755(2.916888)
Iter 79500 | Time 0.0133(0.0166) | Loss 2.883203(2.916703)
Iter 79600 | Time 0.0138(0.0155) | Loss 3.012780(2.931822)
Iter 79700 | Time 0.0159(0.0160) | Loss 2.977277(2.917477)
Iter 79800 | Time 0.0177(0.0164) | Loss 2.959879(2.928154)
Iter 79900 | Time 0.0154(0.0155) | Loss 2.845205(2.924313)
Iter 80000 | Time 0.0157(0.0153) | Loss 2.866931(2.935923)
[TEST] Iter 80000 | Test Loss 2.864532
Iter 80100 | Time 0.0148(0.0152) | Loss 3.030342(2.942921)
Iter 80200 | Time 0.0166(0.0161) | Loss 2.902936(2.906268)
Iter 80300 | Time 0.0177(0.0185) | Loss 2.871603(2.908596)
Iter 80400 | Time 0.0171(0.0204) | Loss 2.825145(2.913419)
Iter 80500 | Time 0.0175(0.0194) | Loss 2.919425(2.940210)
Iter 80600 | Time 0.0169(0.0396) | Loss 3.083671(2.942988)
Iter 80700 | Time 0.0221(0.0411) | Loss 2.977740(2.921172)
Iter 80800 | Time 0.0463(0.0280) | Loss 2.954319(2.928243)
Iter 80900 | Time 0.0351(0.0536) | Loss 3.106051(2.938823)
Iter 81000 | Time 0.0171(0.0422) | Loss 2.939676(2.924189)
[TEST] Iter 81000 | Test Loss 2.936283
Iter 81100 | Time 0.0169(0.0219) | Loss 2.841749(2.922298)
Iter 81200 | Time 0.0121(0.0184) | Loss 2.845583(2.911747)
Iter 81300 | Time 0.0142(0.0221) | Loss 3.007311(2.907274)
Iter 81400 | Time 0.0197(0.0175) | Loss 2.846014(2.908903)
Iter 81500 | Time 0.0152(0.0158) | Loss 2.826504(2.913602)
Iter 81600 | Time 0.0152(0.0151) | Loss 2.955889(2.928779)
Iter 81700 | Time 0.0135(0.0161) | Loss 2.950465(2.906624)
Iter 81800 | Time 0.0187(0.0211) | Loss 2.926742(2.925579)
Iter 81900 | Time 0.0151(0.0183) | Loss 2.939678(2.936251)
Iter 82000 | Time 0.0157(0.0170) | Loss 2.814181(2.910718)
[TEST] Iter 82000 | Test Loss 2.926678
Iter 82100 | Time 0.0166(0.0166) | Loss 2.740597(2.896926)
Iter 82200 | Time 0.0156(0.0170) | Loss 2.907227(2.919814)
Iter 82300 | Time 0.0149(0.0165) | Loss 2.891562(2.909617)
Iter 82400 | Time 0.0185(0.0172) | Loss 2.903904(2.924878)
Iter 82500 | Time 0.0161(0.0168) | Loss 3.023771(2.913567)
Iter 82600 | Time 0.0183(0.0167) | Loss 2.888515(2.907864)
Iter 82700 | Time 0.0176(0.0165) | Loss 2.957003(2.916405)
Iter 82800 | Time 0.0153(0.0167) | Loss 2.884010(2.912595)
Iter 82900 | Time 0.0164(0.0157) | Loss 3.031006(2.923791)
Iter 83000 | Time 0.0169(0.0164) | Loss 2.860026(2.923792)
[TEST] Iter 83000 | Test Loss 2.942400
Iter 83100 | Time 0.0145(0.0161) | Loss 2.937152(2.914820)
Iter 83200 | Time 0.0165(0.0161) | Loss 2.736650(2.904328)
Iter 83300 | Time 0.0151(0.0161) | Loss 2.917742(2.907636)
Iter 83400 | Time 0.0153(0.0173) | Loss 3.082184(2.920717)
Iter 83500 | Time 0.0151(0.0165) | Loss 3.014443(2.922840)
Iter 83600 | Time 0.0142(0.0161) | Loss 3.039666(2.929694)
Iter 83700 | Time 0.0145(0.0157) | Loss 2.915185(2.918096)
Iter 83800 | Time 0.0267(0.0225) | Loss 3.052191(2.923065)
Iter 83900 | Time 0.0141(0.0196) | Loss 2.892502(2.935155)
Iter 84000 | Time 0.0164(0.0166) | Loss 2.833873(2.921133)
[TEST] Iter 84000 | Test Loss 2.945951
Iter 84100 | Time 0.0137(0.0167) | Loss 2.785114(2.883462)
Iter 84200 | Time 0.0168(0.0182) | Loss 3.049580(2.921786)
Iter 84300 | Time 0.0178(0.0183) | Loss 2.874673(2.895426)
Iter 84400 | Time 0.0151(0.0169) | Loss 2.856803(2.915784)
Iter 84500 | Time 0.0157(0.0166) | Loss 2.983779(2.929265)
Iter 84600 | Time 0.0154(0.0162) | Loss 2.861379(2.921511)
Iter 84700 | Time 0.0165(0.0165) | Loss 2.851390(2.911456)
Iter 84800 | Time 0.0185(0.0204) | Loss 3.118716(2.927463)
Iter 84900 | Time 0.0161(0.0179) | Loss 2.905697(2.922125)
Iter 85000 | Time 0.0161(0.0162) | Loss 2.878187(2.914252)
[TEST] Iter 85000 | Test Loss 2.993499
Iter 85100 | Time 0.0149(0.0171) | Loss 2.823094(2.900396)
Iter 85200 | Time 0.0179(0.0164) | Loss 2.890029(2.914032)
Iter 85300 | Time 0.0159(0.0159) | Loss 2.988102(2.904006)
Iter 85400 | Time 0.0149(0.0159) | Loss 2.822530(2.906129)
Iter 85500 | Time 0.0134(0.0154) | Loss 3.079940(2.922769)
Iter 85600 | Time 0.0161(0.0154) | Loss 2.920178(2.907613)
Iter 85700 | Time 0.0216(0.0184) | Loss 2.869559(2.907270)
Iter 85800 | Time 0.0237(0.0203) | Loss 2.797529(2.902956)
Iter 85900 | Time 0.0170(0.0182) | Loss 2.996420(2.901677)
Iter 86000 | Time 0.0175(0.0177) | Loss 2.946012(2.911204)
[TEST] Iter 86000 | Test Loss 2.906565
Iter 86100 | Time 0.0140(0.0165) | Loss 2.889564(2.904888)
Iter 86200 | Time 0.0157(0.0171) | Loss 2.678817(2.902446)
Iter 86300 | Time 0.0158(0.0172) | Loss 2.778487(2.896204)
Iter 86400 | Time 0.0215(0.0159) | Loss 2.795439(2.912487)
Iter 86500 | Time 0.0159(0.0160) | Loss 2.861533(2.921437)
Iter 86600 | Time 0.0272(0.0171) | Loss 3.103478(2.894255)
Iter 86700 | Time 0.0147(0.0168) | Loss 2.944788(2.928072)
Iter 86800 | Time 0.0269(0.0174) | Loss 2.877371(2.918691)
Iter 86900 | Time 0.0148(0.0180) | Loss 2.898930(2.899492)
Iter 87000 | Time 0.0168(0.0166) | Loss 2.995154(2.901787)
[TEST] Iter 87000 | Test Loss 2.951138
Iter 87100 | Time 0.0157(0.0164) | Loss 2.802971(2.918230)
Iter 87200 | Time 0.0126(0.0161) | Loss 3.029130(2.892356)
Iter 87300 | Time 0.0176(0.0153) | Loss 2.873618(2.915765)
Iter 87400 | Time 0.0327(0.0171) | Loss 2.793648(2.916571)
Iter 87500 | Time 0.0154(0.0172) | Loss 2.932228(2.901642)
Iter 87600 | Time 0.0158(0.0153) | Loss 2.857396(2.910754)
Iter 87700 | Time 0.0152(0.0164) | Loss 2.925653(2.917738)
Iter 87800 | Time 0.0169(0.0190) | Loss 2.907892(2.920122)
Iter 87900 | Time 0.0169(0.0181) | Loss 2.732769(2.893411)
Iter 88000 | Time 0.0153(0.0166) | Loss 2.871066(2.910297)
[TEST] Iter 88000 | Test Loss 2.905987
Iter 88100 | Time 0.0130(0.0153) | Loss 2.859005(2.908830)
Iter 88200 | Time 0.0139(0.0153) | Loss 2.819992(2.914857)
Iter 88300 | Time 0.0131(0.0154) | Loss 2.781318(2.905604)
Iter 88400 | Time 0.0147(0.0151) | Loss 2.896705(2.917230)
Iter 88500 | Time 0.0141(0.0160) | Loss 2.978276(2.922010)
Iter 88600 | Time 0.0167(0.0157) | Loss 2.884246(2.903491)
Iter 88700 | Time 0.0215(0.0157) | Loss 2.951365(2.917529)
Iter 88800 | Time 0.0147(0.0190) | Loss 2.719749(2.912199)
Iter 88900 | Time 0.0635(0.0240) | Loss 2.849782(2.922057)
Iter 89000 | Time 0.0203(0.0278) | Loss 2.739552(2.910815)
[TEST] Iter 89000 | Test Loss 2.932953
Iter 89100 | Time 0.0199(0.0209) | Loss 2.879094(2.911076)
Iter 89200 | Time 0.0548(0.0211) | Loss 2.927607(2.918413)
Iter 89300 | Time 0.0180(0.0187) | Loss 2.940282(2.922122)
Iter 89400 | Time 0.0246(0.0197) | Loss 2.900905(2.908527)
Iter 89500 | Time 0.0162(0.0215) | Loss 2.976398(2.901989)
Iter 89600 | Time 0.0155(0.0176) | Loss 2.802871(2.904952)
Iter 89700 | Time 0.0170(0.0167) | Loss 2.918895(2.895392)
Iter 89800 | Time 0.0162(0.0173) | Loss 3.042464(2.922615)
Iter 89900 | Time 0.0189(0.0169) | Loss 2.854317(2.929200)
Iter 90000 | Time 0.0147(0.0194) | Loss 2.759114(2.900290)
[TEST] Iter 90000 | Test Loss 2.878444
Iter 90100 | Time 0.0158(0.0218) | Loss 3.147927(2.907429)
Iter 90200 | Time 0.0163(0.0228) | Loss 2.980080(2.898660)
Iter 90300 | Time 0.0170(0.0183) | Loss 2.836770(2.909564)
Iter 90400 | Time 0.0168(0.0171) | Loss 3.108915(2.891597)
Iter 90500 | Time 0.0171(0.0196) | Loss 2.954470(2.892064)
Iter 90600 | Time 0.0162(0.0189) | Loss 2.880189(2.902306)
Iter 90700 | Time 0.0147(0.0273) | Loss 2.831774(2.898916)
Iter 90800 | Time 0.0149(0.0199) | Loss 2.922297(2.922951)
Iter 90900 | Time 0.0183(0.0199) | Loss 2.792344(2.924045)
Iter 91000 | Time 0.0141(0.0274) | Loss 2.924176(2.917832)
[TEST] Iter 91000 | Test Loss 2.901001
Iter 91100 | Time 0.2587(0.0620) | Loss 2.858432(2.911972)
Iter 91200 | Time 0.0157(0.0244) | Loss 2.794180(2.918625)
Iter 91300 | Time 0.0165(0.0191) | Loss 2.799037(2.924339)
Iter 91400 | Time 0.0193(0.0185) | Loss 2.896436(2.900245)
Iter 91500 | Time 0.0136(0.0205) | Loss 2.949016(2.908014)
Iter 91600 | Time 0.0180(0.0239) | Loss 2.871345(2.903677)
Iter 91700 | Time 0.0178(0.0225) | Loss 2.843290(2.895105)
Iter 91800 | Time 0.0141(0.0168) | Loss 3.231555(2.927133)
Iter 91900 | Time 0.0159(0.0193) | Loss 2.909275(2.912315)
Iter 92000 | Time 0.0149(0.0161) | Loss 3.004338(2.897499)
[TEST] Iter 92000 | Test Loss 2.910671
Iter 92100 | Time 0.0141(0.0166) | Loss 2.966557(2.916328)
Iter 92200 | Time 0.0145(0.0155) | Loss 2.979285(2.895196)
Iter 92300 | Time 0.0174(0.0173) | Loss 2.870567(2.907021)
Iter 92400 | Time 0.0168(0.0168) | Loss 2.759730(2.908421)
Iter 92500 | Time 0.0190(0.0194) | Loss 2.902594(2.903029)
Iter 92600 | Time 0.0156(0.0175) | Loss 2.937454(2.913713)
Iter 92700 | Time 0.0139(0.0165) | Loss 2.779962(2.909097)
Iter 92800 | Time 0.0138(0.0170) | Loss 2.941810(2.921476)
Iter 92900 | Time 0.0279(0.0185) | Loss 3.021482(2.905310)
Iter 93000 | Time 0.0155(0.0199) | Loss 2.789207(2.917036)
[TEST] Iter 93000 | Test Loss 2.932678
Iter 93100 | Time 0.0228(0.0169) | Loss 2.748545(2.931857)
Iter 93200 | Time 0.0179(0.0325) | Loss 3.101425(2.937113)
Iter 93300 | Time 0.0177(0.0263) | Loss 2.909191(2.918037)
Iter 93400 | Time 0.0624(0.0275) | Loss 3.003264(2.910533)
Iter 93500 | Time 0.0457(0.0445) | Loss 2.829675(2.904614)
Iter 93600 | Time 0.0195(0.0240) | Loss 2.994483(2.910722)
Iter 93700 | Time 0.0184(0.0179) | Loss 2.893119(2.911810)
Iter 93800 | Time 0.0145(0.0168) | Loss 2.776293(2.904783)
Iter 93900 | Time 0.0162(0.0166) | Loss 2.930634(2.923584)
Iter 94000 | Time 0.0167(0.0182) | Loss 2.723250(2.901381)
[TEST] Iter 94000 | Test Loss 2.850298
Iter 94100 | Time 0.0237(0.0182) | Loss 2.906482(2.917921)
Iter 94200 | Time 0.0309(0.0201) | Loss 2.903712(2.920900)
Iter 94300 | Time 0.0241(0.0208) | Loss 3.045289(2.918163)
Iter 94400 | Time 0.0171(0.0178) | Loss 2.868275(2.904012)
Iter 94500 | Time 0.0193(0.0388) | Loss 2.783457(2.901280)
Iter 94600 | Time 0.0209(0.0243) | Loss 2.886975(2.905332)
Iter 94700 | Time 0.0425(0.0238) | Loss 2.902761(2.914321)
Iter 94800 | Time 0.0236(0.0228) | Loss 3.065284(2.910903)
Iter 94900 | Time 0.0223(0.0287) | Loss 2.885782(2.919167)
Iter 95000 | Time 0.0192(0.0251) | Loss 3.012891(2.895942)
[TEST] Iter 95000 | Test Loss 2.934198
Iter 95100 | Time 0.0162(0.0269) | Loss 2.874158(2.900248)
Iter 95200 | Time 0.0216(0.0229) | Loss 2.914275(2.911409)
Iter 95300 | Time 0.0231(0.0242) | Loss 2.937788(2.918837)
Iter 95400 | Time 0.0278(0.0221) | Loss 3.118431(2.909620)
Iter 95500 | Time 0.1581(0.0278) | Loss 2.749468(2.912912)
Iter 95600 | Time 0.0268(0.0229) | Loss 3.138416(2.909706)
Iter 95700 | Time 0.0239(0.0234) | Loss 2.981179(2.904552)
Iter 95800 | Time 0.0331(0.0209) | Loss 2.842242(2.895482)
Iter 95900 | Time 0.0163(0.0194) | Loss 2.807240(2.906391)
Iter 96000 | Time 0.0205(0.0193) | Loss 2.954872(2.918581)
[TEST] Iter 96000 | Test Loss 2.883315
Iter 96100 | Time 0.0193(0.0192) | Loss 2.881815(2.920173)
Iter 96200 | Time 0.0176(0.0218) | Loss 2.785218(2.915913)
Iter 96300 | Time 0.0242(0.0205) | Loss 2.938491(2.906697)
Iter 96400 | Time 0.0292(0.0207) | Loss 3.047060(2.896018)
Iter 96500 | Time 0.0153(0.0247) | Loss 2.905367(2.905198)
Iter 96600 | Time 0.0624(0.0284) | Loss 2.994861(2.916903)
Iter 96700 | Time 0.0146(0.0203) | Loss 2.782115(2.913582)
Iter 96800 | Time 0.0266(0.0232) | Loss 2.797126(2.920075)
Iter 96900 | Time 0.0201(0.0391) | Loss 3.092749(2.905145)
Iter 97000 | Time 0.0249(0.0272) | Loss 2.991039(2.912130)
[TEST] Iter 97000 | Test Loss 2.874851
Iter 97100 | Time 0.0163(0.0224) | Loss 3.020915(2.913936)
Iter 97200 | Time 0.0194(0.0214) | Loss 2.810932(2.913958)
Iter 97300 | Time 0.0206(0.0201) | Loss 2.928570(2.912622)
Iter 97400 | Time 0.0166(0.0187) | Loss 2.792212(2.892510)
Iter 97500 | Time 0.0227(0.0196) | Loss 2.891812(2.890130)
Iter 97600 | Time 0.0299(0.0251) | Loss 2.842736(2.899215)
Iter 97700 | Time 0.0218(0.0230) | Loss 2.905419(2.906060)
Iter 97800 | Time 0.0200(0.0239) | Loss 2.936914(2.900765)
Iter 97900 | Time 0.0162(0.0255) | Loss 2.949534(2.911798)
Iter 98000 | Time 0.0206(0.0203) | Loss 2.900555(2.905992)
[TEST] Iter 98000 | Test Loss 2.929425
Iter 98100 | Time 0.0166(0.0181) | Loss 2.916493(2.916838)
Iter 98200 | Time 0.0154(0.0179) | Loss 3.001266(2.931992)
Iter 98300 | Time 0.0184(0.0221) | Loss 2.875798(2.917755)
Iter 98400 | Time 0.0498(0.0196) | Loss 2.849095(2.914977)
Iter 98500 | Time 0.0175(0.0209) | Loss 2.930821(2.916104)
Iter 98600 | Time 0.0279(0.0396) | Loss 2.895338(2.922569)
Iter 98700 | Time 0.0285(0.0300) | Loss 2.897153(2.900451)
Iter 98800 | Time 0.0145(0.0203) | Loss 2.888999(2.892675)
Iter 98900 | Time 0.0168(0.0187) | Loss 2.935079(2.910797)
Iter 99000 | Time 0.0172(0.0174) | Loss 2.950912(2.893978)
[TEST] Iter 99000 | Test Loss 2.897493
Iter 99100 | Time 0.0136(0.0205) | Loss 2.856095(2.909241)
Iter 99200 | Time 0.0173(0.0178) | Loss 3.025550(2.894291)
Iter 99300 | Time 0.0155(0.0197) | Loss 3.026011(2.926110)
Iter 99400 | Time 0.0145(0.0182) | Loss 2.691635(2.896196)
Iter 99500 | Time 0.0610(0.0262) | Loss 2.716596(2.883234)
Iter 99600 | Time 0.0227(0.0213) | Loss 2.817922(2.903699)
Iter 99700 | Time 0.0150(0.0214) | Loss 2.875108(2.925813)
Iter 99800 | Time 0.0202(0.0223) | Loss 2.999594(2.915119)
Iter 99900 | Time 0.0190(0.0228) | Loss 2.792691(2.913895)
Iter 100000 | Time 0.0443(0.0212) | Loss 2.975493(2.918000)
[TEST] Iter 100000 | Test Loss 2.903561
[TEST] Iter 100001 | Test Loss 2.844191
Training has finished.
/Users/bjzhang/gitrepos/tap-sgm/ffjord-master/train_toy.py
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import argparse
import os
import time

import torch
import torch.optim as optim

import lib.toy_data as toy_data
import lib.utils as utils
from lib.visualize_flow import visualize_transform
import lib.layers.odefunc as odefunc

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import build_model_tabular

from diagnostics.viz_toy import save_trajectory, trajectory_to_video

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'fixed_adams']
parser = argparse.ArgumentParser('Continuous Normalizing Flow')
parser.add_argument(
    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='pinwheel'
)
parser.add_argument(
    "--layer_type", type=str, default="concatsquash",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument('--dims', type=str, default='64-64-64')
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')
parser.add_argument('--time_length', type=float, default=0.5)
parser.add_argument('--train_T', type=eval, default=True)
parser.add_argument("--divergence_fn", type=str, default="brute_force", choices=["brute_force", "approximate"])
parser.add_argument("--nonlinearity", type=str, default="tanh", choices=odefunc.NONLINEARITIES)

parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=False, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--batch_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--bn_lag', type=float, default=0)

parser.add_argument('--niters', type=int, default=10000)
parser.add_argument('--batch_size', type=int, default=100)
parser.add_argument('--test_batch_size', type=int, default=1000)
parser.add_argument('--lr', type=float, default=1e-3)
parser.add_argument('--weight_decay', type=float, default=1e-5)

# Track quantities
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument('--save', type=str, default='experiments/cnf')
parser.add_argument('--viz_freq', type=int, default=100)
parser.add_argument('--val_freq', type=int, default=100)
parser.add_argument('--log_freq', type=int, default=10)
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')


def get_transforms(model):

    def sample_fn(z, logpz=None):
        if logpz is not None:
            return model(z, logpz, reverse=True)
        else:
            return model(z, reverse=True)

    def density_fn(x, logpx=None):
        if logpx is not None:
            return model(x, logpx, reverse=False)
        else:
            return model(x, reverse=False)

    return sample_fn, density_fn


def compute_loss(args, model, batch_size=None):
    if batch_size is None: batch_size = args.batch_size

    # load data
    x = toy_data.inf_train_gen(args.data, batch_size=batch_size)
    x = torch.from_numpy(x).type(torch.float32).to(device)
    zero = torch.zeros(x.shape[0], 1).to(x)

    # transform to z
    z, delta_logp = model(x, zero)

    # compute log q(z)
    logpz = standard_normal_logprob(z).sum(1, keepdim=True)

    logpx = logpz - delta_logp
    loss = -torch.mean(logpx)
    return loss


if __name__ == '__main__':

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, 2, regularization_fns).to(device)
    if args.spectral_norm: add_spectral_norm(model)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    end = time.time()
    best_loss = float('inf')
    model.train()
    for itr in range(1, args.niters + 1):
        optimizer.zero_grad()
        if args.spectral_norm: spectral_norm_power_iteration(model, 1)

        loss = compute_loss(args, model)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
            )
            loss = loss + reg_loss

        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)

        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            'Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) | NFE Forward {:.0f}({:.1f})'
            ' | NFE Backward {:.0f}({:.1f}) | CNF Time {:.4f}({:.4f})'.format(
                itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, nfef_meter.val, nfef_meter.avg,
                nfeb_meter.val, nfeb_meter.avg, tt_meter.val, tt_meter.avg
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)

        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                model.eval()
                test_loss = compute_loss(args, model, batch_size=args.test_batch_size)
                test_nfe = count_nfe(model)
                log_message = '[TEST] Iter {:04d} | Test Loss {:.6f} | NFE {:.0f}'.format(itr, test_loss, test_nfe)
                logger.info(log_message)

                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    utils.makedirs(args.save)
                    torch.save({
                        'args': args,
                        'state_dict': model.state_dict(),
                    }, os.path.join(args.save, 'checkpt.pth'))
                model.train()

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                model.eval()
                p_samples = toy_data.inf_train_gen(args.data, batch_size=2000)

                sample_fn, density_fn = get_transforms(model)

                plt.figure(figsize=(9, 3))
                visualize_transform(
                    p_samples, torch.randn, standard_normal_logprob, transform=sample_fn, inverse_transform=density_fn,
                    samples=True, npts=800, device=device
                )
                fig_filename = os.path.join(args.save, 'figs', '{:04d}.jpg'.format(itr))
                utils.makedirs(os.path.dirname(fig_filename))
                plt.savefig(fig_filename)
                plt.close()
                model.train()

        end = time.time()

    logger.info('Training has finished.')

    save_traj_dir = os.path.join(args.save, 'trajectory')
    logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = toy_data.inf_train_gen(args.data, batch_size=2000)
    save_trajectory(model, data_samples, save_traj_dir, device=device)
    trajectory_to_video(save_traj_dir)

Namespace(data='pinwheel', layer_type='concatsquash', dims='64-64-64', num_blocks=1, time_length=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=10000, batch_size=100, test_batch_size=1000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='experiments/cnf', viz_freq=100, val_freq=100, log_freq=10, gpu=0)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=2, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=2, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=2, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=2, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9225
Iter 0001 | Time 0.2389(0.2389) | Loss 4.175737(4.175737) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0002 | Time 0.1258(0.2310) | Loss 4.015407(4.164514) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0003 | Time 0.1419(0.2248) | Loss 4.084425(4.158908) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0004 | Time 0.1392(0.2188) | Loss 4.116217(4.155920) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0005 | Time 0.1591(0.2146) | Loss 3.843551(4.134054) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0006 | Time 0.1654(0.2112) | Loss 3.924026(4.119352) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0007 | Time 0.1482(0.2068) | Loss 3.850503(4.100532) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0008 | Time 0.1356(0.2018) | Loss 4.103516(4.100741) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0009 | Time 0.4938(0.2222) | Loss 4.094011(4.100270) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0010 | Time 0.1392(0.2164) | Loss 3.967548(4.090980) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0011 | Time 0.1480(0.2116) | Loss 3.953470(4.081354) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0012 | Time 0.1226(0.2054) | Loss 4.167248(4.087366) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0013 | Time 0.1654(0.2026) | Loss 3.821631(4.068765) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0014 | Time 0.1500(0.1989) | Loss 3.867145(4.054652) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0015 | Time 0.1436(0.1950) | Loss 4.167717(4.062566) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0016 | Time 0.1363(0.1909) | Loss 3.890300(4.050508) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0017 | Time 0.1407(0.1874) | Loss 3.827575(4.034902) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0018 | Time 0.1194(0.1826) | Loss 3.833036(4.020772) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0019 | Time 0.1620(0.1812) | Loss 3.862908(4.009721) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0020 | Time 0.1434(0.1786) | Loss 3.937043(4.004634) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0021 | Time 0.1632(0.1775) | Loss 3.919178(3.998652) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0022 | Time 0.1370(0.1747) | Loss 3.838925(3.987471) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0023 | Time 0.1557(0.1733) | Loss 3.644921(3.963492) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0024 | Time 0.1266(0.1701) | Loss 3.732529(3.947325) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0025 | Time 0.1581(0.1692) | Loss 3.909115(3.944650) | NFE Forward 20(14.4) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0026 | Time 0.1347(0.1668) | Loss 3.834271(3.936924) | NFE Forward 20(14.8) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0027 | Time 0.1821(0.1679) | Loss 3.864195(3.931833) | NFE Forward 20(15.2) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0028 | Time 0.1739(0.1683) | Loss 3.831734(3.924826) | NFE Forward 20(15.5) | NFE Backward 20(14.4) | CNF Time 0.5000(0.5000)
Iter 0029 | Time 0.2201(0.1719) | Loss 3.747014(3.912379) | NFE Forward 20(15.8) | NFE Backward 20(14.8) | CNF Time 0.5000(0.5000)
Iter 0030 | Time 0.1994(0.1738) | Loss 3.680512(3.896148) | NFE Forward 20(16.1) | NFE Backward 20(15.2) | CNF Time 0.5000(0.5000)
Iter 0031 | Time 0.1791(0.1742) | Loss 3.901613(3.896531) | NFE Forward 20(16.4) | NFE Backward 20(15.5) | CNF Time 0.5000(0.5000)
Iter 0032 | Time 0.2375(0.1786) | Loss 3.683931(3.881649) | NFE Forward 20(16.6) | NFE Backward 20(15.8) | CNF Time 0.5000(0.5000)
Iter 0033 | Time 0.1982(0.1800) | Loss 3.451755(3.851556) | NFE Forward 20(16.9) | NFE Backward 20(16.1) | CNF Time 0.5000(0.5000)
Iter 0034 | Time 0.2602(0.1856) | Loss 3.586614(3.833010) | NFE Forward 20(17.1) | NFE Backward 20(16.4) | CNF Time 0.5000(0.5000)
Iter 0035 | Time 0.1988(0.1865) | Loss 3.787968(3.829857) | NFE Forward 20(17.3) | NFE Backward 20(16.6) | CNF Time 0.5000(0.5000)
Iter 0036 | Time 0.2107(0.1882) | Loss 3.807451(3.828289) | NFE Forward 20(17.5) | NFE Backward 20(16.9) | CNF Time 0.5000(0.5000)
Iter 0037 | Time 0.2010(0.1891) | Loss 3.772305(3.824370) | NFE Forward 20(17.7) | NFE Backward 20(17.1) | CNF Time 0.5000(0.5000)
Iter 0038 | Time 0.1886(0.1891) | Loss 3.726130(3.817493) | NFE Forward 20(17.8) | NFE Backward 20(17.3) | CNF Time 0.5000(0.5000)
Iter 0039 | Time 0.2013(0.1899) | Loss 3.674942(3.807515) | NFE Forward 20(18.0) | NFE Backward 20(17.5) | CNF Time 0.5000(0.5000)
Iter 0040 | Time 0.1998(0.1906) | Loss 3.542269(3.788947) | NFE Forward 20(18.1) | NFE Backward 20(17.7) | CNF Time 0.5000(0.5000)
Iter 0041 | Time 0.2081(0.1919) | Loss 3.559999(3.772921) | NFE Forward 20(18.3) | NFE Backward 20(17.8) | CNF Time 0.5000(0.5000)
Iter 0042 | Time 0.1970(0.1922) | Loss 3.667212(3.765521) | NFE Forward 20(18.4) | NFE Backward 20(18.0) | CNF Time 0.5000(0.5000)
Iter 0043 | Time 0.2027(0.1929) | Loss 3.548285(3.750315) | NFE Forward 20(18.5) | NFE Backward 20(18.1) | CNF Time 0.5000(0.5000)
Iter 0044 | Time 0.1911(0.1928) | Loss 3.620743(3.741245) | NFE Forward 20(18.6) | NFE Backward 20(18.3) | CNF Time 0.5000(0.5000)
Iter 0045 | Time 0.1942(0.1929) | Loss 3.583318(3.730190) | NFE Forward 20(18.7) | NFE Backward 20(18.4) | CNF Time 0.5000(0.5000)
Iter 0046 | Time 0.2038(0.1937) | Loss 3.519541(3.715445) | NFE Forward 20(18.8) | NFE Backward 20(18.5) | CNF Time 0.5000(0.5000)
Iter 0047 | Time 0.1819(0.1928) | Loss 3.563237(3.704790) | NFE Forward 20(18.9) | NFE Backward 20(18.6) | CNF Time 0.5000(0.5000)
Iter 0048 | Time 0.2143(0.1944) | Loss 3.527167(3.692356) | NFE Forward 20(18.9) | NFE Backward 20(18.7) | CNF Time 0.5000(0.5000)
Iter 0049 | Time 0.1982(0.1946) | Loss 3.456237(3.675828) | NFE Forward 20(19.0) | NFE Backward 20(18.8) | CNF Time 0.5000(0.5000)
Iter 0050 | Time 0.1779(0.1935) | Loss 3.540310(3.666342) | NFE Forward 20(19.1) | NFE Backward 20(18.9) | CNF Time 0.5000(0.5000)
Iter 0051 | Time 0.2182(0.1952) | Loss 3.537065(3.657292) | NFE Forward 20(19.2) | NFE Backward 20(18.9) | CNF Time 0.5000(0.5000)
Iter 0052 | Time 0.2099(0.1962) | Loss 3.502392(3.646449) | NFE Forward 20(19.2) | NFE Backward 20(19.0) | CNF Time 0.5000(0.5000)
Iter 0053 | Time 0.1975(0.1963) | Loss 3.516059(3.637322) | NFE Forward 20(19.3) | NFE Backward 20(19.1) | CNF Time 0.5000(0.5000)
Iter 0054 | Time 0.2607(0.2008) | Loss 3.472272(3.625769) | NFE Forward 20(19.3) | NFE Backward 20(19.2) | CNF Time 0.5000(0.5000)
Iter 0055 | Time 0.2132(0.2017) | Loss 3.483462(3.615807) | NFE Forward 20(19.4) | NFE Backward 20(19.2) | CNF Time 0.5000(0.5000)
Iter 0056 | Time 0.2241(0.2032) | Loss 3.500817(3.607758) | NFE Forward 20(19.4) | NFE Backward 20(19.3) | CNF Time 0.5000(0.5000)
Iter 0057 | Time 0.1916(0.2024) | Loss 3.539594(3.602986) | NFE Forward 20(19.5) | NFE Backward 20(19.3) | CNF Time 0.5000(0.5000)
Iter 0058 | Time 0.2128(0.2032) | Loss 3.517603(3.597009) | NFE Forward 20(19.5) | NFE Backward 20(19.4) | CNF Time 0.5000(0.5000)
Iter 0059 | Time 0.2097(0.2036) | Loss 3.545796(3.593424) | NFE Forward 20(19.5) | NFE Backward 20(19.4) | CNF Time 0.5000(0.5000)
Iter 0060 | Time 0.1909(0.2027) | Loss 3.639335(3.596638) | NFE Forward 20(19.6) | NFE Backward 20(19.5) | CNF Time 0.5000(0.5000)
Iter 0061 | Time 0.2234(0.2042) | Loss 3.528627(3.591877) | NFE Forward 20(19.6) | NFE Backward 20(19.5) | CNF Time 0.5000(0.5000)
Iter 0062 | Time 0.2567(0.2078) | Loss 3.500369(3.585472) | NFE Forward 20(19.6) | NFE Backward 20(19.5) | CNF Time 0.5000(0.5000)
Iter 0063 | Time 0.2335(0.2096) | Loss 3.486563(3.578548) | NFE Forward 20(19.6) | NFE Backward 20(19.6) | CNF Time 0.5000(0.5000)
Iter 0064 | Time 0.1823(0.2077) | Loss 3.518244(3.574327) | NFE Forward 20(19.7) | NFE Backward 20(19.6) | CNF Time 0.5000(0.5000)
Iter 0065 | Time 0.2549(0.2110) | Loss 3.579278(3.574673) | NFE Forward 20(19.7) | NFE Backward 20(19.6) | CNF Time 0.5000(0.5000)
Iter 0066 | Time 0.2023(0.2104) | Loss 3.519546(3.570815) | NFE Forward 20(19.7) | NFE Backward 20(19.6) | CNF Time 0.5000(0.5000)
Iter 0067 | Time 0.1900(0.2090) | Loss 3.562949(3.570264) | NFE Forward 20(19.7) | NFE Backward 20(19.7) | CNF Time 0.5000(0.5000)
Iter 0068 | Time 0.2018(0.2085) | Loss 3.470211(3.563260) | NFE Forward 20(19.8) | NFE Backward 20(19.7) | CNF Time 0.5000(0.5000)
Iter 0069 | Time 0.1927(0.2074) | Loss 3.448247(3.555209) | NFE Forward 20(19.8) | NFE Backward 20(19.7) | CNF Time 0.5000(0.5000)
Iter 0070 | Time 0.1930(0.2064) | Loss 3.457960(3.548402) | NFE Forward 20(19.8) | NFE Backward 20(19.7) | CNF Time 0.5000(0.5000)
Iter 0071 | Time 0.2104(0.2067) | Loss 3.525666(3.546810) | NFE Forward 20(19.8) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0072 | Time 0.2109(0.2070) | Loss 3.497503(3.543359) | NFE Forward 20(19.8) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0073 | Time 0.1966(0.2062) | Loss 3.443167(3.536345) | NFE Forward 20(19.8) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0074 | Time 0.2003(0.2058) | Loss 3.540097(3.536608) | NFE Forward 20(19.8) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0075 | Time 0.1848(0.2043) | Loss 3.503524(3.534292) | NFE Forward 20(19.9) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0076 | Time 0.1806(0.2027) | Loss 3.465221(3.529457) | NFE Forward 20(19.9) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0077 | Time 0.2026(0.2027) | Loss 3.500501(3.527430) | NFE Forward 20(19.9) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0078 | Time 0.1926(0.2020) | Loss 3.463894(3.522983) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0079 | Time 0.1851(0.2008) | Loss 3.584189(3.527267) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0080 | Time 0.1928(0.2002) | Loss 3.475141(3.523618) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0081 | Time 0.2321(0.2024) | Loss 3.485722(3.520966) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0082 | Time 0.1721(0.2003) | Loss 3.586827(3.525576) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0083 | Time 0.2110(0.2011) | Loss 3.497185(3.523588) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0084 | Time 0.1933(0.2005) | Loss 3.434608(3.517360) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0085 | Time 0.1815(0.1992) | Loss 3.478798(3.514661) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0086 | Time 0.2156(0.2003) | Loss 3.501339(3.513728) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0087 | Time 0.2005(0.2004) | Loss 3.507254(3.513275) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0088 | Time 0.1932(0.1998) | Loss 3.573087(3.517462) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0089 | Time 0.1899(0.1991) | Loss 3.485320(3.515212) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0090 | Time 0.1778(0.1977) | Loss 3.577333(3.519560) | NFE Forward 20(20.0) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0091 | Time 0.2386(0.2005) | Loss 3.443672(3.514248) | NFE Forward 20(20.0) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0092 | Time 0.1986(0.2004) | Loss 3.431632(3.508465) | NFE Forward 20(20.0) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0093 | Time 0.2093(0.2010) | Loss 3.510691(3.508621) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0094 | Time 0.1661(0.1986) | Loss 3.503780(3.508282) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0095 | Time 0.1951(0.1983) | Loss 3.487221(3.506808) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0096 | Time 0.1753(0.1967) | Loss 3.463694(3.503790) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0097 | Time 0.1873(0.1960) | Loss 3.402258(3.496682) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0098 | Time 0.1985(0.1962) | Loss 3.471441(3.494916) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0099 | Time 0.1730(0.1946) | Loss 3.530133(3.497381) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0100 | Time 0.1967(0.1947) | Loss 3.416353(3.491709) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
[TEST] Iter 0100 | Test Loss 3.477859 | NFE 20
Iter 0101 | Time 0.2140(0.1961) | Loss 3.516887(3.493471) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0102 | Time 0.2418(0.1993) | Loss 3.486501(3.492983) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0103 | Time 0.2267(0.2012) | Loss 3.415286(3.487545) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0104 | Time 0.2590(0.2052) | Loss 3.462195(3.485770) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0105 | Time 0.2531(0.2086) | Loss 3.497474(3.486589) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0106 | Time 0.2615(0.2123) | Loss 3.436027(3.483050) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0107 | Time 0.2505(0.2150) | Loss 3.408206(3.477811) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0108 | Time 0.2088(0.2145) | Loss 3.485833(3.478372) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0109 | Time 0.2232(0.2151) | Loss 3.471497(3.477891) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0110 | Time 0.2553(0.2179) | Loss 3.415708(3.473538) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0111 | Time 0.2166(0.2179) | Loss 3.489724(3.474671) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0112 | Time 0.1987(0.2165) | Loss 3.478570(3.474944) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0113 | Time 0.2275(0.2173) | Loss 3.417668(3.470935) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0114 | Time 0.2261(0.2179) | Loss 3.402850(3.466169) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0115 | Time 0.2081(0.2172) | Loss 3.452883(3.465239) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0116 | Time 0.2167(0.2172) | Loss 3.410737(3.461424) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0117 | Time 0.2212(0.2175) | Loss 3.463673(3.461581) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0118 | Time 0.2427(0.2192) | Loss 3.514245(3.465268) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0119 | Time 0.1979(0.2177) | Loss 3.414717(3.461729) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0120 | Time 0.3931(0.2300) | Loss 3.493913(3.463982) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0121 | Time 0.2379(0.2306) | Loss 3.478543(3.465001) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0122 | Time 0.1884(0.2276) | Loss 3.358094(3.457518) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0123 | Time 0.2018(0.2258) | Loss 3.495787(3.460197) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0124 | Time 0.2224(0.2256) | Loss 3.469420(3.460842) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0125 | Time 0.2062(0.2242) | Loss 3.414941(3.457629) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0126 | Time 0.3173(0.2307) | Loss 3.506305(3.461037) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0127 | Time 0.2063(0.2290) | Loss 3.481536(3.462471) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0128 | Time 0.2579(0.2310) | Loss 3.485261(3.464067) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0129 | Time 0.2038(0.2291) | Loss 3.494399(3.466190) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0130 | Time 0.2265(0.2290) | Loss 3.423461(3.463199) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0131 | Time 0.2141(0.2279) | Loss 3.448657(3.462181) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0132 | Time 0.2250(0.2277) | Loss 3.342959(3.453835) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0133 | Time 0.1992(0.2257) | Loss 3.411215(3.450852) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0134 | Time 0.2192(0.2253) | Loss 3.441890(3.450225) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0135 | Time 0.2280(0.2255) | Loss 3.397954(3.446566) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0136 | Time 0.2355(0.2262) | Loss 3.460976(3.447574) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0137 | Time 0.2181(0.2256) | Loss 3.476183(3.449577) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0138 | Time 0.2245(0.2255) | Loss 3.340427(3.441937) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0139 | Time 0.2308(0.2259) | Loss 3.402126(3.439150) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0140 | Time 0.2218(0.2256) | Loss 3.407924(3.436964) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0141 | Time 0.2018(0.2239) | Loss 3.329247(3.429424) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0142 | Time 0.2264(0.2241) | Loss 3.335816(3.422871) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0143 | Time 0.2502(0.2259) | Loss 3.464075(3.425756) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0144 | Time 0.3730(0.2362) | Loss 3.410889(3.424715) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0145 | Time 0.2682(0.2385) | Loss 3.408948(3.423611) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0146 | Time 0.2743(0.2410) | Loss 3.413012(3.422869) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0147 | Time 0.2832(0.2439) | Loss 3.558583(3.432369) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0148 | Time 0.1833(0.2397) | Loss 3.438066(3.432768) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0149 | Time 0.2230(0.2385) | Loss 3.399160(3.430415) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0150 | Time 0.2239(0.2375) | Loss 3.367847(3.426036) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0151 | Time 0.2355(0.2374) | Loss 3.391573(3.423623) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0152 | Time 0.1980(0.2346) | Loss 3.424937(3.423715) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0153 | Time 0.2369(0.2348) | Loss 3.356177(3.418988) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0154 | Time 0.2778(0.2378) | Loss 3.312794(3.411554) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0155 | Time 0.2580(0.2392) | Loss 3.422394(3.412313) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0156 | Time 0.2421(0.2394) | Loss 3.363030(3.408863) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0157 | Time 0.2044(0.2369) | Loss 3.336647(3.403808) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0158 | Time 0.2160(0.2355) | Loss 3.409923(3.404236) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0159 | Time 0.2942(0.2396) | Loss 3.473405(3.409078) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0160 | Time 0.2777(0.2422) | Loss 3.333758(3.403805) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0161 | Time 0.2845(0.2452) | Loss 3.261350(3.393834) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0162 | Time 0.2440(0.2451) | Loss 3.439443(3.397026) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0163 | Time 0.1879(0.2411) | Loss 3.404807(3.397571) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0164 | Time 0.2266(0.2401) | Loss 3.385386(3.396718) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0165 | Time 0.2165(0.2384) | Loss 3.417053(3.398141) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0166 | Time 0.2335(0.2381) | Loss 3.418299(3.399552) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0167 | Time 0.1869(0.2345) | Loss 3.278907(3.391107) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0168 | Time 0.2239(0.2338) | Loss 3.360840(3.388989) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0169 | Time 0.2594(0.2356) | Loss 3.335742(3.385261) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0170 | Time 0.2259(0.2349) | Loss 3.222257(3.373851) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0171 | Time 0.2068(0.2329) | Loss 3.271115(3.366659) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0172 | Time 0.2246(0.2323) | Loss 3.349460(3.365456) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0173 | Time 0.2207(0.2315) | Loss 3.245214(3.357039) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0174 | Time 0.2033(0.2296) | Loss 3.287326(3.352159) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0175 | Time 0.2067(0.2280) | Loss 3.324172(3.350200) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0176 | Time 0.2088(0.2266) | Loss 3.347359(3.350001) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0177 | Time 0.2108(0.2255) | Loss 3.357779(3.350545) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0178 | Time 0.2070(0.2242) | Loss 3.296392(3.346755) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0179 | Time 0.2453(0.2257) | Loss 3.310524(3.344218) | NFE Forward 20(20.0) | NFE Backward 26(20.4) | CNF Time 0.5000(0.5000)
Iter 0180 | Time 0.2361(0.2264) | Loss 3.345576(3.344313) | NFE Forward 20(20.0) | NFE Backward 20(20.4) | CNF Time 0.5000(0.5000)
Iter 0181 | Time 0.2410(0.2274) | Loss 3.311259(3.342000) | NFE Forward 20(20.0) | NFE Backward 20(20.4) | CNF Time 0.5000(0.5000)
Iter 0182 | Time 0.2658(0.2301) | Loss 3.346353(3.342304) | NFE Forward 20(20.0) | NFE Backward 26(20.8) | CNF Time 0.5000(0.5000)
Iter 0183 | Time 0.2302(0.2301) | Loss 3.227737(3.334285) | NFE Forward 20(20.0) | NFE Backward 26(21.1) | CNF Time 0.5000(0.5000)
Iter 0184 | Time 0.2406(0.2309) | Loss 3.371522(3.336891) | NFE Forward 20(20.0) | NFE Backward 20(21.0) | CNF Time 0.5000(0.5000)
Iter 0185 | Time 0.2555(0.2326) | Loss 3.533442(3.350650) | NFE Forward 20(20.0) | NFE Backward 26(21.4) | CNF Time 0.5000(0.5000)
Iter 0186 | Time 0.1953(0.2300) | Loss 3.230359(3.342229) | NFE Forward 20(20.0) | NFE Backward 20(21.3) | CNF Time 0.5000(0.5000)
Iter 0187 | Time 0.2161(0.2290) | Loss 3.323942(3.340949) | NFE Forward 26(20.4) | NFE Backward 20(21.2) | CNF Time 0.5000(0.5000)
Iter 0188 | Time 0.2525(0.2307) | Loss 3.175907(3.329396) | NFE Forward 26(20.8) | NFE Backward 20(21.1) | CNF Time 0.5000(0.5000)
Iter 0189 | Time 0.1971(0.2283) | Loss 3.321307(3.328830) | NFE Forward 20(20.8) | NFE Backward 20(21.0) | CNF Time 0.5000(0.5000)
Iter 0190 | Time 0.3249(0.2351) | Loss 3.276740(3.325184) | NFE Forward 26(21.1) | NFE Backward 26(21.4) | CNF Time 0.5000(0.5000)
Iter 0191 | Time 0.2544(0.2364) | Loss 3.176451(3.314773) | NFE Forward 20(21.0) | NFE Backward 26(21.7) | CNF Time 0.5000(0.5000)
Iter 0192 | Time 0.3380(0.2435) | Loss 3.258499(3.310833) | NFE Forward 26(21.4) | NFE Backward 26(22.0) | CNF Time 0.5000(0.5000)
Iter 0193 | Time 0.2675(0.2452) | Loss 3.245712(3.306275) | NFE Forward 20(21.3) | NFE Backward 26(22.3) | CNF Time 0.5000(0.5000)
Iter 0194 | Time 0.2585(0.2461) | Loss 3.187325(3.297948) | NFE Forward 20(21.2) | NFE Backward 26(22.6) | CNF Time 0.5000(0.5000)
Iter 0195 | Time 0.2541(0.2467) | Loss 3.238024(3.293754) | NFE Forward 26(21.5) | NFE Backward 20(22.4) | CNF Time 0.5000(0.5000)
Iter 0196 | Time 0.2614(0.2477) | Loss 3.214013(3.288172) | NFE Forward 26(21.9) | NFE Backward 26(22.6) | CNF Time 0.5000(0.5000)
Iter 0197 | Time 0.2866(0.2504) | Loss 3.253559(3.285749) | NFE Forward 26(22.1) | NFE Backward 26(22.9) | CNF Time 0.5000(0.5000)
Iter 0198 | Time 0.2631(0.2513) | Loss 3.228414(3.281735) | NFE Forward 26(22.4) | NFE Backward 26(23.1) | CNF Time 0.5000(0.5000)
Iter 0199 | Time 0.2823(0.2535) | Loss 3.323366(3.284650) | NFE Forward 26(22.7) | NFE Backward 26(23.3) | CNF Time 0.5000(0.5000)
Iter 0200 | Time 0.2835(0.2556) | Loss 3.204545(3.279042) | NFE Forward 26(22.9) | NFE Backward 26(23.5) | CNF Time 0.5000(0.5000)
[TEST] Iter 0200 | Test Loss 3.294578 | NFE 20
Iter 0201 | Time 0.3178(0.2599) | Loss 3.320126(3.281918) | NFE Forward 32(23.5) | NFE Backward 26(23.7) | CNF Time 0.5000(0.5000)
Iter 0202 | Time 0.3789(0.2683) | Loss 3.213924(3.277159) | NFE Forward 32(24.1) | NFE Backward 26(23.8) | CNF Time 0.5000(0.5000)
Iter 0203 | Time 0.3836(0.2763) | Loss 3.259859(3.275948) | NFE Forward 32(24.7) | NFE Backward 26(24.0) | CNF Time 0.5000(0.5000)
Iter 0204 | Time 0.3386(0.2807) | Loss 3.239931(3.273427) | NFE Forward 32(25.2) | NFE Backward 26(24.1) | CNF Time 0.5000(0.5000)
Iter 0205 | Time 0.3023(0.2822) | Loss 3.237503(3.270912) | NFE Forward 32(25.7) | NFE Backward 26(24.2) | CNF Time 0.5000(0.5000)
Iter 0206 | Time 0.4678(0.2952) | Loss 3.357677(3.276985) | NFE Forward 38(26.5) | NFE Backward 26(24.4) | CNF Time 0.5000(0.5000)
Iter 0207 | Time 0.2685(0.2933) | Loss 3.212217(3.272452) | NFE Forward 26(26.5) | NFE Backward 26(24.5) | CNF Time 0.5000(0.5000)
Iter 0208 | Time 0.3797(0.2994) | Loss 3.161199(3.264664) | NFE Forward 32(26.9) | NFE Backward 32(25.0) | CNF Time 0.5000(0.5000)
Iter 0209 | Time 0.3665(0.3041) | Loss 3.224376(3.261844) | NFE Forward 38(27.7) | NFE Backward 26(25.1) | CNF Time 0.5000(0.5000)
Iter 0210 | Time 0.2749(0.3020) | Loss 3.309770(3.265199) | NFE Forward 32(28.0) | NFE Backward 26(25.1) | CNF Time 0.5000(0.5000)
Iter 0211 | Time 0.3679(0.3066) | Loss 3.180097(3.259241) | NFE Forward 32(28.2) | NFE Backward 26(25.2) | CNF Time 0.5000(0.5000)
Iter 0212 | Time 0.3579(0.3102) | Loss 3.167770(3.252839) | NFE Forward 32(28.5) | NFE Backward 26(25.3) | CNF Time 0.5000(0.5000)
Iter 0213 | Time 0.3235(0.3112) | Loss 3.240489(3.251974) | NFE Forward 32(28.8) | NFE Backward 26(25.3) | CNF Time 0.5000(0.5000)
Iter 0214 | Time 0.2981(0.3102) | Loss 3.136347(3.243880) | NFE Forward 32(29.0) | NFE Backward 26(25.4) | CNF Time 0.5000(0.5000)
Iter 0215 | Time 0.5560(0.3275) | Loss 3.220428(3.242238) | NFE Forward 38(29.6) | NFE Backward 26(25.4) | CNF Time 0.5000(0.5000)
Iter 0216 | Time 0.6872(0.3526) | Loss 3.253985(3.243061) | NFE Forward 32(29.8) | NFE Backward 26(25.4) | CNF Time 0.5000(0.5000)
Iter 0217 | Time 1.8174(0.4552) | Loss 3.396479(3.253800) | NFE Forward 32(29.9) | NFE Backward 26(25.5) | CNF Time 0.5000(0.5000)
Iter 0218 | Time 0.9477(0.4896) | Loss 3.159868(3.247225) | NFE Forward 32(30.1) | NFE Backward 32(25.9) | CNF Time 0.5000(0.5000)
Iter 0219 | Time 0.3561(0.4803) | Loss 3.234252(3.246317) | NFE Forward 32(30.2) | NFE Backward 26(25.9) | CNF Time 0.5000(0.5000)
Iter 0220 | Time 0.4689(0.4795) | Loss 3.252120(3.246723) | NFE Forward 32(30.3) | NFE Backward 26(25.9) | CNF Time 0.5000(0.5000)
Iter 0221 | Time 0.3556(0.4708) | Loss 3.184294(3.242353) | NFE Forward 32(30.5) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0222 | Time 0.4515(0.4695) | Loss 3.322705(3.247977) | NFE Forward 32(30.6) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0223 | Time 0.4717(0.4696) | Loss 3.213758(3.245582) | NFE Forward 38(31.1) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0224 | Time 0.5038(0.4720) | Loss 3.111138(3.236171) | NFE Forward 38(31.6) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0225 | Time 0.5321(0.4762) | Loss 3.238674(3.236346) | NFE Forward 32(31.6) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0226 | Time 0.5350(0.4803) | Loss 3.175792(3.232107) | NFE Forward 38(32.0) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0227 | Time 0.3048(0.4680) | Loss 3.239650(3.232635) | NFE Forward 32(32.0) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0228 | Time 0.3997(0.4633) | Loss 3.286924(3.236436) | NFE Forward 38(32.5) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0229 | Time 0.3812(0.4575) | Loss 3.280758(3.239538) | NFE Forward 32(32.4) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0230 | Time 0.3581(0.4506) | Loss 3.087531(3.228898) | NFE Forward 38(32.8) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0231 | Time 0.3199(0.4414) | Loss 3.127555(3.221804) | NFE Forward 32(32.8) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0232 | Time 0.2755(0.4298) | Loss 3.151803(3.216904) | NFE Forward 32(32.7) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0233 | Time 0.3030(0.4209) | Loss 3.358048(3.226784) | NFE Forward 38(33.1) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0234 | Time 0.3040(0.4127) | Loss 3.057106(3.214906) | NFE Forward 38(33.4) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0235 | Time 0.3523(0.4085) | Loss 3.033033(3.202175) | NFE Forward 38(33.7) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0236 | Time 0.5929(0.4214) | Loss 3.138038(3.197686) | NFE Forward 32(33.6) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0237 | Time 0.3085(0.4135) | Loss 3.148366(3.194233) | NFE Forward 32(33.5) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0238 | Time 0.3289(0.4076) | Loss 3.115823(3.188745) | NFE Forward 38(33.8) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0239 | Time 0.2800(0.3986) | Loss 3.112914(3.183436) | NFE Forward 32(33.7) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0240 | Time 0.3002(0.3918) | Loss 3.113594(3.178547) | NFE Forward 32(33.6) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0241 | Time 0.3127(0.3862) | Loss 3.108152(3.173620) | NFE Forward 38(33.9) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0242 | Time 0.3060(0.3806) | Loss 3.061156(3.165747) | NFE Forward 38(34.2) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0243 | Time 0.3359(0.3775) | Loss 3.267993(3.172904) | NFE Forward 38(34.4) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0244 | Time 0.2895(0.3713) | Loss 3.059226(3.164947) | NFE Forward 38(34.7) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0245 | Time 0.3227(0.3679) | Loss 2.943312(3.149433) | NFE Forward 32(34.5) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0246 | Time 0.3284(0.3652) | Loss 3.186715(3.152042) | NFE Forward 32(34.3) | NFE Backward 32(26.4) | CNF Time 0.5000(0.5000)
Iter 0247 | Time 0.3290(0.3626) | Loss 3.193326(3.154932) | NFE Forward 32(34.2) | NFE Backward 26(26.4) | CNF Time 0.5000(0.5000)
Iter 0248 | Time 0.3693(0.3631) | Loss 3.066538(3.148745) | NFE Forward 32(34.0) | NFE Backward 32(26.8) | CNF Time 0.5000(0.5000)
Iter 0249 | Time 0.2874(0.3578) | Loss 3.123601(3.146985) | NFE Forward 32(33.9) | NFE Backward 26(26.7) | CNF Time 0.5000(0.5000)
Iter 0250 | Time 0.3718(0.3588) | Loss 3.283460(3.156538) | NFE Forward 32(33.7) | NFE Backward 26(26.7) | CNF Time 0.5000(0.5000)
Iter 0251 | Time 0.6218(0.3772) | Loss 3.074957(3.150827) | NFE Forward 38(34.0) | NFE Backward 32(27.0) | CNF Time 0.5000(0.5000)
Iter 0252 | Time 0.4279(0.3807) | Loss 3.078518(3.145766) | NFE Forward 38(34.3) | NFE Backward 26(27.0) | CNF Time 0.5000(0.5000)
Iter 0253 | Time 1.4570(0.4561) | Loss 2.987548(3.134690) | NFE Forward 38(34.6) | NFE Backward 26(26.9) | CNF Time 0.5000(0.5000)
Iter 0254 | Time 1.4121(0.5230) | Loss 3.128944(3.134288) | NFE Forward 32(34.4) | NFE Backward 26(26.8) | CNF Time 0.5000(0.5000)
Iter 0255 | Time 0.3302(0.5095) | Loss 3.028537(3.126885) | NFE Forward 38(34.6) | NFE Backward 26(26.8) | CNF Time 0.5000(0.5000)
Iter 0256 | Time 0.4114(0.5026) | Loss 3.142614(3.127986) | NFE Forward 38(34.9) | NFE Backward 26(26.7) | CNF Time 0.5000(0.5000)
Iter 0257 | Time 0.4994(0.5024) | Loss 3.159080(3.130163) | NFE Forward 38(35.1) | NFE Backward 26(26.7) | CNF Time 0.5000(0.5000)
Iter 0258 | Time 0.4570(0.4992) | Loss 3.168078(3.132817) | NFE Forward 38(35.3) | NFE Backward 26(26.6) | CNF Time 0.5000(0.5000)
Iter 0259 | Time 0.4542(0.4961) | Loss 2.960459(3.120752) | NFE Forward 38(35.5) | NFE Backward 32(27.0) | CNF Time 0.5000(0.5000)
Iter 0260 | Time 0.3064(0.4828) | Loss 3.049869(3.115790) | NFE Forward 44(36.1) | NFE Backward 26(26.9) | CNF Time 0.5000(0.5000)
Iter 0261 | Time 0.4516(0.4806) | Loss 3.065855(3.112295) | NFE Forward 44(36.6) | NFE Backward 38(27.7) | CNF Time 0.5000(0.5000)
Iter 0262 | Time 0.6192(0.4903) | Loss 3.348483(3.128828) | NFE Forward 32(36.3) | NFE Backward 26(27.6) | CNF Time 0.5000(0.5000)
Iter 0263 | Time 0.3387(0.4797) | Loss 3.016657(3.120976) | NFE Forward 32(36.0) | NFE Backward 32(27.9) | CNF Time 0.5000(0.5000)
Iter 0264 | Time 0.4657(0.4787) | Loss 2.949180(3.108950) | NFE Forward 32(35.7) | NFE Backward 26(27.8) | CNF Time 0.5000(0.5000)
Iter 0265 | Time 0.4198(0.4746) | Loss 2.846035(3.090546) | NFE Forward 38(35.9) | NFE Backward 32(28.1) | CNF Time 0.5000(0.5000)
Iter 0266 | Time 0.3507(0.4659) | Loss 3.028754(3.086221) | NFE Forward 38(36.0) | NFE Backward 32(28.3) | CNF Time 0.5000(0.5000)
Iter 0267 | Time 0.4560(0.4652) | Loss 3.024353(3.081890) | NFE Forward 38(36.2) | NFE Backward 38(29.0) | CNF Time 0.5000(0.5000)
Iter 0268 | Time 0.3432(0.4567) | Loss 3.221767(3.091681) | NFE Forward 38(36.3) | NFE Backward 32(29.2) | CNF Time 0.5000(0.5000)
Iter 0269 | Time 0.4116(0.4535) | Loss 3.104503(3.092579) | NFE Forward 44(36.8) | NFE Backward 32(29.4) | CNF Time 0.5000(0.5000)
Iter 0270 | Time 0.3927(0.4493) | Loss 3.044426(3.089208) | NFE Forward 38(36.9) | NFE Backward 32(29.6) | CNF Time 0.5000(0.5000)
Iter 0271 | Time 0.3590(0.4430) | Loss 2.995866(3.082674) | NFE Forward 50(37.8) | NFE Backward 32(29.8) | CNF Time 0.5000(0.5000)
Iter 0272 | Time 0.4100(0.4406) | Loss 3.098134(3.083756) | NFE Forward 50(38.7) | NFE Backward 32(29.9) | CNF Time 0.5000(0.5000)
Iter 0273 | Time 0.6121(0.4526) | Loss 3.265804(3.096500) | NFE Forward 32(38.2) | NFE Backward 38(30.5) | CNF Time 0.5000(0.5000)
Iter 0274 | Time 0.5526(0.4596) | Loss 3.011762(3.090568) | NFE Forward 44(38.6) | NFE Backward 32(30.6) | CNF Time 0.5000(0.5000)
Iter 0275 | Time 0.4312(0.4577) | Loss 3.057836(3.088277) | NFE Forward 44(39.0) | NFE Backward 38(31.1) | CNF Time 0.5000(0.5000)
Iter 0276 | Time 0.7047(0.4750) | Loss 3.095618(3.088791) | NFE Forward 38(38.9) | NFE Backward 38(31.6) | CNF Time 0.5000(0.5000)
Iter 0277 | Time 0.4169(0.4709) | Loss 3.037020(3.085167) | NFE Forward 38(38.9) | NFE Backward 32(31.6) | CNF Time 0.5000(0.5000)
Iter 0278 | Time 0.5361(0.4754) | Loss 2.939586(3.074976) | NFE Forward 38(38.8) | NFE Backward 32(31.6) | CNF Time 0.5000(0.5000)
Iter 0279 | Time 0.6338(0.4865) | Loss 3.110901(3.077491) | NFE Forward 50(39.6) | NFE Backward 32(31.7) | CNF Time 0.5000(0.5000)
Iter 0280 | Time 0.7864(0.5075) | Loss 2.830476(3.060200) | NFE Forward 44(39.9) | NFE Backward 32(31.7) | CNF Time 0.5000(0.5000)
Iter 0281 | Time 0.4374(0.5026) | Loss 3.213459(3.070928) | NFE Forward 50(40.6) | NFE Backward 32(31.7) | CNF Time 0.5000(0.5000)
Iter 0282 | Time 0.5304(0.5046) | Loss 3.033328(3.068296) | NFE Forward 44(40.8) | NFE Backward 32(31.7) | CNF Time 0.5000(0.5000)
Iter 0283 | Time 0.4246(0.4990) | Loss 3.013351(3.064450) | NFE Forward 44(41.1) | NFE Backward 32(31.8) | CNF Time 0.5000(0.5000)
Iter 0284 | Time 0.3977(0.4919) | Loss 3.028820(3.061956) | NFE Forward 44(41.3) | NFE Backward 32(31.8) | CNF Time 0.5000(0.5000)
Iter 0285 | Time 0.4669(0.4901) | Loss 3.012020(3.058460) | NFE Forward 44(41.5) | NFE Backward 32(31.8) | CNF Time 0.5000(0.5000)
Iter 0286 | Time 0.3480(0.4802) | Loss 2.994902(3.054011) | NFE Forward 38(41.2) | NFE Backward 32(31.8) | CNF Time 0.5000(0.5000)
Iter 0287 | Time 0.8206(0.5040) | Loss 3.127625(3.059164) | NFE Forward 44(41.4) | NFE Backward 32(31.8) | CNF Time 0.5000(0.5000)
Iter 0288 | Time 0.5005(0.5038) | Loss 3.363303(3.080454) | NFE Forward 38(41.2) | NFE Backward 38(32.3) | CNF Time 0.5000(0.5000)
Iter 0289 | Time 0.4808(0.5022) | Loss 3.146558(3.085081) | NFE Forward 44(41.4) | NFE Backward 38(32.7) | CNF Time 0.5000(0.5000)
Iter 0290 | Time 0.4815(0.5007) | Loss 3.186059(3.092150) | NFE Forward 38(41.1) | NFE Backward 32(32.6) | CNF Time 0.5000(0.5000)
Iter 0291 | Time 0.5799(0.5063) | Loss 3.122025(3.094241) | NFE Forward 38(40.9) | NFE Backward 38(33.0) | CNF Time 0.5000(0.5000)
Iter 0292 | Time 0.5069(0.5063) | Loss 3.153439(3.098385) | NFE Forward 50(41.6) | NFE Backward 32(32.9) | CNF Time 0.5000(0.5000)
Iter 0293 | Time 0.3664(0.4965) | Loss 3.152926(3.102203) | NFE Forward 38(41.3) | NFE Backward 32(32.9) | CNF Time 0.5000(0.5000)
Iter 0294 | Time 0.4689(0.4946) | Loss 3.282609(3.114831) | NFE Forward 38(41.1) | NFE Backward 38(33.2) | CNF Time 0.5000(0.5000)
Iter 0295 | Time 0.3991(0.4879) | Loss 2.974107(3.104980) | NFE Forward 50(41.7) | NFE Backward 32(33.1) | CNF Time 0.5000(0.5000)
Iter 0296 | Time 0.3410(0.4776) | Loss 3.076125(3.102961) | NFE Forward 38(41.4) | NFE Backward 32(33.0) | CNF Time 0.5000(0.5000)
Iter 0297 | Time 0.4114(0.4730) | Loss 2.797441(3.081574) | NFE Forward 38(41.2) | NFE Backward 32(33.0) | CNF Time 0.5000(0.5000)
Iter 0298 | Time 0.4114(0.4687) | Loss 3.085316(3.081836) | NFE Forward 44(41.4) | NFE Backward 32(32.9) | CNF Time 0.5000(0.5000)
Iter 0299 | Time 0.4552(0.4677) | Loss 2.977237(3.074514) | NFE Forward 44(41.6) | NFE Backward 32(32.8) | CNF Time 0.5000(0.5000)
Iter 0300 | Time 0.4937(0.4695) | Loss 2.822181(3.056851) | NFE Forward 44(41.7) | NFE Backward 32(32.8) | CNF Time 0.5000(0.5000)
[TEST] Iter 0300 | Test Loss 2.919248 | NFE 38
Iter 0301 | Time 0.5035(0.4719) | Loss 2.940650(3.048717) | NFE Forward 44(41.9) | NFE Backward 32(32.7) | CNF Time 0.5000(0.5000)
Iter 0302 | Time 0.4737(0.4720) | Loss 2.991664(3.044723) | NFE Forward 44(42.1) | NFE Backward 32(32.7) | CNF Time 0.5000(0.5000)
Iter 0303 | Time 0.3750(0.4652) | Loss 2.920872(3.036054) | NFE Forward 44(42.2) | NFE Backward 32(32.6) | CNF Time 0.5000(0.5000)
Iter 0304 | Time 0.4618(0.4650) | Loss 2.809242(3.020177) | NFE Forward 44(42.3) | NFE Backward 32(32.6) | CNF Time 0.5000(0.5000)
Iter 0305 | Time 0.3904(0.4598) | Loss 3.151831(3.029393) | NFE Forward 44(42.4) | NFE Backward 38(33.0) | CNF Time 0.5000(0.5000)
Iter 0306 | Time 0.3867(0.4547) | Loss 2.904185(3.020628) | NFE Forward 50(43.0) | NFE Backward 32(32.9) | CNF Time 0.5000(0.5000)
Iter 0307 | Time 0.3903(0.4502) | Loss 3.062905(3.023587) | NFE Forward 44(43.0) | NFE Backward 38(33.3) | CNF Time 0.5000(0.5000)
Iter 0308 | Time 0.3938(0.4462) | Loss 2.985234(3.020903) | NFE Forward 38(42.7) | NFE Backward 32(33.2) | CNF Time 0.5000(0.5000)
Iter 0309 | Time 0.3475(0.4393) | Loss 2.874837(3.010678) | NFE Forward 38(42.4) | NFE Backward 32(33.1) | CNF Time 0.5000(0.5000)
Iter 0310 | Time 0.3559(0.4335) | Loss 2.807353(2.996445) | NFE Forward 44(42.5) | NFE Backward 32(33.0) | CNF Time 0.5000(0.5000)
Iter 0311 | Time 0.3931(0.4306) | Loss 2.886122(2.988723) | NFE Forward 44(42.6) | NFE Backward 32(32.9) | CNF Time 0.5000(0.5000)
Iter 0312 | Time 0.3551(0.4254) | Loss 2.818822(2.976830) | NFE Forward 44(42.7) | NFE Backward 32(32.9) | CNF Time 0.5000(0.5000)
Iter 0313 | Time 0.6832(0.4434) | Loss 3.023978(2.980130) | NFE Forward 50(43.2) | NFE Backward 44(33.7) | CNF Time 0.5000(0.5000)
Iter 0314 | Time 0.3392(0.4361) | Loss 3.140879(2.991383) | NFE Forward 38(42.8) | NFE Backward 32(33.5) | CNF Time 0.5000(0.5000)
Iter 0315 | Time 0.4281(0.4356) | Loss 2.863600(2.982438) | NFE Forward 44(42.9) | NFE Backward 32(33.4) | CNF Time 0.5000(0.5000)
Iter 0316 | Time 0.3880(0.4322) | Loss 2.982076(2.982412) | NFE Forward 38(42.6) | NFE Backward 32(33.3) | CNF Time 0.5000(0.5000)
Iter 0317 | Time 0.3810(0.4286) | Loss 3.001722(2.983764) | NFE Forward 44(42.7) | NFE Backward 32(33.2) | CNF Time 0.5000(0.5000)
Iter 0318 | Time 0.4501(0.4301) | Loss 2.994447(2.984512) | NFE Forward 44(42.8) | NFE Backward 38(33.6) | CNF Time 0.5000(0.5000)
Iter 0319 | Time 0.3664(0.4257) | Loss 2.911503(2.979401) | NFE Forward 38(42.4) | NFE Backward 38(33.9) | CNF Time 0.5000(0.5000)
Iter 0320 | Time 0.4214(0.4254) | Loss 2.950794(2.977399) | NFE Forward 50(43.0) | NFE Backward 38(34.2) | CNF Time 0.5000(0.5000)
Iter 0321 | Time 0.4560(0.4275) | Loss 2.999332(2.978934) | NFE Forward 50(43.4) | NFE Backward 32(34.0) | CNF Time 0.5000(0.5000)
Iter 0322 | Time 0.4075(0.4261) | Loss 2.982517(2.979185) | NFE Forward 50(43.9) | NFE Backward 38(34.3) | CNF Time 0.5000(0.5000)
Iter 0323 | Time 0.4354(0.4268) | Loss 2.764101(2.964129) | NFE Forward 44(43.9) | NFE Backward 38(34.6) | CNF Time 0.5000(0.5000)
Iter 0324 | Time 0.3783(0.4234) | Loss 3.051284(2.970230) | NFE Forward 44(43.9) | NFE Backward 32(34.4) | CNF Time 0.5000(0.5000)
Iter 0325 | Time 0.3997(0.4217) | Loss 3.001770(2.972438) | NFE Forward 44(43.9) | NFE Backward 32(34.2) | CNF Time 0.5000(0.5000)
Iter 0326 | Time 0.4201(0.4216) | Loss 3.097375(2.981183) | NFE Forward 56(44.8) | NFE Backward 32(34.1) | CNF Time 0.5000(0.5000)
Iter 0327 | Time 0.5441(0.4302) | Loss 3.152189(2.993154) | NFE Forward 44(44.7) | NFE Backward 38(34.3) | CNF Time 0.5000(0.5000)
Iter 0328 | Time 0.4728(0.4332) | Loss 2.906904(2.987116) | NFE Forward 50(45.1) | NFE Backward 38(34.6) | CNF Time 0.5000(0.5000)
Iter 0329 | Time 0.4771(0.4362) | Loss 2.967066(2.985713) | NFE Forward 50(45.4) | NFE Backward 44(35.2) | CNF Time 0.5000(0.5000)
Iter 0330 | Time 0.4609(0.4380) | Loss 2.788177(2.971885) | NFE Forward 50(45.8) | NFE Backward 44(35.9) | CNF Time 0.5000(0.5000)
Iter 0331 | Time 0.3861(0.4343) | Loss 2.864381(2.964360) | NFE Forward 44(45.6) | NFE Backward 32(35.6) | CNF Time 0.5000(0.5000)
Iter 0332 | Time 0.5234(0.4406) | Loss 3.028373(2.968841) | NFE Forward 44(45.5) | NFE Backward 32(35.3) | CNF Time 0.5000(0.5000)
Iter 0333 | Time 0.5137(0.4457) | Loss 2.814188(2.958015) | NFE Forward 44(45.4) | NFE Backward 38(35.5) | CNF Time 0.5000(0.5000)
Iter 0334 | Time 0.3676(0.4402) | Loss 2.857726(2.950995) | NFE Forward 50(45.7) | NFE Backward 32(35.3) | CNF Time 0.5000(0.5000)
Iter 0335 | Time 0.3810(0.4361) | Loss 3.016971(2.955613) | NFE Forward 38(45.2) | NFE Backward 32(35.0) | CNF Time 0.5000(0.5000)
Iter 0336 | Time 0.4251(0.4353) | Loss 3.022256(2.960278) | NFE Forward 44(45.1) | NFE Backward 32(34.8) | CNF Time 0.5000(0.5000)
Iter 0337 | Time 1.0306(0.4770) | Loss 2.819177(2.950401) | NFE Forward 44(45.0) | NFE Backward 32(34.6) | CNF Time 0.5000(0.5000)
Iter 0338 | Time 0.4859(0.4776) | Loss 2.718377(2.934160) | NFE Forward 56(45.8) | NFE Backward 32(34.5) | CNF Time 0.5000(0.5000)
Iter 0339 | Time 0.5429(0.4822) | Loss 3.010448(2.939500) | NFE Forward 56(46.5) | NFE Backward 38(34.7) | CNF Time 0.5000(0.5000)
Iter 0340 | Time 0.5972(0.4902) | Loss 2.887828(2.935883) | NFE Forward 50(46.8) | NFE Backward 44(35.4) | CNF Time 0.5000(0.5000)
Iter 0341 | Time 0.4400(0.4867) | Loss 3.087336(2.946484) | NFE Forward 38(46.1) | NFE Backward 44(36.0) | CNF Time 0.5000(0.5000)
Iter 0342 | Time 0.5223(0.4892) | Loss 2.837238(2.938837) | NFE Forward 44(46.0) | NFE Backward 38(36.1) | CNF Time 0.5000(0.5000)
Iter 0343 | Time 0.4713(0.4880) | Loss 2.967279(2.940828) | NFE Forward 44(45.9) | NFE Backward 44(36.7) | CNF Time 0.5000(0.5000)
Iter 0344 | Time 0.5792(0.4943) | Loss 2.952105(2.941617) | NFE Forward 50(46.1) | NFE Backward 38(36.7) | CNF Time 0.5000(0.5000)
Iter 0345 | Time 0.5624(0.4991) | Loss 3.087942(2.951860) | NFE Forward 62(47.3) | NFE Backward 44(37.3) | CNF Time 0.5000(0.5000)
Iter 0346 | Time 0.7440(0.5163) | Loss 3.034044(2.957613) | NFE Forward 50(47.4) | NFE Backward 44(37.7) | CNF Time 0.5000(0.5000)
Iter 0347 | Time 0.4158(0.5092) | Loss 2.962233(2.957936) | NFE Forward 44(47.2) | NFE Backward 38(37.7) | CNF Time 0.5000(0.5000)
Iter 0348 | Time 0.4732(0.5067) | Loss 2.800765(2.946934) | NFE Forward 50(47.4) | NFE Backward 44(38.2) | CNF Time 0.5000(0.5000)
Iter 0349 | Time 0.6195(0.5146) | Loss 3.032055(2.952893) | NFE Forward 50(47.6) | NFE Backward 44(38.6) | CNF Time 0.5000(0.5000)
Iter 0350 | Time 0.6219(0.5221) | Loss 2.848901(2.945613) | NFE Forward 62(48.6) | NFE Backward 38(38.5) | CNF Time 0.5000(0.5000)
Iter 0351 | Time 0.6495(0.5310) | Loss 3.176021(2.961742) | NFE Forward 50(48.7) | NFE Backward 44(38.9) | CNF Time 0.5000(0.5000)
Iter 0352 | Time 0.6356(0.5384) | Loss 2.957957(2.961477) | NFE Forward 56(49.2) | NFE Backward 38(38.9) | CNF Time 0.5000(0.5000)
Iter 0353 | Time 0.4540(0.5324) | Loss 2.997511(2.963999) | NFE Forward 44(48.8) | NFE Backward 38(38.8) | CNF Time 0.5000(0.5000)
Iter 0354 | Time 0.7363(0.5467) | Loss 2.879313(2.958071) | NFE Forward 50(48.9) | NFE Backward 50(39.6) | CNF Time 0.5000(0.5000)
Iter 0355 | Time 0.6488(0.5539) | Loss 2.776997(2.945396) | NFE Forward 50(49.0) | NFE Backward 44(39.9) | CNF Time 0.5000(0.5000)
Iter 0356 | Time 0.4090(0.5437) | Loss 2.929705(2.944298) | NFE Forward 44(48.6) | NFE Backward 38(39.8) | CNF Time 0.5000(0.5000)
Iter 0357 | Time 0.3364(0.5292) | Loss 2.857500(2.938222) | NFE Forward 44(48.3) | NFE Backward 32(39.2) | CNF Time 0.5000(0.5000)
Iter 0358 | Time 0.4649(0.5247) | Loss 2.932161(2.937798) | NFE Forward 56(48.9) | NFE Backward 44(39.6) | CNF Time 0.5000(0.5000)
Iter 0359 | Time 0.4578(0.5200) | Loss 2.801183(2.928235) | NFE Forward 50(48.9) | NFE Backward 38(39.4) | CNF Time 0.5000(0.5000)
Iter 0360 | Time 0.4269(0.5135) | Loss 2.830724(2.921409) | NFE Forward 44(48.6) | NFE Backward 38(39.3) | CNF Time 0.5000(0.5000)
Iter 0361 | Time 0.4479(0.5089) | Loss 2.892883(2.919412) | NFE Forward 56(49.1) | NFE Backward 38(39.3) | CNF Time 0.5000(0.5000)
Iter 0362 | Time 0.4174(0.5025) | Loss 2.949485(2.921517) | NFE Forward 44(48.8) | NFE Backward 38(39.2) | CNF Time 0.5000(0.5000)
Iter 0363 | Time 0.4551(0.4992) | Loss 2.855430(2.916891) | NFE Forward 50(48.8) | NFE Backward 44(39.5) | CNF Time 0.5000(0.5000)
Iter 0364 | Time 0.4026(0.4924) | Loss 2.899692(2.915687) | NFE Forward 56(49.3) | NFE Backward 38(39.4) | CNF Time 0.5000(0.5000)
Iter 0365 | Time 0.5114(0.4938) | Loss 2.807372(2.908105) | NFE Forward 50(49.4) | NFE Backward 32(38.9) | CNF Time 0.5000(0.5000)
Iter 0366 | Time 0.4715(0.4922) | Loss 2.925235(2.909304) | NFE Forward 50(49.4) | NFE Backward 44(39.2) | CNF Time 0.5000(0.5000)
Iter 0367 | Time 0.4703(0.4907) | Loss 2.943973(2.911731) | NFE Forward 50(49.5) | NFE Backward 38(39.2) | CNF Time 0.5000(0.5000)
Iter 0368 | Time 0.4541(0.4881) | Loss 2.634648(2.892335) | NFE Forward 44(49.1) | NFE Backward 44(39.5) | CNF Time 0.5000(0.5000)
Iter 0369 | Time 0.5354(0.4914) | Loss 2.841966(2.888809) | NFE Forward 50(49.2) | NFE Backward 38(39.4) | CNF Time 0.5000(0.5000)
Iter 0370 | Time 0.3947(0.4846) | Loss 2.897549(2.889421) | NFE Forward 50(49.2) | NFE Backward 38(39.3) | CNF Time 0.5000(0.5000)
Iter 0371 | Time 0.4561(0.4826) | Loss 2.800871(2.883223) | NFE Forward 50(49.3) | NFE Backward 38(39.2) | CNF Time 0.5000(0.5000)
Iter 0372 | Time 0.4124(0.4777) | Loss 2.934225(2.886793) | NFE Forward 50(49.3) | NFE Backward 38(39.1) | CNF Time 0.5000(0.5000)
Iter 0373 | Time 0.4329(0.4746) | Loss 2.985854(2.893727) | NFE Forward 56(49.8) | NFE Backward 32(38.6) | CNF Time 0.5000(0.5000)
Iter 0374 | Time 0.4734(0.4745) | Loss 3.064893(2.905709) | NFE Forward 44(49.4) | NFE Backward 50(39.4) | CNF Time 0.5000(0.5000)
Iter 0375 | Time 0.6829(0.4891) | Loss 2.823709(2.899969) | NFE Forward 50(49.4) | NFE Backward 44(39.7) | CNF Time 0.5000(0.5000)
Iter 0376 | Time 0.5085(0.4905) | Loss 2.857194(2.896974) | NFE Forward 50(49.5) | NFE Backward 44(40.0) | CNF Time 0.5000(0.5000)
Iter 0377 | Time 0.4411(0.4870) | Loss 2.905309(2.897558) | NFE Forward 50(49.5) | NFE Backward 38(39.9) | CNF Time 0.5000(0.5000)
Iter 0378 | Time 0.4159(0.4820) | Loss 2.968128(2.902498) | NFE Forward 56(50.0) | NFE Backward 38(39.8) | CNF Time 0.5000(0.5000)
Iter 0379 | Time 0.7572(0.5013) | Loss 2.973777(2.907487) | NFE Forward 44(49.5) | NFE Backward 50(40.5) | CNF Time 0.5000(0.5000)
Iter 0380 | Time 0.4194(0.4956) | Loss 2.809894(2.900656) | NFE Forward 56(50.0) | NFE Backward 38(40.3) | CNF Time 0.5000(0.5000)
Iter 0381 | Time 0.5005(0.4959) | Loss 2.988165(2.906781) | NFE Forward 50(50.0) | NFE Backward 38(40.1) | CNF Time 0.5000(0.5000)
Iter 0382 | Time 0.4283(0.4912) | Loss 2.794873(2.898948) | NFE Forward 50(50.0) | NFE Backward 32(39.6) | CNF Time 0.5000(0.5000)
Iter 0383 | Time 0.5491(0.4952) | Loss 2.861940(2.896357) | NFE Forward 44(49.6) | NFE Backward 44(39.9) | CNF Time 0.5000(0.5000)
Iter 0384 | Time 0.6348(0.5050) | Loss 3.099295(2.910563) | NFE Forward 44(49.2) | NFE Backward 56(41.0) | CNF Time 0.5000(0.5000)
Iter 0385 | Time 0.4841(0.5035) | Loss 2.841438(2.905724) | NFE Forward 56(49.7) | NFE Backward 38(40.8) | CNF Time 0.5000(0.5000)
Iter 0386 | Time 0.6379(0.5129) | Loss 2.865856(2.902933) | NFE Forward 44(49.3) | NFE Backward 44(41.0) | CNF Time 0.5000(0.5000)
Iter 0387 | Time 0.4718(0.5101) | Loss 2.909683(2.903406) | NFE Forward 50(49.3) | NFE Backward 44(41.2) | CNF Time 0.5000(0.5000)
Iter 0388 | Time 0.4536(0.5061) | Loss 2.869387(2.901024) | NFE Forward 50(49.4) | NFE Backward 44(41.4) | CNF Time 0.5000(0.5000)
Iter 0389 | Time 0.6794(0.5182) | Loss 2.769845(2.891842) | NFE Forward 62(50.2) | NFE Backward 50(42.0) | CNF Time 0.5000(0.5000)
Iter 0390 | Time 0.6124(0.5248) | Loss 2.829738(2.887495) | NFE Forward 50(50.2) | NFE Backward 38(41.7) | CNF Time 0.5000(0.5000)
Iter 0391 | Time 0.6450(0.5332) | Loss 2.958787(2.892485) | NFE Forward 56(50.6) | NFE Backward 44(41.9) | CNF Time 0.5000(0.5000)
Iter 0392 | Time 0.5025(0.5311) | Loss 2.843661(2.889067) | NFE Forward 56(51.0) | NFE Backward 38(41.6) | CNF Time 0.5000(0.5000)
Iter 0393 | Time 0.4489(0.5253) | Loss 2.945890(2.893045) | NFE Forward 50(50.9) | NFE Backward 38(41.4) | CNF Time 0.5000(0.5000)
Iter 0394 | Time 0.4393(0.5193) | Loss 2.867403(2.891250) | NFE Forward 56(51.3) | NFE Backward 38(41.1) | CNF Time 0.5000(0.5000)
Iter 0395 | Time 0.5245(0.5197) | Loss 2.831695(2.887081) | NFE Forward 50(51.2) | NFE Backward 44(41.3) | CNF Time 0.5000(0.5000)
Iter 0396 | Time 0.6199(0.5267) | Loss 2.684647(2.872911) | NFE Forward 50(51.1) | NFE Backward 44(41.5) | CNF Time 0.5000(0.5000)
Iter 0397 | Time 0.4339(0.5202) | Loss 2.736977(2.863395) | NFE Forward 50(51.0) | NFE Backward 38(41.3) | CNF Time 0.5000(0.5000)
Iter 0398 | Time 0.4753(0.5171) | Loss 3.083808(2.878824) | NFE Forward 56(51.4) | NFE Backward 38(41.0) | CNF Time 0.5000(0.5000)
Iter 0399 | Time 0.4325(0.5111) | Loss 2.815238(2.874373) | NFE Forward 50(51.3) | NFE Backward 44(41.3) | CNF Time 0.5000(0.5000)
Iter 0400 | Time 0.4409(0.5062) | Loss 2.908501(2.876762) | NFE Forward 56(51.6) | NFE Backward 38(41.0) | CNF Time 0.5000(0.5000)
[TEST] Iter 0400 | Test Loss 2.812366 | NFE 44
Iter 0401 | Time 0.5876(0.5119) | Loss 3.056694(2.889357) | NFE Forward 56(51.9) | NFE Backward 38(40.8) | CNF Time 0.5000(0.5000)
Iter 0402 | Time 0.5272(0.5130) | Loss 2.667996(2.873862) | NFE Forward 50(51.8) | NFE Backward 38(40.6) | CNF Time 0.5000(0.5000)
Iter 0403 | Time 0.7062(0.5265) | Loss 3.118664(2.890998) | NFE Forward 44(51.2) | NFE Backward 50(41.3) | CNF Time 0.5000(0.5000)
Iter 0404 | Time 0.7123(0.5395) | Loss 2.882653(2.890414) | NFE Forward 50(51.2) | NFE Backward 44(41.5) | CNF Time 0.5000(0.5000)
Iter 0405 | Time 0.8180(0.5590) | Loss 3.027123(2.899984) | NFE Forward 44(50.7) | NFE Backward 38(41.2) | CNF Time 0.5000(0.5000)
Iter 0406 | Time 0.5343(0.5573) | Loss 2.762736(2.890377) | NFE Forward 50(50.6) | NFE Backward 50(41.8) | CNF Time 0.5000(0.5000)
Iter 0407 | Time 0.5969(0.5601) | Loss 3.081166(2.903732) | NFE Forward 56(51.0) | NFE Backward 44(42.0) | CNF Time 0.5000(0.5000)
Iter 0408 | Time 0.5973(0.5627) | Loss 3.009181(2.911113) | NFE Forward 56(51.3) | NFE Backward 50(42.5) | CNF Time 0.5000(0.5000)
Iter 0409 | Time 0.4769(0.5567) | Loss 2.837173(2.905937) | NFE Forward 62(52.1) | NFE Backward 38(42.2) | CNF Time 0.5000(0.5000)
Iter 0410 | Time 0.6279(0.5616) | Loss 2.736650(2.894087) | NFE Forward 56(52.4) | NFE Backward 38(41.9) | CNF Time 0.5000(0.5000)
Iter 0411 | Time 0.5667(0.5620) | Loss 2.856023(2.891423) | NFE Forward 44(51.8) | NFE Backward 56(42.9) | CNF Time 0.5000(0.5000)
Iter 0412 | Time 0.5004(0.5577) | Loss 2.824805(2.886760) | NFE Forward 50(51.7) | NFE Backward 44(43.0) | CNF Time 0.5000(0.5000)
Iter 0413 | Time 0.7241(0.5693) | Loss 2.884692(2.886615) | NFE Forward 56(52.0) | NFE Backward 44(43.1) | CNF Time 0.5000(0.5000)
Iter 0414 | Time 1.2252(0.6152) | Loss 2.911721(2.888372) | NFE Forward 56(52.2) | NFE Backward 44(43.1) | CNF Time 0.5000(0.5000)
Iter 0415 | Time 1.0103(0.6429) | Loss 2.841052(2.885060) | NFE Forward 44(51.7) | NFE Backward 44(43.2) | CNF Time 0.5000(0.5000)
Iter 0416 | Time 0.4570(0.6299) | Loss 2.716424(2.873255) | NFE Forward 50(51.5) | NFE Backward 38(42.8) | CNF Time 0.5000(0.5000)
Iter 0417 | Time 0.6160(0.6289) | Loss 2.712745(2.862020) | NFE Forward 62(52.3) | NFE Backward 44(42.9) | CNF Time 0.5000(0.5000)
Iter 0418 | Time 0.5073(0.6204) | Loss 2.832726(2.859969) | NFE Forward 56(52.5) | NFE Backward 44(43.0) | CNF Time 0.5000(0.5000)
Iter 0419 | Time 0.4871(0.6111) | Loss 2.843350(2.858806) | NFE Forward 56(52.8) | NFE Backward 44(43.1) | CNF Time 0.5000(0.5000)
Iter 0420 | Time 0.5901(0.6096) | Loss 2.889367(2.860945) | NFE Forward 56(53.0) | NFE Backward 50(43.5) | CNF Time 0.5000(0.5000)
Iter 0421 | Time 0.5751(0.6072) | Loss 2.919118(2.865017) | NFE Forward 50(52.8) | NFE Backward 44(43.6) | CNF Time 0.5000(0.5000)
Iter 0422 | Time 0.5690(0.6045) | Loss 2.800242(2.860483) | NFE Forward 56(53.0) | NFE Backward 50(44.0) | CNF Time 0.5000(0.5000)
Iter 0423 | Time 0.5020(0.5973) | Loss 2.871634(2.861264) | NFE Forward 56(53.2) | NFE Backward 44(44.0) | CNF Time 0.5000(0.5000)
Iter 0424 | Time 0.4800(0.5891) | Loss 2.946967(2.867263) | NFE Forward 62(53.8) | NFE Backward 38(43.6) | CNF Time 0.5000(0.5000)
Iter 0425 | Time 0.5312(0.5851) | Loss 3.036946(2.879141) | NFE Forward 56(54.0) | NFE Backward 44(43.6) | CNF Time 0.5000(0.5000)
Iter 0426 | Time 0.5935(0.5857) | Loss 2.768270(2.871380) | NFE Forward 50(53.7) | NFE Backward 44(43.7) | CNF Time 0.5000(0.5000)
Iter 0427 | Time 0.4195(0.5740) | Loss 2.825210(2.868148) | NFE Forward 50(53.5) | NFE Backward 38(43.3) | CNF Time 0.5000(0.5000)
Iter 0428 | Time 0.6523(0.5795) | Loss 2.900282(2.870397) | NFE Forward 62(54.1) | NFE Backward 38(42.9) | CNF Time 0.5000(0.5000)
Iter 0429 | Time 0.4215(0.5685) | Loss 2.787644(2.864604) | NFE Forward 50(53.8) | NFE Backward 38(42.5) | CNF Time 0.5000(0.5000)
Iter 0430 | Time 0.5010(0.5637) | Loss 2.662193(2.850436) | NFE Forward 68(54.8) | NFE Backward 38(42.2) | CNF Time 0.5000(0.5000)
Iter 0431 | Time 0.4901(0.5586) | Loss 2.739490(2.842669) | NFE Forward 50(54.4) | NFE Backward 38(41.9) | CNF Time 0.5000(0.5000)
Iter 0432 | Time 0.4866(0.5535) | Loss 2.690832(2.832041) | NFE Forward 44(53.7) | NFE Backward 44(42.1) | CNF Time 0.5000(0.5000)
Iter 0433 | Time 0.4848(0.5487) | Loss 2.755750(2.826701) | NFE Forward 50(53.4) | NFE Backward 38(41.8) | CNF Time 0.5000(0.5000)
Iter 0434 | Time 0.4381(0.5410) | Loss 2.940908(2.834695) | NFE Forward 56(53.6) | NFE Backward 38(41.5) | CNF Time 0.5000(0.5000)
Iter 0435 | Time 0.4645(0.5356) | Loss 2.768167(2.830038) | NFE Forward 50(53.4) | NFE Backward 50(42.1) | CNF Time 0.5000(0.5000)
Iter 0436 | Time 0.4484(0.5295) | Loss 2.757695(2.824974) | NFE Forward 56(53.6) | NFE Backward 44(42.3) | CNF Time 0.5000(0.5000)
Iter 0437 | Time 0.5534(0.5312) | Loss 2.840401(2.826054) | NFE Forward 56(53.7) | NFE Backward 56(43.2) | CNF Time 0.5000(0.5000)
Iter 0438 | Time 0.5724(0.5341) | Loss 2.951601(2.834842) | NFE Forward 44(53.0) | NFE Backward 44(43.3) | CNF Time 0.5000(0.5000)
Iter 0439 | Time 0.5390(0.5344) | Loss 2.798487(2.832297) | NFE Forward 56(53.2) | NFE Backward 44(43.3) | CNF Time 0.5000(0.5000)
Iter 0440 | Time 0.6088(0.5396) | Loss 2.759176(2.827179) | NFE Forward 68(54.3) | NFE Backward 38(42.9) | CNF Time 0.5000(0.5000)
Iter 0441 | Time 0.6152(0.5449) | Loss 2.823076(2.826892) | NFE Forward 68(55.2) | NFE Backward 44(43.0) | CNF Time 0.5000(0.5000)
Iter 0442 | Time 0.4516(0.5384) | Loss 2.973777(2.837174) | NFE Forward 62(55.7) | NFE Backward 38(42.7) | CNF Time 0.5000(0.5000)
Iter 0443 | Time 0.5400(0.5385) | Loss 2.811300(2.835362) | NFE Forward 50(55.3) | NFE Backward 44(42.8) | CNF Time 0.5000(0.5000)
Iter 0444 | Time 0.5726(0.5409) | Loss 2.900606(2.839929) | NFE Forward 56(55.4) | NFE Backward 50(43.3) | CNF Time 0.5000(0.5000)
Iter 0445 | Time 0.4414(0.5339) | Loss 2.732910(2.832438) | NFE Forward 56(55.4) | NFE Backward 38(42.9) | CNF Time 0.5000(0.5000)
Iter 0446 | Time 0.5731(0.5367) | Loss 2.790351(2.829492) | NFE Forward 56(55.4) | NFE Backward 50(43.4) | CNF Time 0.5000(0.5000)
Iter 0447 | Time 0.5422(0.5371) | Loss 2.620033(2.814830) | NFE Forward 62(55.9) | NFE Backward 38(43.0) | CNF Time 0.5000(0.5000)
Iter 0448 | Time 0.5982(0.5413) | Loss 2.777951(2.812248) | NFE Forward 68(56.8) | NFE Backward 44(43.1) | CNF Time 0.5000(0.5000)
Iter 0449 | Time 0.7016(0.5526) | Loss 2.802897(2.811594) | NFE Forward 56(56.7) | NFE Backward 44(43.2) | CNF Time 0.5000(0.5000)
Iter 0450 | Time 0.7051(0.5632) | Loss 2.686786(2.802857) | NFE Forward 62(57.1) | NFE Backward 44(43.2) | CNF Time 0.5000(0.5000)
Iter 0451 | Time 0.6267(0.5677) | Loss 2.822670(2.804244) | NFE Forward 68(57.8) | NFE Backward 44(43.3) | CNF Time 0.5000(0.5000)
Iter 0452 | Time 0.4741(0.5611) | Loss 2.736828(2.799525) | NFE Forward 68(58.5) | NFE Backward 44(43.3) | CNF Time 0.5000(0.5000)
Iter 0453 | Time 0.5399(0.5596) | Loss 2.779264(2.798107) | NFE Forward 68(59.2) | NFE Backward 50(43.8) | CNF Time 0.5000(0.5000)
Iter 0454 | Time 0.5363(0.5580) | Loss 2.521737(2.778761) | NFE Forward 62(59.4) | NFE Backward 44(43.8) | CNF Time 0.5000(0.5000)
/Users/bjzhang/gitrepos/tap-sgm/ffjord-master/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(data='mnist', dims='64,64,64', strides='1,1,1,1', num_blocks=2, conv=True, layer_type='concat', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, imagesize=None, alpha=1e-06, time_length=1.0, train_T=True, num_epochs=1000, batch_size=200, batch_size_schedule='', test_batch_size=200, lr=0.001, warmup_iters=1000, weight_decay=0.0, spectral_norm_niter=10, add_noise=True, batch_norm=False, residual=False, autoencode=False, rademacher=True, spectral_norm=False, multiscale=True, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments/cnf', val_freq=1, log_freq=10)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
/work/bjzhang_umass_edu/generative_tmula/ffjord-master/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":
    print(torch.cuda.is_available())
    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(data='mnist', dims='64,64,64', strides='1,1,1,1', num_blocks=2, conv=True, layer_type='concat', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, imagesize=None, alpha=1e-06, time_length=1.0, train_T=True, num_epochs=1000, batch_size=200, batch_size_schedule='', test_batch_size=200, lr=0.001, warmup_iters=1000, weight_decay=0.0, spectral_norm_niter=10, add_noise=True, batch_norm=False, residual=False, autoencode=False, rademacher=True, spectral_norm=False, multiscale=True, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments/cnf', val_freq=1, log_freq=10)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 10.1440(10.1440) | Bit/dim 30.1535(30.1535) | Steps 406(406.00) | Grad Norm 242.6498(242.6498) | Total Time 10.00(10.00)
Iter 0010 | Time 2.8308(8.2833) | Bit/dim 27.2754(29.8102) | Steps 400(406.72) | Grad Norm 221.3472(239.9490) | Total Time 10.00(10.00)
Iter 0020 | Time 2.8386(6.8823) | Bit/dim 20.8741(28.2136) | Steps 400(406.01) | Grad Norm 163.7130(226.2318) | Total Time 10.00(10.00)
Iter 0030 | Time 2.8407(5.8202) | Bit/dim 14.8642(25.3412) | Steps 400(404.43) | Grad Norm 91.5182(198.8208) | Total Time 10.00(10.00)
Iter 0040 | Time 2.8299(5.0366) | Bit/dim 11.9199(22.0602) | Steps 400(403.27) | Grad Norm 32.9287(161.2598) | Total Time 10.00(10.00)
Iter 0050 | Time 2.8419(4.4589) | Bit/dim 10.1065(19.1134) | Steps 400(402.41) | Grad Norm 18.5271(124.8234) | Total Time 10.00(10.00)
Iter 0060 | Time 2.8434(4.0346) | Bit/dim 8.7235(16.5281) | Steps 400(401.78) | Grad Norm 13.8157(96.1344) | Total Time 10.00(10.00)
/work/bjzhang_umass_edu/generative_tmula/ffjord-master/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)

print(torch.cuda.is_available())



def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":
    print(torch.cuda.is_available())
    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(data='mnist', dims='64,64,64', strides='1,1,1,1', num_blocks=2, conv=True, layer_type='concat', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, imagesize=None, alpha=1e-06, time_length=1.0, train_T=True, num_epochs=1000, batch_size=200, batch_size_schedule='', test_batch_size=200, lr=0.001, warmup_iters=1000, weight_decay=0.0, spectral_norm_niter=10, add_noise=True, batch_norm=False, residual=False, autoencode=False, rademacher=True, spectral_norm=False, multiscale=True, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments/cnf', val_freq=1, log_freq=10)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 5.4338(5.4338) | Bit/dim 27.5431(27.5431) | Steps 406(406.00) | Grad Norm 217.1853(217.1853) | Total Time 10.00(10.00)
Iter 0010 | Time 2.8288(4.7695) | Bit/dim 25.2660(27.3015) | Steps 400(405.05) | Grad Norm 198.0794(215.3303) | Total Time 10.00(10.00)
Iter 0020 | Time 2.8428(4.2802) | Bit/dim 20.1004(26.0178) | Steps 400(404.33) | Grad Norm 150.3652(204.1677) | Total Time 10.00(10.00)
Iter 0030 | Time 2.8479(3.9021) | Bit/dim 15.0521(23.6800) | Steps 400(403.19) | Grad Norm 93.4240(181.3395) | Total Time 10.00(10.00)
/work/bjzhang_umass_edu/generative_tmula/ffjord-master/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)




def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":
    print(torch.cuda.is_available())
    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(data='mnist', dims='64,64,64', strides='1,1,1,1', num_blocks=2, conv=True, layer_type='concat', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, imagesize=None, alpha=1e-06, time_length=1.0, train_T=True, num_epochs=1000, batch_size=200, batch_size_schedule='', test_batch_size=200, lr=0.001, warmup_iters=1000, weight_decay=0.0, spectral_norm_niter=10, add_noise=True, batch_norm=False, residual=False, autoencode=False, rademacher=True, spectral_norm=False, multiscale=True, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments/cnf', val_freq=1, log_freq=10)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 5.5918(5.5918) | Bit/dim 26.3237(26.3237) | Steps 412(412.00) | Grad Norm 210.2658(210.2658) | Total Time 10.00(10.00)
Iter 0010 | Time 2.8356(4.8776) | Bit/dim 24.0213(26.0675) | Steps 400(409.26) | Grad Norm 192.2722(208.6441) | Total Time 10.00(10.00)
Iter 0020 | Time 2.8337(4.3507) | Bit/dim 18.7870(24.7523) | Steps 400(407.14) | Grad Norm 140.9946(197.0253) | Total Time 10.00(10.00)
Iter 0030 | Time 2.8373(3.9540) | Bit/dim 14.2482(22.4228) | Steps 400(405.26) | Grad Norm 75.9145(173.0385) | Total Time 10.00(10.00)
Iter 0040 | Time 2.8423(3.6614) | Bit/dim 11.5004(19.7844) | Steps 400(403.88) | Grad Norm 32.2529(140.5898) | Total Time 10.00(10.00)
Iter 0050 | Time 2.8402(3.4452) | Bit/dim 9.4141(17.3281) | Steps 400(402.86) | Grad Norm 17.4495(109.5292) | Total Time 10.00(10.00)
Iter 0060 | Time 2.8304(3.2867) | Bit/dim 8.3879(15.0819) | Steps 400(402.11) | Grad Norm 14.8458(84.7162) | Total Time 10.00(10.00)
Iter 0070 | Time 2.8373(3.1683) | Bit/dim 7.0016(13.0860) | Steps 400(401.56) | Grad Norm 15.7183(66.6109) | Total Time 10.00(10.00)
Iter 0080 | Time 2.8439(3.0830) | Bit/dim 5.6744(11.2852) | Steps 400(401.15) | Grad Norm 12.7096(52.8436) | Total Time 10.00(10.00)
Iter 0090 | Time 2.8417(3.0202) | Bit/dim 4.5466(9.6562) | Steps 400(400.85) | Grad Norm 10.7311(41.9993) | Total Time 10.00(10.00)
Iter 0100 | Time 2.8426(2.9736) | Bit/dim 3.6749(8.1923) | Steps 400(400.62) | Grad Norm 8.9554(33.5444) | Total Time 10.00(10.00)
Iter 0110 | Time 3.0510(2.9607) | Bit/dim 3.1301(6.9231) | Steps 412(401.99) | Grad Norm 6.6933(26.7710) | Total Time 10.00(10.00)
Iter 0120 | Time 3.2471(3.0023) | Bit/dim 2.7002(5.8546) | Steps 424(405.99) | Grad Norm 4.4111(21.1277) | Total Time 10.00(10.00)
Iter 0130 | Time 3.2462(3.0665) | Bit/dim 2.5087(4.9924) | Steps 424(410.72) | Grad Norm 2.7780(16.4807) | Total Time 10.00(10.00)
Iter 0140 | Time 3.3352(3.1337) | Bit/dim 2.3678(4.3177) | Steps 436(416.80) | Grad Norm 1.6763(12.6899) | Total Time 10.00(10.00)
Iter 0150 | Time 3.4946(3.2251) | Bit/dim 2.2981(3.7963) | Steps 442(423.28) | Grad Norm 1.2433(9.7366) | Total Time 10.00(10.00)
Iter 0160 | Time 3.4953(3.2951) | Bit/dim 2.2474(3.4001) | Steps 442(428.20) | Grad Norm 1.0346(7.4763) | Total Time 10.00(10.00)
Iter 0170 | Time 3.5032(3.3470) | Bit/dim 2.2508(3.0945) | Steps 442(431.82) | Grad Norm 0.8362(5.7581) | Total Time 10.00(10.00)
Iter 0180 | Time 3.4971(3.3863) | Bit/dim 2.1977(2.8674) | Steps 442(434.49) | Grad Norm 0.6792(4.4369) | Total Time 10.00(10.00)
Iter 0190 | Time 3.4922(3.4153) | Bit/dim 2.2218(2.6940) | Steps 442(436.46) | Grad Norm 0.6190(3.4365) | Total Time 10.00(10.00)
Iter 0200 | Time 3.4830(3.4363) | Bit/dim 2.1919(2.5609) | Steps 442(437.92) | Grad Norm 0.5507(2.6909) | Total Time 10.00(10.00)
Iter 0210 | Time 3.4977(3.4515) | Bit/dim 2.1879(2.4585) | Steps 442(438.99) | Grad Norm 0.5406(2.1355) | Total Time 10.00(10.00)
Iter 0220 | Time 3.5044(3.4644) | Bit/dim 2.1687(2.3794) | Steps 442(439.78) | Grad Norm 0.5876(1.7174) | Total Time 10.00(10.00)
Iter 0230 | Time 3.5008(3.4447) | Bit/dim 2.1366(2.3238) | Steps 442(439.27) | Grad Norm 0.4990(1.4096) | Total Time 10.00(10.00)
Iter 0240 | Time 3.3310(3.4215) | Bit/dim 2.1490(2.2772) | Steps 436(438.57) | Grad Norm 0.6330(1.1980) | Total Time 10.00(10.00)
Iter 0250 | Time 3.3380(3.3996) | Bit/dim 2.1375(2.2393) | Steps 436(437.89) | Grad Norm 0.5710(1.0300) | Total Time 10.00(10.00)
Iter 0260 | Time 3.3340(3.3834) | Bit/dim 2.1016(2.2074) | Steps 436(437.40) | Grad Norm 0.4371(0.8887) | Total Time 10.00(10.00)
Iter 0270 | Time 3.3426(3.3716) | Bit/dim 2.0946(2.1775) | Steps 436(437.03) | Grad Norm 0.4500(0.7763) | Total Time 10.00(10.00)
Iter 0280 | Time 3.3391(3.3627) | Bit/dim 2.1229(2.1578) | Steps 436(436.76) | Grad Norm 0.3942(0.7046) | Total Time 10.00(10.00)
Iter 0290 | Time 3.1632(3.3286) | Bit/dim 2.0715(2.1369) | Steps 430(435.58) | Grad Norm 0.4343(0.6376) | Total Time 10.00(10.00)
validating...
/work/bjzhang_umass_edu/generative_tmula/ffjord-master/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)




def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":
    print(torch.cuda.is_available())
    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses.deatch().cpu().clone().numpy())
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(data='mnist', dims='64,64,64', strides='1,1,1,1', num_blocks=2, conv=True, layer_type='concat', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, imagesize=None, alpha=1e-06, time_length=1.0, train_T=True, num_epochs=1000, batch_size=200, batch_size_schedule='', test_batch_size=200, lr=0.001, warmup_iters=1000, weight_decay=0.0, spectral_norm_niter=10, add_noise=True, batch_norm=False, residual=False, autoencode=False, rademacher=True, spectral_norm=False, multiscale=True, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments/cnf', val_freq=1, log_freq=10)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 5.4613(5.4613) | Bit/dim 22.5767(22.5767) | Steps 406(406.00) | Grad Norm 167.5661(167.5661) | Total Time 10.00(10.00)
Iter 0010 | Time 2.8273(4.7779) | Bit/dim 20.5697(22.3357) | Steps 400(404.70) | Grad Norm 153.9431(166.1081) | Total Time 10.00(10.00)
Iter 0020 | Time 2.8387(4.2689) | Bit/dim 16.4983(21.2848) | Steps 400(403.46) | Grad Norm 108.6905(156.0066) | Total Time 10.00(10.00)
Iter 0030 | Time 2.8393(3.8931) | Bit/dim 13.1972(19.4923) | Steps 400(402.55) | Grad Norm 54.7065(135.3938) | Total Time 10.00(10.00)
Iter 0040 | Time 2.8344(3.6161) | Bit/dim 11.4015(17.5142) | Steps 400(401.88) | Grad Norm 23.4423(109.0724) | Total Time 10.00(10.00)
Iter 0050 | Time 2.8380(3.4109) | Bit/dim 9.5382(15.5977) | Steps 400(401.39) | Grad Norm 14.3444(85.0409) | Total Time 10.00(10.00)
Iter 0060 | Time 2.8481(3.2614) | Bit/dim 8.1862(13.8242) | Steps 400(401.02) | Grad Norm 14.5453(66.3394) | Total Time 10.00(10.00)
Iter 0070 | Time 3.0024(3.1609) | Bit/dim 6.9067(12.1579) | Steps 406(401.11) | Grad Norm 14.3415(52.8244) | Total Time 10.00(10.00)
Iter 0080 | Time 2.8413(3.0964) | Bit/dim 5.6517(10.5570) | Steps 400(401.55) | Grad Norm 11.9211(42.3190) | Total Time 10.00(10.00)
Iter 0090 | Time 2.8478(3.0299) | Bit/dim 4.3670(9.0419) | Steps 400(401.14) | Grad Norm 10.1886(34.0360) | Total Time 10.00(10.00)
Iter 0100 | Time 3.0454(2.9965) | Bit/dim 3.4084(7.6447) | Steps 412(402.20) | Grad Norm 7.8342(27.4302) | Total Time 10.00(10.00)
Iter 0110 | Time 3.0472(3.0085) | Bit/dim 2.8423(6.4362) | Steps 412(404.77) | Grad Norm 4.8124(21.7973) | Total Time 10.00(10.00)
Iter 0120 | Time 3.1380(3.0288) | Bit/dim 2.5605(5.4487) | Steps 424(407.88) | Grad Norm 2.9787(17.0358) | Total Time 10.00(10.00)
Iter 0130 | Time 3.2944(3.0797) | Bit/dim 2.3926(4.6673) | Steps 430(412.96) | Grad Norm 1.9194(13.1651) | Total Time 10.00(10.00)
Iter 0140 | Time 3.2995(3.1368) | Bit/dim 2.3555(4.0623) | Steps 430(417.44) | Grad Norm 1.4154(10.1352) | Total Time 10.00(10.00)
Iter 0150 | Time 3.6111(3.2381) | Bit/dim 2.2255(3.5963) | Steps 442(423.04) | Grad Norm 1.0375(7.7863) | Total Time 10.00(10.00)
Iter 0160 | Time 3.6174(3.3371) | Bit/dim 2.2388(3.2446) | Steps 442(428.02) | Grad Norm 0.8523(5.9838) | Total Time 10.00(10.00)
Iter 0170 | Time 3.6106(3.4093) | Bit/dim 2.2292(2.9781) | Steps 442(431.69) | Grad Norm 0.6864(4.6048) | Total Time 10.00(10.00)
Iter 0180 | Time 3.6160(3.4627) | Bit/dim 2.2091(2.7765) | Steps 442(434.40) | Grad Norm 0.6338(3.5648) | Total Time 10.00(10.00)
Iter 0190 | Time 3.6133(3.5014) | Bit/dim 2.1836(2.6236) | Steps 442(436.39) | Grad Norm 0.5975(2.7849) | Total Time 10.00(10.00)
Iter 0200 | Time 3.4585(3.5053) | Bit/dim 2.1662(2.5104) | Steps 436(436.89) | Grad Norm 0.5574(2.2016) | Total Time 10.00(10.00)
Iter 0210 | Time 3.2980(3.4826) | Bit/dim 2.1182(2.4206) | Steps 430(436.30) | Grad Norm 0.5439(1.7702) | Total Time 10.00(10.00)
Iter 0220 | Time 3.2519(3.4281) | Bit/dim 2.1507(2.3500) | Steps 424(433.81) | Grad Norm 0.5470(1.4461) | Total Time 10.00(10.00)
Iter 0230 | Time 3.2363(3.3814) | Bit/dim 2.1721(2.2990) | Steps 424(431.23) | Grad Norm 0.4869(1.2072) | Total Time 10.00(10.00)
Iter 0240 | Time 3.2522(3.3469) | Bit/dim 2.1222(2.2567) | Steps 424(429.33) | Grad Norm 0.5036(1.0202) | Total Time 10.00(10.00)
Iter 0250 | Time 3.2616(3.3226) | Bit/dim 2.1265(2.2221) | Steps 424(427.93) | Grad Norm 0.4818(0.8864) | Total Time 10.00(10.00)
Iter 0260 | Time 3.2558(3.3044) | Bit/dim 2.1336(2.1925) | Steps 424(426.90) | Grad Norm 0.4539(0.7769) | Total Time 10.00(10.00)
Iter 0270 | Time 3.2528(3.2909) | Bit/dim 2.0819(2.1665) | Steps 424(426.14) | Grad Norm 0.4542(0.6979) | Total Time 10.00(10.00)
Iter 0280 | Time 3.2481(3.2791) | Bit/dim 2.1033(2.1471) | Steps 424(425.58) | Grad Norm 0.4697(0.6340) | Total Time 10.00(10.00)
Iter 0290 | Time 3.2514(3.2716) | Bit/dim 2.0379(2.1245) | Steps 424(425.16) | Grad Norm 0.4574(0.5954) | Total Time 10.00(10.00)
validating...
/work/bjzhang_umass_edu/generative_tmula/ffjord-master/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)




def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":
    print(torch.cuda.is_available())
    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.detach().cpu().clone().numpy())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(data='mnist', dims='64,64,64', strides='1,1,1,1', num_blocks=2, conv=True, layer_type='concat', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, imagesize=None, alpha=1e-06, time_length=1.0, train_T=True, num_epochs=1000, batch_size=200, batch_size_schedule='', test_batch_size=200, lr=0.001, warmup_iters=1000, weight_decay=0.0, spectral_norm_niter=10, add_noise=True, batch_norm=False, residual=False, autoencode=False, rademacher=True, spectral_norm=False, multiscale=True, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments/cnf', val_freq=1, log_freq=10)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 13.6766(13.6766) | Bit/dim 26.1074(26.1074) | Steps 400(400.00) | Grad Norm 221.5234(221.5234) | Total Time 10.00(10.00)
Iter 0010 | Time 2.8795(10.8630) | Bit/dim 23.7522(25.8503) | Steps 400(400.82) | Grad Norm 195.9547(218.7834) | Total Time 10.00(10.00)
Iter 0020 | Time 2.9011(8.7795) | Bit/dim 18.5515(24.5580) | Steps 400(400.90) | Grad Norm 144.5869(205.4125) | Total Time 10.00(10.00)
Iter 0030 | Time 2.8930(7.2340) | Bit/dim 13.8875(22.2580) | Steps 400(400.66) | Grad Norm 78.4974(179.0730) | Total Time 10.00(10.00)
Iter 0040 | Time 2.8962(6.0947) | Bit/dim 11.4211(19.6571) | Steps 400(400.49) | Grad Norm 31.0800(144.6507) | Total Time 10.00(10.00)
Iter 0050 | Time 2.8826(5.2549) | Bit/dim 9.6182(17.2164) | Steps 400(400.36) | Grad Norm 16.8794(112.2075) | Total Time 10.00(10.00)
Iter 0060 | Time 2.8970(4.6337) | Bit/dim 8.2761(15.0115) | Steps 400(400.27) | Grad Norm 14.5809(86.7017) | Total Time 10.00(10.00)
Iter 0070 | Time 2.9097(4.1772) | Bit/dim 6.8273(13.0302) | Steps 400(400.20) | Grad Norm 15.1676(67.8794) | Total Time 10.00(10.00)
Iter 0080 | Time 2.8834(3.8413) | Bit/dim 5.6796(11.2519) | Steps 400(400.14) | Grad Norm 13.2737(53.7844) | Total Time 10.00(10.00)
Iter 0090 | Time 2.8984(3.5934) | Bit/dim 4.4681(9.6272) | Steps 400(400.11) | Grad Norm 10.8547(42.8077) | Total Time 10.00(10.00)
Iter 0100 | Time 3.1143(3.4259) | Bit/dim 3.7520(8.1582) | Steps 412(401.12) | Grad Norm 8.6849(34.0731) | Total Time 10.00(10.00)
Iter 0110 | Time 3.0929(3.3393) | Bit/dim 3.0692(6.8969) | Steps 412(403.98) | Grad Norm 6.3476(27.0625) | Total Time 10.00(10.00)
Iter 0120 | Time 3.1499(3.2837) | Bit/dim 2.7471(5.8497) | Steps 418(406.61) | Grad Norm 4.2234(21.2951) | Total Time 10.00(10.00)
Iter 0130 | Time 3.3168(3.2836) | Bit/dim 2.5826(5.0151) | Steps 424(410.90) | Grad Norm 2.7555(16.5891) | Total Time 10.00(10.00)
Iter 0140 | Time 3.3230(3.2910) | Bit/dim 2.4326(4.3502) | Steps 424(414.34) | Grad Norm 1.8536(12.8052) | Total Time 10.00(10.00)
Iter 0150 | Time 3.3807(3.3173) | Bit/dim 2.3279(3.8364) | Steps 436(419.89) | Grad Norm 1.2851(9.8406) | Total Time 10.00(10.00)
Iter 0160 | Time 3.4007(3.3385) | Bit/dim 2.2965(3.4365) | Steps 436(424.12) | Grad Norm 1.0540(7.5603) | Total Time 10.00(10.00)
Iter 0170 | Time 3.3980(3.3551) | Bit/dim 2.2327(3.1326) | Steps 436(427.24) | Grad Norm 0.8949(5.8261) | Total Time 10.00(10.00)
Iter 0180 | Time 3.3941(3.3679) | Bit/dim 2.2318(2.8968) | Steps 436(429.54) | Grad Norm 0.7733(4.5121) | Total Time 10.00(10.00)
Iter 0190 | Time 3.4462(3.3833) | Bit/dim 2.2388(2.7186) | Steps 442(432.06) | Grad Norm 0.6878(3.5201) | Total Time 10.00(10.00)
Iter 0200 | Time 3.3956(3.3910) | Bit/dim 2.2053(2.5845) | Steps 436(433.67) | Grad Norm 0.6619(2.7768) | Total Time 10.00(10.00)
Iter 0210 | Time 3.4017(3.3937) | Bit/dim 2.1542(2.4799) | Steps 436(434.28) | Grad Norm 0.5923(2.2146) | Total Time 10.00(10.00)
Iter 0220 | Time 3.3999(3.3962) | Bit/dim 2.1632(2.3950) | Steps 436(434.73) | Grad Norm 0.5742(1.7895) | Total Time 10.00(10.00)
Iter 0230 | Time 3.3978(3.3970) | Bit/dim 2.1652(2.3306) | Steps 436(435.07) | Grad Norm 0.5576(1.4655) | Total Time 10.00(10.00)
Iter 0240 | Time 3.3892(3.3960) | Bit/dim 2.1576(2.2823) | Steps 436(435.31) | Grad Norm 0.6824(1.2252) | Total Time 10.00(10.00)
Iter 0250 | Time 3.3864(3.3957) | Bit/dim 2.1344(2.2413) | Steps 436(435.49) | Grad Norm 0.5414(1.0529) | Total Time 10.00(10.00)
Iter 0260 | Time 3.3995(3.3961) | Bit/dim 2.0756(2.2063) | Steps 436(435.63) | Grad Norm 0.5045(0.9144) | Total Time 10.00(10.00)
Iter 0270 | Time 3.4142(3.3979) | Bit/dim 2.1311(2.1800) | Steps 436(435.72) | Grad Norm 0.4517(0.8056) | Total Time 10.00(10.00)
Iter 0280 | Time 3.3437(3.3894) | Bit/dim 2.0842(2.1557) | Steps 430(434.64) | Grad Norm 0.5247(0.7243) | Total Time 10.00(10.00)
Iter 0290 | Time 3.3653(3.3800) | Bit/dim 2.0764(2.1359) | Steps 430(433.42) | Grad Norm 0.4952(0.6637) | Total Time 10.00(10.00)
validating...
Epoch 0001 | Time 47.9650, Bit/dim 2.0527
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0300 | Time 3.3711(3.3744) | Bit/dim 2.0176(2.1166) | Steps 430(432.53) | Grad Norm 0.5231(0.6218) | Total Time 10.00(10.00)
Iter 0310 | Time 3.3755(3.3762) | Bit/dim 2.0518(2.0980) | Steps 436(432.99) | Grad Norm 0.5239(0.5810) | Total Time 10.00(10.00)
Iter 0320 | Time 3.4036(3.3819) | Bit/dim 2.0101(2.0817) | Steps 436(433.78) | Grad Norm 0.4923(0.5712) | Total Time 10.00(10.00)
Iter 0330 | Time 3.3999(3.3864) | Bit/dim 2.0133(2.0655) | Steps 436(434.36) | Grad Norm 0.7261(0.5832) | Total Time 10.00(10.00)
Iter 0340 | Time 3.4046(3.3925) | Bit/dim 2.0107(2.0497) | Steps 436(434.79) | Grad Norm 0.6081(0.5815) | Total Time 10.00(10.00)
Iter 0350 | Time 3.4617(3.4046) | Bit/dim 1.9610(2.0301) | Steps 442(436.25) | Grad Norm 0.4675(0.5964) | Total Time 10.00(10.00)
Iter 0360 | Time 3.6497(3.4559) | Bit/dim 1.9472(2.0144) | Steps 454(439.91) | Grad Norm 0.5034(0.6012) | Total Time 10.00(10.00)
Iter 0370 | Time 3.6641(3.5082) | Bit/dim 1.9581(1.9991) | Steps 454(443.61) | Grad Norm 0.5227(0.6132) | Total Time 10.00(10.00)
Iter 0380 | Time 3.9720(3.6160) | Bit/dim 1.9131(1.9810) | Steps 466(448.93) | Grad Norm 0.7798(0.6445) | Total Time 10.00(10.00)
Iter 0390 | Time 3.9462(3.7092) | Bit/dim 1.9246(1.9683) | Steps 466(453.41) | Grad Norm 0.4280(0.6205) | Total Time 10.00(10.00)
Iter 0400 | Time 3.9700(3.7769) | Bit/dim 1.9049(1.9563) | Steps 466(456.72) | Grad Norm 0.5734(0.6008) | Total Time 10.00(10.00)
Iter 0410 | Time 4.1411(3.8463) | Bit/dim 1.8660(1.9413) | Steps 472(459.83) | Grad Norm 0.7094(0.6042) | Total Time 10.00(10.00)
Iter 0420 | Time 4.1398(3.9201) | Bit/dim 1.9142(1.9331) | Steps 472(462.86) | Grad Norm 0.3423(0.5897) | Total Time 10.00(10.00)
Iter 0430 | Time 4.1234(3.9774) | Bit/dim 1.9088(1.9229) | Steps 472(465.58) | Grad Norm 0.4611(0.5521) | Total Time 10.00(10.00)
Iter 0440 | Time 4.1225(4.0229) | Bit/dim 1.8991(1.9149) | Steps 472(467.87) | Grad Norm 0.4400(0.5445) | Total Time 10.00(10.00)
Iter 0450 | Time 4.1750(4.0602) | Bit/dim 1.8781(1.9067) | Steps 478(470.09) | Grad Norm 0.4389(0.5096) | Total Time 10.00(10.00)
Iter 0460 | Time 4.1365(4.0871) | Bit/dim 1.9023(1.9019) | Steps 472(471.71) | Grad Norm 0.3543(0.5206) | Total Time 10.00(10.00)
Iter 0470 | Time 4.2254(4.1158) | Bit/dim 1.8570(1.8957) | Steps 484(474.22) | Grad Norm 0.5076(0.5602) | Total Time 10.00(10.00)
Iter 0480 | Time 4.2299(4.1420) | Bit/dim 1.8860(1.8899) | Steps 484(476.11) | Grad Norm 0.5391(0.5543) | Total Time 10.00(10.00)
Iter 0490 | Time 4.2190(4.1636) | Bit/dim 1.8535(1.8843) | Steps 484(478.18) | Grad Norm 0.4116(0.5294) | Total Time 10.00(10.00)
Iter 0500 | Time 4.3225(4.2025) | Bit/dim 1.7982(1.8751) | Steps 484(479.73) | Grad Norm 0.3512(0.5241) | Total Time 10.00(10.00)
Iter 0510 | Time 4.3967(4.2497) | Bit/dim 1.8311(1.8617) | Steps 490(482.43) | Grad Norm 0.5338(0.5171) | Total Time 10.00(10.00)
Iter 0520 | Time 4.3778(4.2843) | Bit/dim 1.7907(1.8487) | Steps 490(484.42) | Grad Norm 0.5163(0.5231) | Total Time 10.00(10.00)
Iter 0530 | Time 4.3706(4.3190) | Bit/dim 1.8275(1.8371) | Steps 490(486.22) | Grad Norm 0.4098(0.5433) | Total Time 10.00(10.00)
Iter 0540 | Time 4.5399(4.3762) | Bit/dim 1.7948(1.8209) | Steps 496(488.79) | Grad Norm 0.4706(0.5515) | Total Time 10.00(10.00)
Iter 0550 | Time 4.5495(4.4202) | Bit/dim 1.7620(1.8033) | Steps 496(490.68) | Grad Norm 0.8567(0.6188) | Total Time 10.00(10.00)
Iter 0560 | Time 4.5823(4.4615) | Bit/dim 1.7250(1.7802) | Steps 502(493.38) | Grad Norm 1.3938(0.8788) | Total Time 10.00(10.00)
Iter 0570 | Time 4.7408(4.4979) | Bit/dim 1.6675(1.7541) | Steps 508(495.82) | Grad Norm 1.5624(0.9322) | Total Time 10.00(10.00)
Iter 0580 | Time 4.7597(4.5555) | Bit/dim 1.6904(1.7344) | Steps 508(498.69) | Grad Norm 1.0134(1.2882) | Total Time 10.00(10.00)
Iter 0590 | Time 4.7548(4.5887) | Bit/dim 1.6291(1.7160) | Steps 508(500.52) | Grad Norm 5.1736(1.8782) | Total Time 10.00(10.00)
validating...
Epoch 0002 | Time 62.3788, Bit/dim 1.6284
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0600 | Time 4.7961(4.6182) | Bit/dim 1.6378(1.6971) | Steps 514(502.21) | Grad Norm 1.7239(2.0237) | Total Time 10.00(10.00)
Iter 0610 | Time 4.6328(4.6511) | Bit/dim 1.6219(1.6786) | Steps 508(504.67) | Grad Norm 2.7443(2.3404) | Total Time 10.00(10.00)
Iter 0620 | Time 4.6949(4.6739) | Bit/dim 1.5800(1.6594) | Steps 502(506.11) | Grad Norm 5.6415(3.2211) | Total Time 10.00(10.00)
Iter 0630 | Time 4.8603(4.7343) | Bit/dim 1.5757(1.6408) | Steps 508(508.42) | Grad Norm 7.6850(3.8579) | Total Time 10.00(10.00)
Iter 0640 | Time 4.8753(4.7667) | Bit/dim 1.5849(1.6251) | Steps 508(509.24) | Grad Norm 5.3870(4.0824) | Total Time 10.00(10.00)
Iter 0650 | Time 4.8961(4.7536) | Bit/dim 1.5743(1.6068) | Steps 514(509.07) | Grad Norm 3.0317(4.3071) | Total Time 10.00(10.00)
Iter 0660 | Time 4.7477(4.7255) | Bit/dim 1.5840(1.5994) | Steps 508(507.69) | Grad Norm 17.9537(7.0305) | Total Time 10.00(10.00)
Iter 0670 | Time 4.4410(4.7010) | Bit/dim 1.5615(1.5883) | Steps 496(506.81) | Grad Norm 11.9020(7.8644) | Total Time 10.00(10.00)
Iter 0680 | Time 4.5532(4.6582) | Bit/dim 1.4968(1.5746) | Steps 496(504.50) | Grad Norm 2.9035(7.2606) | Total Time 10.00(10.00)
Iter 0690 | Time 4.5472(4.6032) | Bit/dim 1.5397(1.5635) | Steps 496(501.36) | Grad Norm 6.9128(6.3248) | Total Time 10.00(10.00)
Iter 0700 | Time 4.5397(4.5469) | Bit/dim 1.4959(1.5576) | Steps 496(498.40) | Grad Norm 1.2138(7.7339) | Total Time 10.00(10.00)
Iter 0710 | Time 4.5333(4.5209) | Bit/dim 1.5219(1.5501) | Steps 496(496.87) | Grad Norm 8.7929(8.2344) | Total Time 10.00(10.00)
Iter 0720 | Time 4.2390(4.4903) | Bit/dim 1.5022(1.5398) | Steps 484(495.23) | Grad Norm 16.9706(9.6563) | Total Time 10.00(10.00)
Iter 0730 | Time 4.7039(4.4941) | Bit/dim 1.5040(1.5306) | Steps 502(495.06) | Grad Norm 2.1267(8.7724) | Total Time 10.00(10.00)
Iter 0740 | Time 4.3669(4.4768) | Bit/dim 1.5711(1.5308) | Steps 490(494.41) | Grad Norm 23.1937(10.9640) | Total Time 10.00(10.00)
Iter 0750 | Time 4.1792(4.4458) | Bit/dim 1.4876(1.5250) | Steps 478(492.38) | Grad Norm 14.6574(11.4518) | Total Time 10.00(10.00)
Iter 0760 | Time 4.3377(4.4332) | Bit/dim 1.5423(1.5167) | Steps 484(490.78) | Grad Norm 15.7722(11.4850) | Total Time 10.00(10.00)
Iter 0770 | Time 4.3491(4.4073) | Bit/dim 1.5122(1.5093) | Steps 484(488.94) | Grad Norm 14.8276(10.7914) | Total Time 10.00(10.00)
Iter 0780 | Time 4.5836(4.3955) | Bit/dim 1.4988(1.5017) | Steps 502(488.30) | Grad Norm 15.5151(10.8765) | Total Time 10.00(10.00)
Iter 0790 | Time 4.0048(4.3802) | Bit/dim 1.4528(1.5022) | Steps 472(487.50) | Grad Norm 14.7583(12.6101) | Total Time 10.00(10.00)
Iter 0800 | Time 4.3846(4.3654) | Bit/dim 1.4576(1.4925) | Steps 490(487.37) | Grad Norm 10.1059(10.8494) | Total Time 10.00(10.00)
Iter 0810 | Time 4.5189(4.3637) | Bit/dim 1.4543(1.4893) | Steps 496(488.11) | Grad Norm 7.9700(12.7758) | Total Time 10.00(10.00)
Iter 0820 | Time 4.5895(4.3893) | Bit/dim 1.4870(1.4840) | Steps 502(489.88) | Grad Norm 22.4348(13.2560) | Total Time 10.00(10.00)
Iter 0830 | Time 4.3876(4.3504) | Bit/dim 1.4448(1.4763) | Steps 490(488.65) | Grad Norm 3.4400(12.0947) | Total Time 10.00(10.00)
Iter 0840 | Time 4.4231(4.3483) | Bit/dim 1.4936(1.4698) | Steps 496(489.07) | Grad Norm 18.7936(13.1425) | Total Time 10.00(10.00)
Iter 0850 | Time 4.2226(4.3302) | Bit/dim 1.4251(1.4650) | Steps 484(488.80) | Grad Norm 19.0821(13.7643) | Total Time 10.00(10.00)
Iter 0860 | Time 4.4271(4.3266) | Bit/dim 1.4390(1.4564) | Steps 496(488.80) | Grad Norm 12.0620(12.0343) | Total Time 10.00(10.00)
Iter 0870 | Time 4.2236(4.3391) | Bit/dim 1.4481(1.4614) | Steps 484(490.03) | Grad Norm 13.3088(15.0302) | Total Time 10.00(10.00)
Iter 0880 | Time 4.2956(4.3149) | Bit/dim 1.4506(1.4550) | Steps 496(489.70) | Grad Norm 14.8198(14.1082) | Total Time 10.00(10.00)
Iter 0890 | Time 4.4707(4.3038) | Bit/dim 1.4537(1.4485) | Steps 502(490.30) | Grad Norm 7.8375(12.4680) | Total Time 10.00(10.00)
validating...
Epoch 0003 | Time 63.0995, Bit/dim 1.4287
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0900 | Time 4.4680(4.3051) | Bit/dim 1.4461(1.4461) | Steps 502(490.64) | Grad Norm 16.0286(13.7872) | Total Time 10.00(10.00)
Iter 0910 | Time 4.2105(4.3162) | Bit/dim 1.4385(1.4399) | Steps 484(491.68) | Grad Norm 23.9879(13.0811) | Total Time 10.00(10.00)
Iter 0920 | Time 4.4720(4.3289) | Bit/dim 1.4514(1.4389) | Steps 502(492.62) | Grad Norm 25.6599(14.7949) | Total Time 10.00(10.00)
Iter 0930 | Time 4.4501(4.3336) | Bit/dim 1.4178(1.4337) | Steps 502(493.68) | Grad Norm 17.7952(14.7535) | Total Time 10.00(10.00)
Iter 0940 | Time 4.1694(4.3052) | Bit/dim 1.3874(1.4269) | Steps 490(493.22) | Grad Norm 14.2904(13.3199) | Total Time 10.00(10.00)
Iter 0950 | Time 4.4720(4.3212) | Bit/dim 1.4420(1.4286) | Steps 502(493.94) | Grad Norm 20.8688(15.3450) | Total Time 10.00(10.00)
Iter 0960 | Time 4.1709(4.3165) | Bit/dim 1.3991(1.4217) | Steps 490(494.13) | Grad Norm 3.3417(14.7411) | Total Time 10.00(10.00)
Iter 0970 | Time 4.3672(4.3122) | Bit/dim 1.4255(1.4207) | Steps 502(495.12) | Grad Norm 15.9644(14.1110) | Total Time 10.00(10.00)
Iter 0980 | Time 4.4778(4.3281) | Bit/dim 1.4664(1.4197) | Steps 502(496.17) | Grad Norm 23.2402(14.4021) | Total Time 10.00(10.00)
Iter 0990 | Time 4.0684(4.3371) | Bit/dim 1.4314(1.4186) | Steps 478(496.66) | Grad Norm 20.6214(15.3694) | Total Time 10.00(10.00)
Iter 1000 | Time 4.2041(4.3353) | Bit/dim 1.3539(1.4150) | Steps 496(497.57) | Grad Norm 4.8873(13.9052) | Total Time 10.00(10.00)
Iter 1010 | Time 4.4754(4.3556) | Bit/dim 1.4154(1.4143) | Steps 502(498.51) | Grad Norm 7.6734(14.8419) | Total Time 10.00(10.00)
Iter 1020 | Time 4.3431(4.3614) | Bit/dim 1.3789(1.4099) | Steps 502(498.97) | Grad Norm 12.0245(13.2932) | Total Time 10.00(10.00)
Iter 1030 | Time 4.1694(4.3576) | Bit/dim 1.4505(1.4088) | Steps 478(499.07) | Grad Norm 30.6293(15.0979) | Total Time 10.00(10.00)
Iter 1040 | Time 4.3125(4.3674) | Bit/dim 1.3737(1.4097) | Steps 496(500.14) | Grad Norm 5.2591(15.2755) | Total Time 10.00(10.00)
Iter 1050 | Time 4.3613(4.3642) | Bit/dim 1.3672(1.4092) | Steps 502(500.35) | Grad Norm 7.3953(14.4044) | Total Time 10.00(10.00)
Iter 1060 | Time 4.5321(4.3679) | Bit/dim 1.4250(1.4013) | Steps 508(500.84) | Grad Norm 15.6387(13.3360) | Total Time 10.00(10.00)
Iter 1070 | Time 4.3610(4.3693) | Bit/dim 1.3967(1.3924) | Steps 502(501.31) | Grad Norm 11.0540(12.5981) | Total Time 10.00(10.00)
Iter 1080 | Time 4.2675(4.3838) | Bit/dim 1.4800(1.3965) | Steps 490(501.65) | Grad Norm 34.6898(15.5046) | Total Time 10.00(10.00)
Iter 1090 | Time 4.4166(4.3912) | Bit/dim 1.3660(1.3925) | Steps 508(502.20) | Grad Norm 18.4092(15.5762) | Total Time 10.00(10.00)
Iter 1100 | Time 4.1515(4.3741) | Bit/dim 1.3323(1.3873) | Steps 490(501.97) | Grad Norm 5.2479(14.7627) | Total Time 10.00(10.00)
Iter 1110 | Time 4.2606(4.3788) | Bit/dim 1.3148(1.3808) | Steps 502(503.07) | Grad Norm 4.3367(13.9624) | Total Time 10.00(10.00)
Iter 1120 | Time 4.4083(4.3950) | Bit/dim 1.3390(1.3741) | Steps 508(504.64) | Grad Norm 8.3458(12.7824) | Total Time 10.00(10.00)
Iter 1130 | Time 4.5783(4.4166) | Bit/dim 1.4271(1.3724) | Steps 514(506.17) | Grad Norm 29.2868(13.4338) | Total Time 10.00(10.00)
Iter 1140 | Time 4.5603(4.4249) | Bit/dim 1.3857(1.3845) | Steps 514(506.68) | Grad Norm 12.3803(16.0807) | Total Time 10.00(10.00)
Iter 1150 | Time 4.5756(4.4209) | Bit/dim 1.3805(1.3850) | Steps 514(506.11) | Grad Norm 27.8851(17.1534) | Total Time 10.00(10.00)
Iter 1160 | Time 4.3974(4.4079) | Bit/dim 1.3796(1.3824) | Steps 508(505.61) | Grad Norm 8.1448(15.8551) | Total Time 10.00(10.00)
Iter 1170 | Time 4.4161(4.4125) | Bit/dim 1.3785(1.3753) | Steps 508(506.44) | Grad Norm 10.3520(13.7316) | Total Time 10.00(10.00)
Iter 1180 | Time 4.5646(4.4409) | Bit/dim 1.2999(1.3653) | Steps 514(507.96) | Grad Norm 4.6339(11.8283) | Total Time 10.00(10.00)
Iter 1190 | Time 4.5798(4.4564) | Bit/dim 1.3485(1.3602) | Steps 514(508.77) | Grad Norm 9.8226(11.0078) | Total Time 10.00(10.00)
validating...
Epoch 0004 | Time 67.7772, Bit/dim 1.3340
===> Using batch size 200. Total 300 iterations/epoch.
Iter 1200 | Time 4.5636(4.4723) | Bit/dim 1.3138(1.3539) | Steps 514(509.69) | Grad Norm 6.1077(11.6460) | Total Time 10.00(10.00)
Iter 1210 | Time 4.5680(4.4770) | Bit/dim 1.3293(1.3545) | Steps 514(510.03) | Grad Norm 8.0084(12.6078) | Total Time 10.00(10.00)
Iter 1220 | Time 4.5640(4.4810) | Bit/dim 1.3424(1.3571) | Steps 514(510.13) | Grad Norm 24.6275(14.6089) | Total Time 10.00(10.00)
Iter 1230 | Time 4.2515(4.4749) | Bit/dim 1.3521(1.3556) | Steps 502(509.74) | Grad Norm 5.6923(14.8927) | Total Time 10.00(10.00)
Iter 1240 | Time 4.5808(4.4725) | Bit/dim 1.2864(1.3494) | Steps 514(509.79) | Grad Norm 2.8198(13.5001) | Total Time 10.00(10.00)
Iter 1250 | Time 4.5424(4.4796) | Bit/dim 1.3365(1.3486) | Steps 514(510.27) | Grad Norm 23.2764(13.7295) | Total Time 10.00(10.00)
Iter 1260 | Time 4.4073(4.4780) | Bit/dim 1.3360(1.3494) | Steps 508(509.60) | Grad Norm 15.4863(15.0081) | Total Time 10.00(10.00)
Iter 1270 | Time 4.5553(4.4758) | Bit/dim 1.3474(1.3504) | Steps 514(509.70) | Grad Norm 9.1167(14.7274) | Total Time 10.00(10.00)
Iter 1280 | Time 4.4205(4.4799) | Bit/dim 1.3232(1.3449) | Steps 508(510.06) | Grad Norm 10.2365(12.4066) | Total Time 10.00(10.00)
Iter 1290 | Time 4.5674(4.4887) | Bit/dim 1.3576(1.3402) | Steps 514(510.60) | Grad Norm 19.6551(12.1984) | Total Time 10.00(10.00)
Iter 1300 | Time 4.5675(4.5026) | Bit/dim 1.3081(1.3360) | Steps 514(511.21) | Grad Norm 2.1248(11.1826) | Total Time 10.00(10.00)
Iter 1310 | Time 4.5702(4.5108) | Bit/dim 1.3189(1.3410) | Steps 514(511.47) | Grad Norm 21.0774(12.9829) | Total Time 10.00(10.00)
Iter 1320 | Time 4.2467(4.4928) | Bit/dim 1.3325(1.3471) | Steps 502(510.50) | Grad Norm 9.0308(14.9617) | Total Time 10.00(10.00)
Iter 1330 | Time 4.5670(4.4926) | Bit/dim 1.3435(1.3427) | Steps 514(510.63) | Grad Norm 14.0899(14.9615) | Total Time 10.00(10.00)
Iter 1340 | Time 4.4079(4.4876) | Bit/dim 1.3262(1.3431) | Steps 508(510.55) | Grad Norm 3.5756(14.6936) | Total Time 10.00(10.00)
Iter 1350 | Time 4.5653(4.4904) | Bit/dim 1.3098(1.3369) | Steps 514(510.83) | Grad Norm 8.6996(12.9691) | Total Time 10.00(10.00)
Iter 1360 | Time 4.5654(4.5019) | Bit/dim 1.3101(1.3326) | Steps 514(511.36) | Grad Norm 3.8963(12.1681) | Total Time 10.00(10.00)
Iter 1370 | Time 4.4088(4.5073) | Bit/dim 1.2829(1.3283) | Steps 508(511.58) | Grad Norm 5.2505(11.6001) | Total Time 10.00(10.00)
Iter 1380 | Time 4.5426(4.5232) | Bit/dim 1.3451(1.3266) | Steps 514(512.21) | Grad Norm 19.6298(12.2506) | Total Time 10.00(10.00)
Iter 1390 | Time 4.2591(4.5165) | Bit/dim 1.4338(1.3540) | Steps 502(511.90) | Grad Norm 14.5349(16.5473) | Total Time 10.00(10.00)
Iter 1400 | Time 4.4367(4.4903) | Bit/dim 1.3712(1.3581) | Steps 514(510.88) | Grad Norm 12.9024(16.9836) | Total Time 10.00(10.00)
Iter 1410 | Time 4.4374(4.4616) | Bit/dim 1.3455(1.3495) | Steps 514(510.34) | Grad Norm 4.8896(15.1313) | Total Time 10.00(10.00)
Iter 1420 | Time 4.5937(4.4722) | Bit/dim 1.3262(1.3428) | Steps 520(511.79) | Grad Norm 7.7804(13.3882) | Total Time 10.00(10.00)
Iter 1430 | Time 4.4415(4.4848) | Bit/dim 1.2930(1.3316) | Steps 514(512.98) | Grad Norm 2.8647(10.8394) | Total Time 10.00(10.00)
Iter 1440 | Time 4.5766(4.4918) | Bit/dim 1.3589(1.3264) | Steps 514(512.78) | Grad Norm 0.9775(8.6186) | Total Time 10.00(10.00)
Iter 1450 | Time 4.5733(4.5039) | Bit/dim 1.2825(1.3168) | Steps 514(512.79) | Grad Norm 2.5258(7.2907) | Total Time 10.00(10.00)
Iter 1460 | Time 4.5634(4.5206) | Bit/dim 1.3321(1.3123) | Steps 514(513.11) | Grad Norm 15.9070(7.3167) | Total Time 10.00(10.00)
Iter 1470 | Time 4.5726(4.5211) | Bit/dim 1.3492(1.3148) | Steps 514(512.89) | Grad Norm 16.5913(10.4716) | Total Time 10.00(10.00)
Iter 1480 | Time 4.4107(4.5115) | Bit/dim 1.3906(1.3237) | Steps 508(512.02) | Grad Norm 27.1419(14.0332) | Total Time 10.00(10.00)
Iter 1490 | Time 4.3063(4.5022) | Bit/dim 1.3107(1.3304) | Steps 496(511.62) | Grad Norm 5.7644(15.2709) | Total Time 10.00(10.00)
validating...
Epoch 0005 | Time 65.0655, Bit/dim 1.2929
===> Using batch size 200. Total 300 iterations/epoch.
Iter 1500 | Time 4.3798(4.4805) | Bit/dim 1.2896(1.3241) | Steps 508(510.86) | Grad Norm 3.2911(13.9043) | Total Time 10.00(10.00)
Iter 1510 | Time 4.5932(4.4792) | Bit/dim 1.2843(1.3170) | Steps 520(511.44) | Grad Norm 4.6422(11.8152) | Total Time 10.00(10.00)
Iter 1520 | Time 4.5767(4.4916) | Bit/dim 1.2693(1.3078) | Steps 514(511.83) | Grad Norm 1.5856(10.1094) | Total Time 10.00(10.00)
Iter 1530 | Time 4.5121(4.4965) | Bit/dim 1.2634(1.3002) | Steps 508(511.75) | Grad Norm 9.1477(8.6999) | Total Time 10.00(10.00)
Iter 1540 | Time 4.5654(4.5087) | Bit/dim 1.2580(1.2965) | Steps 514(512.02) | Grad Norm 10.3045(7.8247) | Total Time 10.00(10.00)
Iter 1550 | Time 4.5316(4.5179) | Bit/dim 1.3594(1.3032) | Steps 508(512.05) | Grad Norm 22.7573(11.5894) | Total Time 10.00(10.00)
Iter 1560 | Time 4.1379(4.5234) | Bit/dim 1.3722(1.3221) | Steps 490(512.20) | Grad Norm 25.2205(15.1458) | Total Time 10.00(10.00)
Iter 1570 | Time 4.3052(4.4913) | Bit/dim 1.3098(1.3210) | Steps 496(510.66) | Grad Norm 7.1039(14.2826) | Total Time 10.00(10.00)
Iter 1580 | Time 4.5921(4.4756) | Bit/dim 1.2628(1.3103) | Steps 520(510.36) | Grad Norm 5.2570(12.1940) | Total Time 10.00(10.00)
Iter 1590 | Time 4.4443(4.4784) | Bit/dim 1.2595(1.3024) | Steps 514(511.49) | Grad Norm 3.3564(9.7965) | Total Time 10.00(10.00)
Iter 1600 | Time 4.5604(4.4857) | Bit/dim 1.2628(1.2958) | Steps 514(511.92) | Grad Norm 1.6364(7.9291) | Total Time 10.00(10.00)
Iter 1610 | Time 4.5329(4.4946) | Bit/dim 1.2702(1.2881) | Steps 508(511.52) | Grad Norm 2.7494(6.6358) | Total Time 10.00(10.00)
Iter 1620 | Time 4.5327(4.5060) | Bit/dim 1.2935(1.2844) | Steps 508(511.37) | Grad Norm 20.9481(7.0751) | Total Time 10.00(10.00)
Iter 1630 | Time 4.5343(4.5144) | Bit/dim 1.2699(1.2833) | Steps 514(511.59) | Grad Norm 5.3904(7.9363) | Total Time 10.00(10.00)
Iter 1640 | Time 4.5789(4.5207) | Bit/dim 1.2567(1.2794) | Steps 514(511.39) | Grad Norm 12.1640(8.9804) | Total Time 10.00(10.00)
Iter 1650 | Time 4.5567(4.5233) | Bit/dim 1.2644(1.2814) | Steps 514(511.11) | Grad Norm 10.6129(11.3840) | Total Time 10.00(10.00)
Iter 1660 | Time 4.5760(4.5266) | Bit/dim 1.3528(1.3051) | Steps 520(512.17) | Grad Norm 9.9477(15.0587) | Total Time 10.00(10.00)
Iter 1670 | Time 4.2921(4.4976) | Bit/dim 1.2867(1.3170) | Steps 496(511.29) | Grad Norm 6.7388(15.9401) | Total Time 10.00(10.00)
Iter 1680 | Time 4.2986(4.4872) | Bit/dim 1.2786(1.3116) | Steps 496(511.15) | Grad Norm 8.8659(13.9009) | Total Time 10.00(10.00)
Iter 1690 | Time 4.4495(4.4743) | Bit/dim 1.2605(1.3023) | Steps 514(511.46) | Grad Norm 2.3707(11.6152) | Total Time 10.00(10.00)
Iter 1700 | Time 4.5979(4.4612) | Bit/dim 1.2791(1.2929) | Steps 520(511.07) | Grad Norm 3.9120(9.4484) | Total Time 10.00(10.00)
Iter 1710 | Time 4.3635(4.4686) | Bit/dim 1.2425(1.2847) | Steps 508(511.67) | Grad Norm 1.0746(7.8246) | Total Time 10.00(10.00)
Iter 1720 | Time 4.3613(4.4865) | Bit/dim 1.2421(1.2750) | Steps 502(512.52) | Grad Norm 1.2100(6.4410) | Total Time 10.00(10.00)
Iter 1730 | Time 4.5188(4.4981) | Bit/dim 1.2854(1.2700) | Steps 508(512.28) | Grad Norm 10.4055(5.9959) | Total Time 10.00(10.00)
Iter 1740 | Time 4.3326(4.5034) | Bit/dim 1.2313(1.2695) | Steps 502(511.79) | Grad Norm 8.9160(7.1764) | Total Time 10.00(10.00)
Iter 1750 | Time 4.5826(4.5080) | Bit/dim 1.2653(1.2672) | Steps 514(511.57) | Grad Norm 9.1762(8.3509) | Total Time 10.00(10.00)
Iter 1760 | Time 4.5671(4.5103) | Bit/dim 1.2840(1.2697) | Steps 514(511.04) | Grad Norm 11.0526(10.6549) | Total Time 10.00(10.00)
Iter 1770 | Time 4.2398(4.5175) | Bit/dim 1.2800(1.2891) | Steps 502(512.31) | Grad Norm 13.6997(14.7959) | Total Time 10.00(10.00)
Iter 1780 | Time 4.3101(4.4997) | Bit/dim 1.3010(1.2963) | Steps 496(511.19) | Grad Norm 12.5265(15.9932) | Total Time 10.00(10.00)
Iter 1790 | Time 4.5962(4.4876) | Bit/dim 1.2672(1.2894) | Steps 520(510.91) | Grad Norm 3.8258(14.5117) | Total Time 10.00(10.00)
validating...
Epoch 0006 | Time 67.3585, Bit/dim 1.2484
===> Using batch size 200. Total 300 iterations/epoch.
Iter 1800 | Time 4.4420(4.4798) | Bit/dim 1.2403(1.2821) | Steps 514(510.99) | Grad Norm 4.5560(12.6977) | Total Time 10.00(10.00)
Iter 1810 | Time 4.5815(4.4733) | Bit/dim 1.2552(1.2761) | Steps 520(510.91) | Grad Norm 1.8453(10.6612) | Total Time 10.00(10.00)
Iter 1820 | Time 4.3778(4.4798) | Bit/dim 1.2260(1.2658) | Steps 502(511.37) | Grad Norm 4.5877(9.2951) | Total Time 10.00(10.00)
Iter 1830 | Time 4.5285(4.4913) | Bit/dim 1.2370(1.2591) | Steps 508(510.93) | Grad Norm 7.0442(7.9000) | Total Time 10.00(10.00)
Iter 1840 | Time 4.5831(4.5062) | Bit/dim 1.2116(1.2529) | Steps 514(510.98) | Grad Norm 6.9516(7.4885) | Total Time 10.00(10.00)
Iter 1850 | Time 4.5181(4.5082) | Bit/dim 1.2592(1.2509) | Steps 508(510.53) | Grad Norm 7.0999(8.7871) | Total Time 10.00(10.00)
Iter 1860 | Time 4.6908(4.5179) | Bit/dim 1.3270(1.2719) | Steps 532(511.09) | Grad Norm 10.5751(12.0430) | Total Time 10.00(10.00)
Iter 1870 | Time 4.6306(4.5114) | Bit/dim 1.2902(1.2854) | Steps 526(511.42) | Grad Norm 7.8411(11.5233) | Total Time 10.00(10.00)
Iter 1880 | Time 4.2930(4.4919) | Bit/dim 1.2596(1.2815) | Steps 496(511.34) | Grad Norm 5.3606(10.5091) | Total Time 10.00(10.00)
Iter 1890 | Time 4.4965(4.4855) | Bit/dim 1.2125(1.2723) | Steps 514(511.92) | Grad Norm 6.3323(9.4037) | Total Time 10.00(10.00)
Iter 1900 | Time 4.5901(4.4816) | Bit/dim 1.2159(1.2635) | Steps 520(512.48) | Grad Norm 1.5798(8.4257) | Total Time 10.00(10.00)
Iter 1910 | Time 5.0084(4.5047) | Bit/dim 1.2752(1.2605) | Steps 544(513.94) | Grad Norm 33.3646(10.5379) | Total Time 10.00(10.00)
Iter 1920 | Time 4.5485(4.5008) | Bit/dim 1.3073(1.2719) | Steps 514(513.77) | Grad Norm 17.2206(13.7856) | Total Time 10.00(10.00)
Iter 1930 | Time 4.3000(4.4959) | Bit/dim 1.2951(1.2795) | Steps 496(513.20) | Grad Norm 19.6991(15.5056) | Total Time 10.00(10.00)
Iter 1940 | Time 4.5810(4.4781) | Bit/dim 1.2709(1.2773) | Steps 520(512.28) | Grad Norm 16.0003(15.1742) | Total Time 10.00(10.00)
Iter 1950 | Time 4.5915(4.4684) | Bit/dim 1.2513(1.2712) | Steps 520(511.89) | Grad Norm 8.2313(13.1908) | Total Time 10.00(10.00)
Iter 1960 | Time 4.5474(4.4677) | Bit/dim 1.2370(1.2617) | Steps 514(512.18) | Grad Norm 3.2870(10.7594) | Total Time 10.00(10.00)
Iter 1970 | Time 4.3614(4.4584) | Bit/dim 1.2348(1.2544) | Steps 502(511.32) | Grad Norm 2.4424(8.5836) | Total Time 10.00(10.00)
Iter 1980 | Time 4.5651(4.4657) | Bit/dim 1.2317(1.2479) | Steps 514(510.90) | Grad Norm 3.4548(7.4254) | Total Time 10.00(10.00)
Iter 1990 | Time 4.5680(4.4762) | Bit/dim 1.2716(1.2450) | Steps 514(510.64) | Grad Norm 11.3980(7.4795) | Total Time 10.00(10.00)
Iter 2000 | Time 4.5768(4.4794) | Bit/dim 1.2296(1.2394) | Steps 514(510.29) | Grad Norm 6.2789(7.8214) | Total Time 10.00(10.00)
Iter 2010 | Time 4.8299(4.4966) | Bit/dim 1.2464(1.2392) | Steps 532(511.03) | Grad Norm 23.9275(8.8575) | Total Time 10.00(10.00)
Iter 2020 | Time 4.7205(4.5298) | Bit/dim 1.3208(1.2654) | Steps 520(513.69) | Grad Norm 41.1599(13.7521) | Total Time 10.00(10.00)
Iter 2030 | Time 4.8776(4.4948) | Bit/dim 1.3665(1.2922) | Steps 544(512.39) | Grad Norm 15.8748(14.5098) | Total Time 10.00(10.00)
Iter 2040 | Time 4.2622(4.4883) | Bit/dim 1.2982(1.2956) | Steps 496(512.37) | Grad Norm 17.7481(14.9522) | Total Time 10.00(10.00)
Iter 2050 | Time 4.6405(4.4733) | Bit/dim 1.2241(1.2855) | Steps 526(511.87) | Grad Norm 8.9366(13.4540) | Total Time 10.00(10.00)
Iter 2060 | Time 4.4454(4.4548) | Bit/dim 1.2504(1.2727) | Steps 514(511.28) | Grad Norm 4.2472(11.1712) | Total Time 10.00(10.00)
Iter 2070 | Time 4.4392(4.4380) | Bit/dim 1.2603(1.2632) | Steps 514(510.64) | Grad Norm 4.6647(9.2673) | Total Time 10.00(10.00)
Iter 2080 | Time 4.4477(4.4269) | Bit/dim 1.2046(1.2550) | Steps 514(510.13) | Grad Norm 1.8625(8.0484) | Total Time 10.00(10.00)
Iter 2090 | Time 4.5760(4.4215) | Bit/dim 1.2406(1.2441) | Steps 514(508.97) | Grad Norm 5.6612(7.1941) | Total Time 10.00(10.00)
validating...
Epoch 0007 | Time 66.5998, Bit/dim 1.2120
===> Using batch size 200. Total 300 iterations/epoch.
Iter 2100 | Time 4.4109(4.4225) | Bit/dim 1.2181(1.2383) | Steps 508(508.25) | Grad Norm 2.7702(6.0024) | Total Time 10.00(10.00)
Iter 2110 | Time 4.4123(4.4322) | Bit/dim 1.2102(1.2332) | Steps 508(508.15) | Grad Norm 6.9946(5.7905) | Total Time 10.00(10.00)
Iter 2120 | Time 4.3445(4.4357) | Bit/dim 1.2150(1.2283) | Steps 502(507.59) | Grad Norm 7.1610(6.6856) | Total Time 10.00(10.00)
Iter 2130 | Time 4.5197(4.4457) | Bit/dim 1.2001(1.2277) | Steps 508(507.87) | Grad Norm 7.7144(7.4344) | Total Time 10.00(10.00)
Iter 2140 | Time 4.7312(4.4526) | Bit/dim 1.3264(1.2424) | Steps 538(508.65) | Grad Norm 13.2189(11.0457) | Total Time 10.00(10.00)
Iter 2150 | Time 4.1669(4.4709) | Bit/dim 1.4285(1.2656) | Steps 496(510.89) | Grad Norm 34.0946(13.4034) | Total Time 10.00(10.00)
Iter 2160 | Time 4.8951(4.4621) | Bit/dim 1.4350(1.2860) | Steps 544(511.11) | Grad Norm 13.1294(14.5855) | Total Time 10.00(10.00)
Iter 2170 | Time 4.3319(4.4576) | Bit/dim 1.2473(1.2860) | Steps 502(510.71) | Grad Norm 5.5700(12.9171) | Total Time 10.00(10.00)
Iter 2180 | Time 4.3794(4.4324) | Bit/dim 1.2296(1.2726) | Steps 508(509.07) | Grad Norm 2.7078(10.8372) | Total Time 10.00(10.00)
Iter 2190 | Time 4.3802(4.4166) | Bit/dim 1.2161(1.2579) | Steps 508(508.27) | Grad Norm 2.4284(9.0369) | Total Time 10.00(10.00)
Iter 2200 | Time 4.2885(4.3963) | Bit/dim 1.2000(1.2466) | Steps 496(506.57) | Grad Norm 2.4974(7.3676) | Total Time 10.00(10.00)
Iter 2210 | Time 4.4101(4.3827) | Bit/dim 1.2122(1.2393) | Steps 508(505.12) | Grad Norm 6.2459(6.4060) | Total Time 10.00(10.00)
Iter 2220 | Time 4.3225(4.3879) | Bit/dim 1.2158(1.2330) | Steps 496(505.18) | Grad Norm 9.1229(6.2028) | Total Time 10.00(10.00)
Iter 2230 | Time 4.4088(4.3951) | Bit/dim 1.2178(1.2279) | Steps 508(505.75) | Grad Norm 2.3662(6.2058) | Total Time 10.00(10.00)
Iter 2240 | Time 4.6394(4.4141) | Bit/dim 1.2637(1.2365) | Steps 526(506.38) | Grad Norm 11.7094(9.2375) | Total Time 10.00(10.00)
Iter 2250 | Time 4.1611(4.4086) | Bit/dim 1.3841(1.2588) | Steps 496(505.65) | Grad Norm 28.5493(11.9349) | Total Time 10.00(10.00)
Iter 2260 | Time 4.6325(4.4119) | Bit/dim 1.2903(1.2631) | Steps 526(506.95) | Grad Norm 16.0872(12.9863) | Total Time 10.00(10.00)
Iter 2270 | Time 4.7984(4.4155) | Bit/dim 1.2554(1.2589) | Steps 532(507.48) | Grad Norm 23.9103(13.4026) | Total Time 10.00(10.00)
Iter 2280 | Time 4.6168(4.4261) | Bit/dim 1.2114(1.2538) | Steps 520(508.60) | Grad Norm 5.6870(13.2947) | Total Time 10.00(10.00)
Iter 2290 | Time 4.3181(4.4269) | Bit/dim 1.2273(1.2495) | Steps 502(508.73) | Grad Norm 11.0079(13.4963) | Total Time 10.00(10.00)
Iter 2300 | Time 4.1390(4.4261) | Bit/dim 1.2324(1.2445) | Steps 490(509.04) | Grad Norm 14.1869(13.5488) | Total Time 10.00(10.00)
Iter 2310 | Time 4.3578(4.4216) | Bit/dim 1.2266(1.2361) | Steps 502(507.98) | Grad Norm 10.4003(13.0682) | Total Time 10.00(10.00)
Iter 2320 | Time 4.2954(4.4197) | Bit/dim 1.1947(1.2326) | Steps 496(507.32) | Grad Norm 8.7021(12.2762) | Total Time 10.00(10.00)
Iter 2330 | Time 4.5585(4.4393) | Bit/dim 1.2281(1.2300) | Steps 514(508.67) | Grad Norm 14.4022(12.1364) | Total Time 10.00(10.00)
Iter 2340 | Time 4.1376(4.4376) | Bit/dim 1.2382(1.2271) | Steps 490(508.20) | Grad Norm 18.5060(12.5707) | Total Time 10.00(10.00)
Iter 2350 | Time 4.3091(4.4394) | Bit/dim 1.2445(1.2270) | Steps 496(508.34) | Grad Norm 9.9625(12.3953) | Total Time 10.00(10.00)
Iter 2360 | Time 4.3577(4.4436) | Bit/dim 1.1848(1.2237) | Steps 502(508.69) | Grad Norm 7.2846(11.8430) | Total Time 10.00(10.00)
Iter 2370 | Time 4.1567(4.4418) | Bit/dim 1.3050(1.2289) | Steps 490(509.28) | Grad Norm 17.9025(14.1050) | Total Time 10.00(10.00)
Iter 2380 | Time 4.7227(4.4559) | Bit/dim 1.2531(1.2336) | Steps 520(510.24) | Grad Norm 25.9622(14.2585) | Total Time 10.00(10.00)
Iter 2390 | Time 4.3779(4.4437) | Bit/dim 1.1933(1.2362) | Steps 508(509.66) | Grad Norm 6.2293(14.5384) | Total Time 10.00(10.00)
validating...
Epoch 0008 | Time 66.7275, Bit/dim 1.2090
===> Using batch size 200. Total 300 iterations/epoch.
Iter 2400 | Time 4.3946(4.4425) | Bit/dim 1.1821(1.2306) | Steps 508(510.00) | Grad Norm 9.2033(14.0577) | Total Time 10.00(10.00)
Iter 2410 | Time 4.4335(4.4449) | Bit/dim 1.2166(1.2252) | Steps 514(510.09) | Grad Norm 4.6897(12.1908) | Total Time 10.00(10.00)
Iter 2420 | Time 4.7593(4.4561) | Bit/dim 1.1926(1.2207) | Steps 526(510.85) | Grad Norm 14.7370(11.3130) | Total Time 10.00(10.00)
Iter 2430 | Time 4.1507(4.4650) | Bit/dim 1.2128(1.2187) | Steps 490(511.80) | Grad Norm 15.0428(11.5005) | Total Time 10.00(10.00)
Iter 2440 | Time 4.3762(4.4659) | Bit/dim 1.1795(1.2166) | Steps 508(511.93) | Grad Norm 11.1588(11.8975) | Total Time 10.00(10.00)
Iter 2450 | Time 4.2782(4.4650) | Bit/dim 1.2436(1.2165) | Steps 508(512.34) | Grad Norm 14.6330(12.5818) | Total Time 10.00(10.00)
Iter 2460 | Time 4.5609(4.4872) | Bit/dim 1.1661(1.2107) | Steps 514(513.09) | Grad Norm 4.6343(11.2569) | Total Time 10.00(10.00)
Iter 2470 | Time 4.6424(4.5012) | Bit/dim 1.1918(1.2077) | Steps 526(514.14) | Grad Norm 2.3442(9.8062) | Total Time 10.00(10.00)
Iter 2480 | Time 4.6404(4.5018) | Bit/dim 1.2353(1.2072) | Steps 526(514.59) | Grad Norm 5.1494(8.6366) | Total Time 10.00(10.00)
Iter 2490 | Time 4.6440(4.5081) | Bit/dim 1.2354(1.2035) | Steps 526(514.78) | Grad Norm 5.2398(8.4328) | Total Time 10.00(10.00)
Iter 2500 | Time 4.5492(4.5192) | Bit/dim 1.2035(1.2041) | Steps 514(515.24) | Grad Norm 2.9458(8.7099) | Total Time 10.00(10.00)
Iter 2510 | Time 4.3150(4.5276) | Bit/dim 1.3268(1.2111) | Steps 496(515.91) | Grad Norm 22.8955(11.0255) | Total Time 10.00(10.00)
Iter 2520 | Time 4.3443(4.5182) | Bit/dim 1.2443(1.2281) | Steps 502(515.56) | Grad Norm 13.5028(13.0500) | Total Time 10.00(10.00)
Iter 2530 | Time 4.5995(4.5064) | Bit/dim 1.3356(1.2506) | Steps 520(515.01) | Grad Norm 10.2140(14.6335) | Total Time 10.00(10.00)
Iter 2540 | Time 4.3633(4.4914) | Bit/dim 1.2336(1.2527) | Steps 508(514.29) | Grad Norm 2.6050(14.0247) | Total Time 10.00(10.00)
Iter 2550 | Time 4.4821(4.4761) | Bit/dim 1.2401(1.2544) | Steps 520(514.48) | Grad Norm 6.6974(14.5086) | Total Time 10.00(10.00)
Iter 2560 | Time 4.3744(4.4548) | Bit/dim 1.2089(1.2439) | Steps 508(512.88) | Grad Norm 3.2656(11.8853) | Total Time 10.00(10.00)
Iter 2570 | Time 4.3806(4.4408) | Bit/dim 1.1880(1.2325) | Steps 508(511.15) | Grad Norm 2.6903(9.5251) | Total Time 10.00(10.00)
Iter 2580 | Time 4.4799(4.4243) | Bit/dim 1.1658(1.2193) | Steps 520(510.22) | Grad Norm 2.0504(7.5685) | Total Time 10.00(10.00)
Iter 2590 | Time 4.4386(4.4188) | Bit/dim 1.1889(1.2100) | Steps 514(509.95) | Grad Norm 2.5292(6.0399) | Total Time 10.00(10.00)
Iter 2600 | Time 4.3012(4.4202) | Bit/dim 1.2055(1.2052) | Steps 496(510.03) | Grad Norm 10.2771(6.0088) | Total Time 10.00(10.00)
Iter 2610 | Time 4.6443(4.4363) | Bit/dim 1.1887(1.2031) | Steps 526(511.13) | Grad Norm 8.1622(6.1037) | Total Time 10.00(10.00)
Iter 2620 | Time 4.4605(4.4509) | Bit/dim 1.1944(1.2021) | Steps 508(511.57) | Grad Norm 11.2351(7.5385) | Total Time 10.00(10.00)
Iter 2630 | Time 4.4507(4.4686) | Bit/dim 1.2300(1.2124) | Steps 514(513.21) | Grad Norm 11.5147(10.1489) | Total Time 10.00(10.00)
Iter 2640 | Time 4.3500(4.4581) | Bit/dim 1.2027(1.2097) | Steps 502(511.78) | Grad Norm 4.9149(10.6663) | Total Time 10.00(10.00)
Iter 2650 | Time 4.1877(4.4635) | Bit/dim 1.2280(1.2150) | Steps 496(512.21) | Grad Norm 14.8460(12.4101) | Total Time 10.00(10.00)
Iter 2660 | Time 4.2630(4.4728) | Bit/dim 1.1749(1.2159) | Steps 508(512.71) | Grad Norm 9.7824(13.5515) | Total Time 10.00(10.00)
Iter 2670 | Time 4.7886(4.4751) | Bit/dim 1.2576(1.2173) | Steps 532(512.91) | Grad Norm 31.2836(14.2249) | Total Time 10.00(10.00)
Iter 2680 | Time 4.4818(4.4667) | Bit/dim 1.1928(1.2153) | Steps 520(512.68) | Grad Norm 4.8515(13.6164) | Total Time 10.00(10.00)
Iter 2690 | Time 4.6283(4.4698) | Bit/dim 1.1891(1.2102) | Steps 526(512.44) | Grad Norm 8.6315(12.3032) | Total Time 10.00(10.00)
validating...
Epoch 0009 | Time 68.2635, Bit/dim 1.1781
===> Using batch size 200. Total 300 iterations/epoch.
Iter 2700 | Time 4.3787(4.4640) | Bit/dim 1.1964(1.2031) | Steps 508(512.70) | Grad Norm 3.5851(10.1257) | Total Time 10.00(10.00)
Iter 2710 | Time 4.3169(4.4611) | Bit/dim 1.2083(1.1998) | Steps 496(512.68) | Grad Norm 9.6374(8.7318) | Total Time 10.00(10.00)
Iter 2720 | Time 4.6484(4.4875) | Bit/dim 1.1714(1.1936) | Steps 526(514.58) | Grad Norm 8.5880(8.2702) | Total Time 10.00(10.00)
Iter 2730 | Time 4.5975(4.5126) | Bit/dim 1.2177(1.1906) | Steps 514(515.52) | Grad Norm 7.3757(7.5469) | Total Time 10.00(10.00)
Iter 2740 | Time 4.3778(4.5142) | Bit/dim 1.1564(1.1874) | Steps 508(515.46) | Grad Norm 5.4500(7.1252) | Total Time 10.00(10.00)
Iter 2750 | Time 4.5561(4.5284) | Bit/dim 1.1701(1.1841) | Steps 514(516.14) | Grad Norm 1.0385(6.4852) | Total Time 10.00(10.00)
Iter 2760 | Time 4.6395(4.5242) | Bit/dim 1.1653(1.1830) | Steps 526(516.20) | Grad Norm 4.2885(6.7350) | Total Time 10.00(10.00)
Iter 2770 | Time 4.3026(4.5289) | Bit/dim 1.2663(1.1884) | Steps 496(515.77) | Grad Norm 22.4550(8.9905) | Total Time 10.00(10.00)
Iter 2780 | Time 4.6275(4.5461) | Bit/dim 1.1894(1.2059) | Steps 526(517.94) | Grad Norm 5.9208(11.9032) | Total Time 10.00(10.00)
Iter 2790 | Time 4.3402(4.5151) | Bit/dim 1.2489(1.2176) | Steps 502(516.15) | Grad Norm 5.5855(13.1622) | Total Time 10.00(10.00)
Iter 2800 | Time 4.9838(4.5099) | Bit/dim 1.2651(1.2320) | Steps 556(517.02) | Grad Norm 13.7737(14.5391) | Total Time 10.00(10.00)
Iter 2810 | Time 4.3356(4.4866) | Bit/dim 1.2890(1.2369) | Steps 502(515.21) | Grad Norm 16.6794(14.4509) | Total Time 10.00(10.00)
Iter 2820 | Time 4.6279(4.4669) | Bit/dim 1.2072(1.2332) | Steps 526(513.43) | Grad Norm 10.7834(13.8397) | Total Time 10.00(10.00)
Iter 2830 | Time 4.3408(4.4500) | Bit/dim 1.1834(1.2235) | Steps 502(511.69) | Grad Norm 3.4870(12.4456) | Total Time 10.00(10.00)
Iter 2840 | Time 4.4451(4.4490) | Bit/dim 1.1966(1.2139) | Steps 514(511.01) | Grad Norm 4.5231(11.4293) | Total Time 10.00(10.00)
Iter 2850 | Time 4.7537(4.4568) | Bit/dim 1.1808(1.2076) | Steps 526(511.09) | Grad Norm 15.8605(10.7072) | Total Time 10.00(10.00)
Iter 2860 | Time 4.3597(4.4525) | Bit/dim 1.1802(1.2021) | Steps 502(510.83) | Grad Norm 10.3127(11.5695) | Total Time 10.00(10.00)
Iter 2870 | Time 4.3425(4.4562) | Bit/dim 1.1849(1.1965) | Steps 502(510.37) | Grad Norm 5.2233(11.2768) | Total Time 10.00(10.00)
Iter 2880 | Time 4.7381(4.4664) | Bit/dim 1.1761(1.1959) | Steps 526(511.08) | Grad Norm 16.4362(12.1212) | Total Time 10.00(10.00)
Iter 2890 | Time 4.3285(4.4597) | Bit/dim 1.1624(1.1927) | Steps 502(511.58) | Grad Norm 6.1804(10.5810) | Total Time 10.00(10.00)
Iter 2900 | Time 4.4473(4.4683) | Bit/dim 1.1929(1.1909) | Steps 514(512.52) | Grad Norm 2.9800(8.8282) | Total Time 10.00(10.00)
Iter 2910 | Time 4.7318(4.4765) | Bit/dim 1.1456(1.1846) | Steps 526(513.71) | Grad Norm 3.6839(7.1760) | Total Time 10.00(10.00)
Iter 2920 | Time 4.4137(4.4851) | Bit/dim 1.1659(1.1813) | Steps 514(514.68) | Grad Norm 11.2783(7.1318) | Total Time 10.00(10.00)
Iter 2930 | Time 5.1561(4.5088) | Bit/dim 1.2622(1.1882) | Steps 568(516.54) | Grad Norm 42.1361(9.5020) | Total Time 10.00(10.00)
Iter 2940 | Time 4.4590(4.5132) | Bit/dim 1.1543(1.1958) | Steps 520(517.62) | Grad Norm 4.3145(10.6691) | Total Time 10.00(10.00)
Iter 2950 | Time 4.2902(4.5177) | Bit/dim 1.3267(1.2088) | Steps 502(518.52) | Grad Norm 15.9286(13.4876) | Total Time 10.00(10.00)
Iter 2960 | Time 4.3275(4.5136) | Bit/dim 1.2210(1.2138) | Steps 502(518.11) | Grad Norm 5.7378(12.4380) | Total Time 10.00(10.00)
Iter 2970 | Time 4.4499(4.4946) | Bit/dim 1.1992(1.2104) | Steps 514(517.04) | Grad Norm 7.6701(12.0709) | Total Time 10.00(10.00)
Iter 2980 | Time 4.4397(4.4637) | Bit/dim 1.1553(1.2007) | Steps 514(513.90) | Grad Norm 8.2218(10.2171) | Total Time 10.00(10.00)
Iter 2990 | Time 4.3535(4.4356) | Bit/dim 1.1449(1.1921) | Steps 508(511.91) | Grad Norm 4.2919(8.3778) | Total Time 10.00(10.00)
validating...
Epoch 0010 | Time 67.8624, Bit/dim 1.1632
===> Using batch size 200. Total 300 iterations/epoch.
Iter 3000 | Time 4.4534(4.4265) | Bit/dim 1.1826(1.1862) | Steps 520(511.85) | Grad Norm 2.4621(7.1426) | Total Time 10.00(10.00)
Iter 3010 | Time 4.3099(4.4179) | Bit/dim 1.1879(1.1787) | Steps 502(511.55) | Grad Norm 6.8984(6.3309) | Total Time 10.00(10.00)
Iter 3020 | Time 4.4650(4.4350) | Bit/dim 1.1621(1.1747) | Steps 520(512.97) | Grad Norm 5.9750(6.1532) | Total Time 10.00(10.00)
Iter 3030 | Time 4.4679(4.4502) | Bit/dim 1.1392(1.1728) | Steps 520(514.49) | Grad Norm 2.2906(5.7664) | Total Time 10.00(10.00)
Iter 3040 | Time 4.5112(4.4648) | Bit/dim 1.1886(1.1735) | Steps 526(515.61) | Grad Norm 4.3290(6.0741) | Total Time 10.00(10.00)
Iter 3050 | Time 5.0018(4.4919) | Bit/dim 1.2793(1.1817) | Steps 562(517.98) | Grad Norm 11.2463(8.5751) | Total Time 10.00(10.00)
Iter 3060 | Time 4.3954(4.5104) | Bit/dim 1.2469(1.1974) | Steps 514(519.16) | Grad Norm 15.4114(11.4675) | Total Time 10.00(10.00)
Iter 3070 | Time 4.6207(4.5264) | Bit/dim 1.4360(1.2322) | Steps 526(521.70) | Grad Norm 7.4518(13.1730) | Total Time 10.00(10.00)
Iter 3080 | Time 4.4113(4.5003) | Bit/dim 1.2191(1.2524) | Steps 514(519.16) | Grad Norm 4.3900(11.2444) | Total Time 10.00(10.00)
Iter 3090 | Time 4.5047(4.4880) | Bit/dim 1.1789(1.2396) | Steps 508(517.41) | Grad Norm 8.5036(9.8953) | Total Time 10.00(10.00)
Iter 3100 | Time 4.3260(4.4783) | Bit/dim 1.1828(1.2247) | Steps 508(516.08) | Grad Norm 4.7844(8.7165) | Total Time 10.00(10.00)
Iter 3110 | Time 4.3738(4.4524) | Bit/dim 1.1642(1.2101) | Steps 508(513.96) | Grad Norm 3.1566(7.3083) | Total Time 10.00(10.00)
Iter 3120 | Time 4.5096(4.4462) | Bit/dim 1.1541(1.2005) | Steps 526(514.46) | Grad Norm 2.9759(6.1759) | Total Time 10.00(10.00)
Iter 3130 | Time 4.3725(4.4552) | Bit/dim 1.1495(1.1882) | Steps 508(515.73) | Grad Norm 7.1964(5.7781) | Total Time 10.00(10.00)
Iter 3140 | Time 4.3804(4.4608) | Bit/dim 1.1652(1.1834) | Steps 508(517.29) | Grad Norm 6.2790(6.1570) | Total Time 10.00(10.00)
Iter 3150 | Time 4.5119(4.4676) | Bit/dim 1.1744(1.1800) | Steps 526(518.79) | Grad Norm 1.8341(5.1891) | Total Time 10.00(10.00)
Iter 3160 | Time 4.6118(4.4822) | Bit/dim 1.1430(1.1751) | Steps 526(520.39) | Grad Norm 7.7054(4.9057) | Total Time 10.00(10.00)
Iter 3170 | Time 4.4948(4.5169) | Bit/dim 1.1615(1.1760) | Steps 526(522.95) | Grad Norm 2.5335(5.4037) | Total Time 10.00(10.00)
Iter 3180 | Time 4.8359(4.5487) | Bit/dim 1.1773(1.1741) | Steps 538(524.52) | Grad Norm 27.2729(6.9628) | Total Time 10.00(10.00)
Iter 3190 | Time 4.8769(4.5566) | Bit/dim 1.2888(1.1964) | Steps 544(524.96) | Grad Norm 9.6320(10.4481) | Total Time 10.00(10.00)
Iter 3200 | Time 4.6312(4.5748) | Bit/dim 1.2330(1.2267) | Steps 538(527.20) | Grad Norm 7.3279(11.5204) | Total Time 10.00(10.00)
Iter 3210 | Time 4.5860(4.5767) | Bit/dim 1.3015(1.2718) | Steps 544(530.33) | Grad Norm 9.9012(12.3266) | Total Time 10.00(10.00)
Iter 3220 | Time 4.2977(4.5316) | Bit/dim 1.2460(1.2715) | Steps 514(525.28) | Grad Norm 5.5016(11.0000) | Total Time 10.00(10.00)
Iter 3230 | Time 4.2868(4.4816) | Bit/dim 1.1875(1.2544) | Steps 502(520.67) | Grad Norm 2.6205(9.2581) | Total Time 10.00(10.00)
Iter 3240 | Time 4.3616(4.4548) | Bit/dim 1.2007(1.2378) | Steps 502(518.08) | Grad Norm 2.6238(7.4391) | Total Time 10.00(10.00)
Iter 3250 | Time 4.5497(4.4604) | Bit/dim 1.1648(1.2177) | Steps 514(517.32) | Grad Norm 2.4040(6.0896) | Total Time 10.00(10.00)
Iter 3260 | Time 4.4169(4.4514) | Bit/dim 1.1634(1.2054) | Steps 514(516.31) | Grad Norm 1.2650(5.1928) | Total Time 10.00(10.00)
Iter 3270 | Time 4.4294(4.4474) | Bit/dim 1.1552(1.1934) | Steps 520(516.50) | Grad Norm 2.4611(4.3844) | Total Time 10.00(10.00)
Iter 3280 | Time 4.4314(4.4455) | Bit/dim 1.1485(1.1859) | Steps 520(516.81) | Grad Norm 3.7999(4.1123) | Total Time 10.00(10.00)
Iter 3290 | Time 4.3695(4.4577) | Bit/dim 1.1319(1.1777) | Steps 508(517.62) | Grad Norm 2.2970(3.6351) | Total Time 10.00(10.00)
validating...
Epoch 0011 | Time 65.3877, Bit/dim 1.1600
===> Using batch size 200. Total 300 iterations/epoch.
Iter 3300 | Time 4.5945(4.4735) | Bit/dim 1.1865(1.1741) | Steps 526(518.80) | Grad Norm 9.9300(4.2535) | Total Time 10.00(10.00)
Iter 3310 | Time 4.6737(4.4957) | Bit/dim 1.1449(1.1689) | Steps 532(519.96) | Grad Norm 11.6526(5.3372) | Total Time 10.00(10.00)
Iter 3320 | Time 4.3139(4.5087) | Bit/dim 1.3498(1.1815) | Steps 502(520.59) | Grad Norm 14.3089(8.2713) | Total Time 10.00(10.00)
Iter 3330 | Time 4.6205(4.5316) | Bit/dim 1.1557(1.1975) | Steps 532(523.27) | Grad Norm 4.3383(8.0800) | Total Time 10.00(10.00)
Iter 3340 | Time 4.7526(4.5382) | Bit/dim 1.1818(1.1958) | Steps 526(522.95) | Grad Norm 12.3652(8.0620) | Total Time 10.00(10.00)
Iter 3350 | Time 5.0771(4.5345) | Bit/dim 1.4223(1.2248) | Steps 574(523.72) | Grad Norm 8.7066(10.3803) | Total Time 10.00(10.00)
Iter 3360 | Time 4.4124(4.5071) | Bit/dim 1.2394(1.2426) | Steps 514(521.43) | Grad Norm 4.7861(10.5489) | Total Time 10.00(10.00)
Iter 3370 | Time 4.6838(4.4823) | Bit/dim 1.1879(1.2314) | Steps 520(518.96) | Grad Norm 4.4371(9.7841) | Total Time 10.00(10.00)
Iter 3380 | Time 4.4011(4.4497) | Bit/dim 1.1895(1.2148) | Steps 514(517.02) | Grad Norm 3.7084(8.2589) | Total Time 10.00(10.00)
Iter 3390 | Time 4.5135(4.4537) | Bit/dim 1.1611(1.2011) | Steps 514(516.22) | Grad Norm 2.4049(6.7397) | Total Time 10.00(10.00)
Iter 3400 | Time 4.4135(4.4552) | Bit/dim 1.1641(1.1880) | Steps 514(515.97) | Grad Norm 2.9958(5.8535) | Total Time 10.00(10.00)
Iter 3410 | Time 4.6265(4.4513) | Bit/dim 1.1381(1.1799) | Steps 532(515.81) | Grad Norm 2.1376(5.1224) | Total Time 10.00(10.00)
Iter 3420 | Time 4.5223(4.4529) | Bit/dim 1.1652(1.1743) | Steps 514(516.28) | Grad Norm 3.4804(4.7381) | Total Time 10.00(10.00)
Iter 3430 | Time 4.4083(4.4675) | Bit/dim 1.1582(1.1699) | Steps 520(517.69) | Grad Norm 11.2713(5.3167) | Total Time 10.00(10.00)
Iter 3440 | Time 4.3781(4.4862) | Bit/dim 1.1854(1.1693) | Steps 514(518.97) | Grad Norm 15.8016(6.3313) | Total Time 10.00(10.00)
Iter 3450 | Time 4.0268(4.4927) | Bit/dim 1.3102(1.1759) | Steps 496(520.45) | Grad Norm 23.1508(8.8601) | Total Time 10.00(10.00)
Iter 3460 | Time 4.5213(4.5049) | Bit/dim 1.4688(1.2120) | Steps 514(521.47) | Grad Norm 7.4657(11.9107) | Total Time 10.00(10.00)
Iter 3470 | Time 4.1950(4.4714) | Bit/dim 1.3204(1.2555) | Steps 496(519.76) | Grad Norm 4.0053(10.0361) | Total Time 10.00(10.00)
Iter 3480 | Time 4.5130(4.4686) | Bit/dim 1.2216(1.2481) | Steps 514(518.16) | Grad Norm 3.6745(8.2150) | Total Time 10.00(10.00)
Iter 3490 | Time 4.4105(4.4834) | Bit/dim 1.1860(1.2319) | Steps 514(518.47) | Grad Norm 1.3034(6.7899) | Total Time 10.00(10.00)
Iter 3500 | Time 4.4077(4.4922) | Bit/dim 1.1697(1.2152) | Steps 514(518.54) | Grad Norm 2.3021(5.7411) | Total Time 10.00(10.00)
Iter 3510 | Time 4.5427(4.4881) | Bit/dim 1.1578(1.2005) | Steps 520(517.97) | Grad Norm 1.3677(4.5305) | Total Time 10.00(10.00)
Iter 3520 | Time 4.5779(4.4892) | Bit/dim 1.1567(1.1872) | Steps 520(517.86) | Grad Norm 3.5511(4.0697) | Total Time 10.00(10.00)
Iter 3530 | Time 4.3958(4.4826) | Bit/dim 1.1177(1.1768) | Steps 514(517.46) | Grad Norm 3.0883(3.3684) | Total Time 10.00(10.00)
Iter 3540 | Time 4.3787(4.4941) | Bit/dim 1.1480(1.1710) | Steps 508(518.88) | Grad Norm 11.5602(4.3175) | Total Time 10.00(10.00)
Iter 3550 | Time 5.0784(4.5367) | Bit/dim 1.2921(1.1928) | Steps 574(523.13) | Grad Norm 8.8262(7.5531) | Total Time 10.00(10.00)
Iter 3560 | Time 4.7229(4.5444) | Bit/dim 1.1713(1.1966) | Steps 526(523.31) | Grad Norm 5.7121(7.2673) | Total Time 10.00(10.00)
Iter 3570 | Time 4.4721(4.5460) | Bit/dim 1.1849(1.1917) | Steps 520(522.86) | Grad Norm 2.9928(6.4343) | Total Time 10.00(10.00)
Iter 3580 | Time 4.3705(4.5210) | Bit/dim 1.1357(1.1819) | Steps 508(520.98) | Grad Norm 2.1221(5.7936) | Total Time 10.00(10.00)
Iter 3590 | Time 4.5672(4.5086) | Bit/dim 1.1491(1.1742) | Steps 520(519.48) | Grad Norm 5.6435(5.0978) | Total Time 10.00(10.00)
validating...
Epoch 0012 | Time 78.1028, Bit/dim 1.2751
===> Using batch size 200. Total 300 iterations/epoch.
Iter 3600 | Time 5.3334(4.5394) | Bit/dim 1.2801(1.1787) | Steps 574(521.15) | Grad Norm 56.9028(8.1072) | Total Time 10.00(10.00)
Iter 3610 | Time 3.9777(4.5381) | Bit/dim 1.3356(1.2116) | Steps 484(522.20) | Grad Norm 21.3798(9.9927) | Total Time 10.00(10.00)
Iter 3620 | Time 4.3138(4.5012) | Bit/dim 1.2373(1.2263) | Steps 502(519.96) | Grad Norm 10.2468(10.0294) | Total Time 10.00(10.00)
Iter 3630 | Time 4.1032(4.4785) | Bit/dim 1.2192(1.2210) | Steps 502(520.19) | Grad Norm 10.8312(10.1488) | Total Time 10.00(10.00)
Iter 3640 | Time 4.2507(4.4637) | Bit/dim 1.2033(1.2153) | Steps 508(519.91) | Grad Norm 15.4000(11.3410) | Total Time 10.00(10.00)
Iter 3650 | Time 4.3566(4.4520) | Bit/dim 1.2278(1.2129) | Steps 508(519.06) | Grad Norm 10.1758(12.4414) | Total Time 10.00(10.00)
Iter 3660 | Time 4.5413(4.4649) | Bit/dim 1.2677(1.2254) | Steps 532(520.57) | Grad Norm 8.5792(13.1940) | Total Time 10.00(10.00)
Iter 3670 | Time 4.4712(4.4710) | Bit/dim 1.1668(1.2209) | Steps 520(521.71) | Grad Norm 7.8407(11.9096) | Total Time 10.00(10.00)
Iter 3680 | Time 4.5902(4.4597) | Bit/dim 1.1881(1.2095) | Steps 520(521.11) | Grad Norm 6.3430(10.4679) | Total Time 10.00(10.00)
Iter 3690 | Time 4.5433(4.4485) | Bit/dim 1.1525(1.1962) | Steps 520(519.53) | Grad Norm 2.0325(8.3902) | Total Time 10.00(10.00)
Iter 3700 | Time 4.5746(4.4658) | Bit/dim 1.1359(1.1867) | Steps 520(519.63) | Grad Norm 2.0591(6.6810) | Total Time 10.00(10.00)
Iter 3710 | Time 4.5617(4.4919) | Bit/dim 1.1410(1.1761) | Steps 520(519.43) | Grad Norm 1.8064(5.4411) | Total Time 10.00(10.00)
Iter 3720 | Time 4.6211(4.5077) | Bit/dim 1.1685(1.1681) | Steps 526(519.28) | Grad Norm 2.5224(4.5496) | Total Time 10.00(10.00)
Iter 3730 | Time 4.5120(4.5042) | Bit/dim 1.1564(1.1625) | Steps 508(517.24) | Grad Norm 4.1022(4.2047) | Total Time 10.00(10.00)
Iter 3740 | Time 4.4981(4.5037) | Bit/dim 1.1515(1.1578) | Steps 508(515.75) | Grad Norm 3.6344(3.7858) | Total Time 10.00(10.00)
Iter 3750 | Time 4.3766(4.5033) | Bit/dim 1.1408(1.1556) | Steps 508(516.64) | Grad Norm 1.7680(3.4582) | Total Time 10.00(10.00)
Iter 3760 | Time 4.4499(4.5226) | Bit/dim 1.1315(1.1525) | Steps 520(518.56) | Grad Norm 5.8472(3.9108) | Total Time 10.00(10.00)
Iter 3770 | Time 4.6730(4.5296) | Bit/dim 1.1637(1.1517) | Steps 532(518.82) | Grad Norm 11.7398(5.0671) | Total Time 10.00(10.00)
Iter 3780 | Time 4.1711(4.5333) | Bit/dim 1.2695(1.1606) | Steps 496(519.06) | Grad Norm 16.3173(7.9408) | Total Time 10.00(10.00)
Iter 3790 | Time 4.4222(4.5539) | Bit/dim 1.5385(1.2138) | Steps 520(521.01) | Grad Norm 6.7399(10.9324) | Total Time 10.00(10.00)
Iter 3800 | Time 4.4518(4.5571) | Bit/dim 1.4907(1.2810) | Steps 514(522.19) | Grad Norm 3.1942(8.9905) | Total Time 10.00(10.00)
Iter 3810 | Time 4.3580(4.5210) | Bit/dim 1.3466(1.3127) | Steps 526(521.64) | Grad Norm 2.1249(7.2086) | Total Time 10.00(10.00)
Iter 3820 | Time 4.3352(4.4955) | Bit/dim 1.3254(1.3203) | Steps 520(522.93) | Grad Norm 1.5608(5.8949) | Total Time 10.00(10.00)
Iter 3830 | Time 4.6318(4.4886) | Bit/dim 1.2775(1.3100) | Steps 526(522.20) | Grad Norm 2.3087(5.0089) | Total Time 10.00(10.00)
Iter 3840 | Time 4.7665(4.5399) | Bit/dim 1.2036(1.2881) | Steps 526(522.42) | Grad Norm 3.7539(4.3249) | Total Time 10.00(10.00)
Iter 3850 | Time 4.2390(4.5165) | Bit/dim 1.1739(1.2635) | Steps 502(519.76) | Grad Norm 6.3353(5.3062) | Total Time 10.00(10.00)
Iter 3860 | Time 4.3090(4.4870) | Bit/dim 1.3046(1.2699) | Steps 520(517.46) | Grad Norm 8.5066(8.5052) | Total Time 10.00(10.00)
Iter 3870 | Time 4.1477(4.4430) | Bit/dim 1.2677(1.2726) | Steps 490(515.09) | Grad Norm 8.5506(8.7675) | Total Time 10.00(10.00)
Iter 3880 | Time 4.3976(4.4107) | Bit/dim 1.1483(1.2572) | Steps 508(513.05) | Grad Norm 4.9046(9.4617) | Total Time 10.00(10.00)
Iter 3890 | Time 4.3726(4.4031) | Bit/dim 1.1626(1.2461) | Steps 508(513.22) | Grad Norm 2.3488(10.1496) | Total Time 10.00(10.00)
validating...
Epoch 0013 | Time 63.2181, Bit/dim 1.1740
===> Using batch size 200. Total 300 iterations/epoch.
Iter 3900 | Time 4.6086(4.3838) | Bit/dim 1.1713(1.2307) | Steps 526(512.96) | Grad Norm 15.0784(10.0837) | Total Time 10.00(10.00)
Iter 3910 | Time 4.2173(4.3803) | Bit/dim 1.1861(1.2169) | Steps 508(513.97) | Grad Norm 11.4027(10.2058) | Total Time 10.00(10.00)
Iter 3920 | Time 4.4256(4.3889) | Bit/dim 1.1787(1.2086) | Steps 514(515.28) | Grad Norm 22.4681(10.8594) | Total Time 10.00(10.00)
Iter 3930 | Time 4.2481(4.3758) | Bit/dim 1.1782(1.1990) | Steps 508(514.56) | Grad Norm 10.3789(10.3979) | Total Time 10.00(10.00)
Iter 3940 | Time 4.8375(4.3831) | Bit/dim 1.2233(1.1905) | Steps 544(514.61) | Grad Norm 30.2805(11.1602) | Total Time 10.00(10.00)
Iter 3950 | Time 4.4220(4.3875) | Bit/dim 1.1806(1.1870) | Steps 514(514.44) | Grad Norm 8.0453(11.0481) | Total Time 10.00(10.00)
Iter 3960 | Time 4.5675(4.3834) | Bit/dim 1.1667(1.1764) | Steps 520(512.47) | Grad Norm 2.8197(9.5848) | Total Time 10.00(10.00)
Iter 3970 | Time 4.5991(4.4190) | Bit/dim 1.1285(1.1697) | Steps 526(512.82) | Grad Norm 1.3644(7.9569) | Total Time 10.00(10.00)
Iter 3980 | Time 4.5435(4.4348) | Bit/dim 1.1546(1.1621) | Steps 520(512.81) | Grad Norm 2.5165(6.7495) | Total Time 10.00(10.00)
Iter 3990 | Time 4.4817(4.4533) | Bit/dim 1.1342(1.1593) | Steps 508(512.65) | Grad Norm 2.7068(6.2218) | Total Time 10.00(10.00)
Iter 4000 | Time 4.3626(4.4422) | Bit/dim 1.1245(1.1591) | Steps 508(512.17) | Grad Norm 6.7347(7.8041) | Total Time 10.00(10.00)
Iter 4010 | Time 4.5618(4.4611) | Bit/dim 1.1224(1.1682) | Steps 520(514.11) | Grad Norm 5.8054(9.7296) | Total Time 10.00(10.00)
Iter 4020 | Time 4.4584(4.4547) | Bit/dim 1.1575(1.1649) | Steps 520(513.54) | Grad Norm 17.1328(9.8822) | Total Time 10.00(10.00)
Iter 4030 | Time 4.5275(4.4490) | Bit/dim 1.1348(1.1602) | Steps 514(512.26) | Grad Norm 7.8553(8.7167) | Total Time 10.00(10.00)
Iter 4040 | Time 4.0039(4.4413) | Bit/dim 1.2113(1.1580) | Steps 496(512.22) | Grad Norm 15.1371(9.0938) | Total Time 10.00(10.00)
Iter 4050 | Time 4.8281(4.4624) | Bit/dim 1.1671(1.1574) | Steps 538(513.68) | Grad Norm 19.2944(9.8086) | Total Time 10.00(10.00)
Iter 4060 | Time 4.1567(4.4552) | Bit/dim 1.2785(1.1649) | Steps 496(512.72) | Grad Norm 14.5449(11.3001) | Total Time 10.00(10.00)
Iter 4070 | Time 4.3209(4.4613) | Bit/dim 1.2729(1.1799) | Steps 502(513.36) | Grad Norm 9.1295(11.8590) | Total Time 10.00(10.00)
Iter 4080 | Time 4.5322(4.4781) | Bit/dim 1.1562(1.1844) | Steps 520(514.68) | Grad Norm 4.1301(10.9435) | Total Time 10.00(10.00)
Iter 4090 | Time 4.1850(4.4527) | Bit/dim 1.1466(1.1796) | Steps 502(513.11) | Grad Norm 4.3571(9.9465) | Total Time 10.00(10.00)
Iter 4100 | Time 4.5591(4.4346) | Bit/dim 1.1586(1.1732) | Steps 520(511.82) | Grad Norm 2.3628(9.1950) | Total Time 10.00(10.00)
Iter 4110 | Time 4.5776(4.4604) | Bit/dim 1.1577(1.1671) | Steps 526(514.19) | Grad Norm 6.3773(8.4327) | Total Time 10.00(10.00)
Iter 4120 | Time 4.7922(4.4810) | Bit/dim 1.1559(1.1611) | Steps 538(514.96) | Grad Norm 6.6820(7.3106) | Total Time 10.00(10.00)
Iter 4130 | Time 4.3923(4.4989) | Bit/dim 1.1702(1.1608) | Steps 514(516.65) | Grad Norm 12.2870(8.5267) | Total Time 10.00(10.00)
Iter 4140 | Time 4.1490(4.4853) | Bit/dim 1.2313(1.1628) | Steps 496(515.95) | Grad Norm 15.4652(9.8534) | Total Time 10.00(10.00)
Iter 4150 | Time 4.3181(4.4719) | Bit/dim 1.1469(1.1682) | Steps 502(515.13) | Grad Norm 6.9129(10.5662) | Total Time 10.00(10.00)
Iter 4160 | Time 4.3370(4.4626) | Bit/dim 1.1022(1.1612) | Steps 502(513.62) | Grad Norm 6.5310(9.9474) | Total Time 10.00(10.00)
Iter 4170 | Time 4.5860(4.4776) | Bit/dim 1.1656(1.1610) | Steps 526(514.49) | Grad Norm 4.5932(9.7492) | Total Time 10.00(10.00)
Iter 4180 | Time 4.3657(4.4834) | Bit/dim 1.1511(1.1553) | Steps 508(513.83) | Grad Norm 2.7307(8.0346) | Total Time 10.00(10.00)
Iter 4190 | Time 4.6261(4.4933) | Bit/dim 1.1132(1.1512) | Steps 514(513.28) | Grad Norm 2.1077(6.7971) | Total Time 10.00(10.00)
validating...
Epoch 0014 | Time 60.5264, Bit/dim 1.1285
===> Using batch size 200. Total 300 iterations/epoch.
Iter 4200 | Time 4.4935(4.4983) | Bit/dim 1.0974(1.1467) | Steps 508(513.10) | Grad Norm 2.0051(5.8212) | Total Time 10.00(10.00)
Iter 4210 | Time 4.5461(4.5059) | Bit/dim 1.1104(1.1414) | Steps 520(513.57) | Grad Norm 1.2500(5.0535) | Total Time 10.00(10.00)
Iter 4220 | Time 4.6418(4.5176) | Bit/dim 1.1383(1.1415) | Steps 532(514.20) | Grad Norm 6.6498(4.8145) | Total Time 10.00(10.00)
Iter 4230 | Time 4.6343(4.5375) | Bit/dim 1.1242(1.1445) | Steps 514(515.46) | Grad Norm 5.7204(5.8196) | Total Time 10.00(10.00)
Iter 4240 | Time 4.6626(4.5359) | Bit/dim 1.2768(1.1799) | Steps 514(517.45) | Grad Norm 20.9591(8.0748) | Total Time 10.00(10.00)
Iter 4250 | Time 4.4221(4.5365) | Bit/dim 1.2080(1.1895) | Steps 496(516.55) | Grad Norm 10.5129(8.5274) | Total Time 10.00(10.00)
Iter 4260 | Time 4.1707(4.5070) | Bit/dim 1.2092(1.1914) | Steps 502(514.87) | Grad Norm 15.0002(10.9182) | Total Time 10.00(10.00)
Iter 4270 | Time 4.5195(4.4905) | Bit/dim 1.1908(1.1912) | Steps 514(515.31) | Grad Norm 10.2937(11.5628) | Total Time 10.00(10.00)
Iter 4280 | Time 4.8595(4.5059) | Bit/dim 1.2072(1.1827) | Steps 544(516.40) | Grad Norm 32.0415(11.9165) | Total Time 10.00(10.00)
Iter 4290 | Time 4.4012(4.4847) | Bit/dim 1.1509(1.1786) | Steps 514(515.25) | Grad Norm 11.4264(12.4932) | Total Time 10.00(10.00)
Iter 4300 | Time 4.6400(4.4693) | Bit/dim 1.1678(1.1758) | Steps 526(513.27) | Grad Norm 18.7930(12.4722) | Total Time 10.00(10.00)
Iter 4310 | Time 4.1469(4.4538) | Bit/dim 1.2007(1.1718) | Steps 490(511.59) | Grad Norm 11.8346(12.3365) | Total Time 10.00(10.00)
Iter 4320 | Time 4.7417(4.4483) | Bit/dim 1.1874(1.1699) | Steps 526(510.55) | Grad Norm 23.9494(12.7439) | Total Time 10.00(10.00)
Iter 4330 | Time 3.9721(4.4211) | Bit/dim 1.1564(1.1659) | Steps 484(508.42) | Grad Norm 11.8923(12.4597) | Total Time 10.00(10.00)
Iter 4340 | Time 4.9222(4.4421) | Bit/dim 1.1600(1.1651) | Steps 532(508.87) | Grad Norm 24.1965(12.7810) | Total Time 10.00(10.00)
Iter 4350 | Time 4.5005(4.4394) | Bit/dim 1.1518(1.1632) | Steps 514(508.85) | Grad Norm 11.2704(12.2397) | Total Time 10.00(10.00)
Iter 4360 | Time 4.6597(4.4532) | Bit/dim 1.1534(1.1618) | Steps 532(510.73) | Grad Norm 21.2839(11.9651) | Total Time 10.00(10.00)
Iter 4370 | Time 4.4479(4.4740) | Bit/dim 1.1864(1.1600) | Steps 502(511.18) | Grad Norm 13.7097(12.1545) | Total Time 10.00(10.00)
Iter 4380 | Time 4.4632(4.4681) | Bit/dim 1.1435(1.1623) | Steps 502(511.00) | Grad Norm 1.8434(12.1588) | Total Time 10.00(10.00)
Iter 4390 | Time 4.9980(4.4687) | Bit/dim 1.2190(1.1623) | Steps 544(510.68) | Grad Norm 29.0949(12.6905) | Total Time 10.00(10.00)
Iter 4400 | Time 4.0462(4.4286) | Bit/dim 1.1457(1.1619) | Steps 496(509.74) | Grad Norm 9.3138(12.0283) | Total Time 10.00(10.00)
Iter 4410 | Time 4.2921(4.4395) | Bit/dim 1.1678(1.1604) | Steps 496(509.48) | Grad Norm 3.6684(11.7403) | Total Time 10.00(10.00)
Iter 4420 | Time 4.3296(4.4377) | Bit/dim 1.1058(1.1559) | Steps 502(509.82) | Grad Norm 6.6113(11.4906) | Total Time 10.00(10.00)
Iter 4430 | Time 4.4755(4.4476) | Bit/dim 1.1398(1.1538) | Steps 508(510.12) | Grad Norm 7.2983(11.1602) | Total Time 10.00(10.00)
Iter 4440 | Time 4.3744(4.4529) | Bit/dim 1.1545(1.1541) | Steps 508(511.03) | Grad Norm 11.3777(11.6347) | Total Time 10.00(10.00)
Iter 4450 | Time 4.3287(4.4507) | Bit/dim 1.1507(1.1540) | Steps 502(510.83) | Grad Norm 6.3492(11.5058) | Total Time 10.00(10.00)
Iter 4460 | Time 4.3254(4.4485) | Bit/dim 1.1086(1.1510) | Steps 508(511.01) | Grad Norm 6.4276(10.1544) | Total Time 10.00(10.00)
Iter 4470 | Time 4.4963(4.4599) | Bit/dim 1.1261(1.1462) | Steps 514(511.05) | Grad Norm 3.7955(8.7140) | Total Time 10.00(10.00)
Iter 4480 | Time 4.5224(4.4746) | Bit/dim 1.1532(1.1394) | Steps 514(512.13) | Grad Norm 2.5528(7.6647) | Total Time 10.00(10.00)
Iter 4490 | Time 4.6480(4.4911) | Bit/dim 1.1099(1.1377) | Steps 538(514.45) | Grad Norm 1.2977(6.3695) | Total Time 10.00(10.00)
validating...
Epoch 0015 | Time 62.6269, Bit/dim 1.1216
===> Using batch size 200. Total 300 iterations/epoch.
Iter 4500 | Time 4.5696(4.5058) | Bit/dim 1.1462(1.1356) | Steps 526(516.49) | Grad Norm 0.7012(5.0885) | Total Time 10.00(10.00)
Iter 4510 | Time 4.6059(4.5153) | Bit/dim 1.1156(1.1342) | Steps 532(517.44) | Grad Norm 3.4850(4.5160) | Total Time 10.00(10.00)
Iter 4520 | Time 4.5042(4.5237) | Bit/dim 1.1358(1.1328) | Steps 520(519.05) | Grad Norm 5.6408(4.3598) | Total Time 10.00(10.00)
Iter 4530 | Time 4.5359(4.5412) | Bit/dim 1.1449(1.1328) | Steps 520(521.61) | Grad Norm 1.1202(4.3278) | Total Time 10.00(10.00)
Iter 4540 | Time 4.4804(4.5549) | Bit/dim 1.1309(1.1333) | Steps 514(523.30) | Grad Norm 3.9129(4.5570) | Total Time 10.00(10.00)
Iter 4550 | Time 4.6994(4.5610) | Bit/dim 1.1240(1.1318) | Steps 544(524.38) | Grad Norm 2.3705(3.9642) | Total Time 10.00(10.00)
Iter 4560 | Time 4.7903(4.5803) | Bit/dim 1.1257(1.1288) | Steps 538(525.79) | Grad Norm 10.7292(4.3885) | Total Time 10.00(10.00)
Iter 4570 | Time 4.4943(4.5874) | Bit/dim 1.1204(1.1270) | Steps 514(526.63) | Grad Norm 15.1825(5.7151) | Total Time 10.00(10.00)
Iter 4580 | Time 5.0972(4.6232) | Bit/dim 1.2279(1.1406) | Steps 562(529.46) | Grad Norm 42.2866(8.6473) | Total Time 10.00(10.00)
Iter 4590 | Time 4.6700(4.5841) | Bit/dim 1.1943(1.1664) | Steps 520(526.57) | Grad Norm 17.7002(9.3782) | Total Time 10.00(10.00)
Iter 4600 | Time 4.4781(4.5686) | Bit/dim 1.1296(1.1663) | Steps 508(524.04) | Grad Norm 3.3225(8.8223) | Total Time 10.00(10.00)
Iter 4610 | Time 4.0907(4.5492) | Bit/dim 1.2322(1.1702) | Steps 484(521.06) | Grad Norm 14.3468(10.9041) | Total Time 10.00(10.00)
Iter 4620 | Time 4.4970(4.5376) | Bit/dim 1.1682(1.1716) | Steps 526(520.91) | Grad Norm 5.6671(11.2733) | Total Time 10.00(10.00)
Iter 4630 | Time 4.4496(4.5331) | Bit/dim 1.1838(1.1717) | Steps 502(519.62) | Grad Norm 14.0986(12.2520) | Total Time 10.00(10.00)
Iter 4640 | Time 4.4612(4.5323) | Bit/dim 1.2265(1.1752) | Steps 508(519.24) | Grad Norm 13.0480(12.9065) | Total Time 10.00(10.00)
Iter 4650 | Time 4.4918(4.4988) | Bit/dim 1.1523(1.1740) | Steps 508(517.60) | Grad Norm 8.1887(12.0600) | Total Time 10.00(10.00)
Iter 4660 | Time 4.5504(4.4958) | Bit/dim 1.1331(1.1656) | Steps 520(516.49) | Grad Norm 8.1902(11.5366) | Total Time 10.00(10.00)
Iter 4670 | Time 4.5545(4.4950) | Bit/dim 1.1476(1.1581) | Steps 520(515.89) | Grad Norm 14.4496(10.5619) | Total Time 10.00(10.00)
Iter 4680 | Time 4.4906(4.4955) | Bit/dim 1.1665(1.1517) | Steps 532(517.12) | Grad Norm 14.6377(9.7737) | Total Time 10.00(10.00)
Iter 4690 | Time 4.5216(4.5132) | Bit/dim 1.1247(1.1432) | Steps 520(518.82) | Grad Norm 3.8863(8.8985) | Total Time 10.00(10.00)
Iter 4700 | Time 4.5776(4.5278) | Bit/dim 1.1235(1.1374) | Steps 526(520.64) | Grad Norm 1.7084(7.5951) | Total Time 10.00(10.00)
Iter 4710 | Time 4.4506(4.5521) | Bit/dim 1.1523(1.1403) | Steps 526(523.12) | Grad Norm 7.1210(8.8074) | Total Time 10.00(10.00)
Iter 4720 | Time 4.9533(4.5648) | Bit/dim 1.1998(1.1544) | Steps 562(525.78) | Grad Norm 7.8697(10.7089) | Total Time 10.00(10.00)
Iter 4730 | Time 4.5821(4.5426) | Bit/dim 1.1426(1.1571) | Steps 526(524.12) | Grad Norm 4.0430(10.4133) | Total Time 10.00(10.00)
Iter 4740 | Time 4.5363(4.5358) | Bit/dim 1.1379(1.1539) | Steps 520(521.63) | Grad Norm 11.2529(9.9886) | Total Time 10.00(10.00)
Iter 4750 | Time 4.6261(4.5403) | Bit/dim 1.1033(1.1472) | Steps 526(519.84) | Grad Norm 5.5265(9.0926) | Total Time 10.00(10.00)
Iter 4760 | Time 4.6779(4.5490) | Bit/dim 1.1164(1.1409) | Steps 526(519.75) | Grad Norm 6.6923(8.6489) | Total Time 10.00(10.00)
Iter 4770 | Time 4.7297(4.5809) | Bit/dim 1.1231(1.1376) | Steps 532(521.87) | Grad Norm 6.7659(9.2344) | Total Time 10.00(10.00)
Iter 4780 | Time 4.6092(4.5758) | Bit/dim 1.1571(1.1387) | Steps 532(523.25) | Grad Norm 19.5515(9.9293) | Total Time 10.00(10.00)
Iter 4790 | Time 4.6698(4.5841) | Bit/dim 1.1692(1.1382) | Steps 538(524.92) | Grad Norm 14.9392(10.0517) | Total Time 10.00(10.00)
validating...
Epoch 0016 | Time 65.8520, Bit/dim 1.1458
===> Using batch size 200. Total 300 iterations/epoch.
Iter 4800 | Time 4.6172(4.5854) | Bit/dim 1.1464(1.1394) | Steps 532(525.41) | Grad Norm 26.2117(10.7250) | Total Time 10.00(10.00)
Iter 4810 | Time 4.5849(4.5910) | Bit/dim 1.1800(1.1487) | Steps 526(526.80) | Grad Norm 12.2476(12.0282) | Total Time 10.00(10.00)
Iter 4820 | Time 4.8311(4.5846) | Bit/dim 1.2007(1.1679) | Steps 538(524.39) | Grad Norm 11.8581(12.2295) | Total Time 10.00(10.00)
Iter 4830 | Time 4.3284(4.5370) | Bit/dim 1.1311(1.1637) | Steps 502(521.06) | Grad Norm 5.6984(10.6265) | Total Time 10.00(10.00)
Iter 4840 | Time 4.6948(4.5413) | Bit/dim 1.1094(1.1556) | Steps 526(519.56) | Grad Norm 6.1654(9.6671) | Total Time 10.00(10.00)
Iter 4850 | Time 4.5778(4.5457) | Bit/dim 1.1433(1.1514) | Steps 520(518.55) | Grad Norm 2.3854(8.6886) | Total Time 10.00(10.00)
Iter 4860 | Time 4.5092(4.5456) | Bit/dim 1.1303(1.1441) | Steps 520(518.93) | Grad Norm 2.6950(7.2789) | Total Time 10.00(10.00)
Iter 4870 | Time 4.6580(4.5561) | Bit/dim 1.1073(1.1373) | Steps 538(521.48) | Grad Norm 1.2734(5.9422) | Total Time 10.00(10.00)
Iter 4880 | Time 4.6693(4.5597) | Bit/dim 1.1148(1.1315) | Steps 538(523.00) | Grad Norm 1.2004(4.8609) | Total Time 10.00(10.00)
Iter 4890 | Time 4.5132(4.5696) | Bit/dim 1.1148(1.1272) | Steps 526(525.66) | Grad Norm 0.6134(3.9596) | Total Time 10.00(10.00)
Iter 4900 | Time 4.6323(4.5879) | Bit/dim 1.1250(1.1253) | Steps 538(529.23) | Grad Norm 4.0083(3.4727) | Total Time 10.00(10.00)
Iter 4910 | Time 4.6100(4.6017) | Bit/dim 1.1065(1.1209) | Steps 532(531.37) | Grad Norm 4.9170(3.2868) | Total Time 10.00(10.00)
Iter 4920 | Time 4.6758(4.6119) | Bit/dim 1.1088(1.1204) | Steps 544(532.99) | Grad Norm 7.7144(3.5430) | Total Time 10.00(10.00)
Iter 4930 | Time 4.6358(4.6236) | Bit/dim 1.1308(1.1195) | Steps 538(534.79) | Grad Norm 3.9616(3.8179) | Total Time 10.00(10.00)
Iter 4940 | Time 4.5499(4.6423) | Bit/dim 1.1841(1.1235) | Steps 526(535.21) | Grad Norm 13.8772(5.9875) | Total Time 10.00(10.00)
Iter 4950 | Time 4.3294(4.6355) | Bit/dim 1.4259(1.1713) | Steps 532(535.45) | Grad Norm 4.9955(8.6278) | Total Time 10.00(10.00)
Iter 4960 | Time 4.5872(4.5927) | Bit/dim 1.3189(1.2138) | Steps 520(532.07) | Grad Norm 2.6587(7.3153) | Total Time 10.00(10.00)
Iter 4970 | Time 4.5154(4.5647) | Bit/dim 1.1315(1.2076) | Steps 508(527.32) | Grad Norm 3.1973(6.3704) | Total Time 10.00(10.00)
Iter 4980 | Time 4.5456(4.5540) | Bit/dim 1.1430(1.1934) | Steps 520(523.83) | Grad Norm 10.2075(6.2262) | Total Time 10.00(10.00)
Iter 4990 | Time 4.4741(4.5410) | Bit/dim 1.1449(1.1836) | Steps 508(521.03) | Grad Norm 9.6984(7.3304) | Total Time 10.00(10.00)
Iter 5000 | Time 4.6002(4.5455) | Bit/dim 1.1280(1.1713) | Steps 520(519.14) | Grad Norm 10.7953(7.6208) | Total Time 10.00(10.00)
Iter 5010 | Time 4.4843(4.5213) | Bit/dim 1.1280(1.1609) | Steps 508(516.65) | Grad Norm 7.6842(7.5293) | Total Time 10.00(10.00)
Iter 5020 | Time 4.5030(4.5467) | Bit/dim 1.1063(1.1528) | Steps 520(519.26) | Grad Norm 10.6222(8.6696) | Total Time 10.00(10.00)
Iter 5030 | Time 4.5076(4.5714) | Bit/dim 1.1525(1.1481) | Steps 520(522.03) | Grad Norm 9.7899(8.5628) | Total Time 10.00(10.00)
Iter 5040 | Time 4.4660(4.5916) | Bit/dim 1.1168(1.1455) | Steps 514(522.88) | Grad Norm 7.5666(9.4336) | Total Time 10.00(10.00)
Iter 5050 | Time 4.6629(4.5831) | Bit/dim 1.1240(1.1414) | Steps 538(523.02) | Grad Norm 23.4862(9.4898) | Total Time 10.00(10.00)
Iter 5060 | Time 4.6078(4.5722) | Bit/dim 1.2311(1.1495) | Steps 532(523.67) | Grad Norm 9.8387(11.1706) | Total Time 10.00(10.00)
Iter 5070 | Time 4.7274(4.5638) | Bit/dim 1.1647(1.1547) | Steps 526(522.46) | Grad Norm 15.8713(10.8954) | Total Time 10.00(10.00)
Iter 5080 | Time 4.5160(4.5361) | Bit/dim 1.1299(1.1526) | Steps 514(519.60) | Grad Norm 9.1235(10.7351) | Total Time 10.00(10.00)
Iter 5090 | Time 4.5008(4.5216) | Bit/dim 1.1478(1.1506) | Steps 514(517.80) | Grad Norm 11.8332(11.3463) | Total Time 10.00(10.00)
validating...
Epoch 0017 | Time 67.3719, Bit/dim 1.1629
===> Using batch size 200. Total 300 iterations/epoch.
Iter 5100 | Time 4.6406(4.5152) | Bit/dim 1.1623(1.1530) | Steps 532(518.12) | Grad Norm 10.8550(11.9284) | Total Time 10.00(10.00)
Iter 5110 | Time 4.6669(4.5160) | Bit/dim 1.1807(1.1580) | Steps 532(518.29) | Grad Norm 8.6797(12.3536) | Total Time 10.00(10.00)
Iter 5120 | Time 4.4333(4.4960) | Bit/dim 1.1335(1.1581) | Steps 514(516.55) | Grad Norm 14.5176(11.8187) | Total Time 10.00(10.00)
Iter 5130 | Time 4.1883(4.4924) | Bit/dim 1.1463(1.1557) | Steps 502(515.59) | Grad Norm 10.7270(11.6970) | Total Time 10.00(10.00)
Iter 5140 | Time 4.5092(4.4900) | Bit/dim 1.1873(1.1515) | Steps 532(515.87) | Grad Norm 14.4706(11.1175) | Total Time 10.00(10.00)
Iter 5150 | Time 4.5596(4.5040) | Bit/dim 1.1068(1.1429) | Steps 526(517.52) | Grad Norm 5.5261(10.2774) | Total Time 10.00(10.00)
Iter 5160 | Time 4.7124(4.5462) | Bit/dim 1.1490(1.1376) | Steps 526(520.04) | Grad Norm 6.7186(9.5433) | Total Time 10.00(10.00)
Iter 5170 | Time 4.6048(4.5655) | Bit/dim 1.1110(1.1328) | Steps 526(522.15) | Grad Norm 0.7663(8.0783) | Total Time 10.00(10.00)
Iter 5180 | Time 4.6826(4.5916) | Bit/dim 1.1136(1.1288) | Steps 538(523.99) | Grad Norm 22.5978(8.3065) | Total Time 10.00(10.00)
Iter 5190 | Time 4.8638(4.5872) | Bit/dim 1.1430(1.1334) | Steps 550(524.95) | Grad Norm 4.9060(9.1406) | Total Time 10.00(10.00)
Iter 5200 | Time 4.8910(4.5846) | Bit/dim 1.2368(1.1454) | Steps 550(525.13) | Grad Norm 44.1271(11.4228) | Total Time 10.00(10.00)
Iter 5210 | Time 4.5691(4.5573) | Bit/dim 1.1962(1.1613) | Steps 520(523.21) | Grad Norm 5.7196(10.8779) | Total Time 10.00(10.00)
Iter 5220 | Time 4.6627(4.5455) | Bit/dim 1.1473(1.1595) | Steps 532(522.46) | Grad Norm 25.4917(10.7020) | Total Time 10.00(10.00)
Iter 5230 | Time 4.5584(4.5303) | Bit/dim 1.1529(1.1585) | Steps 526(521.14) | Grad Norm 9.6810(11.1845) | Total Time 10.00(10.00)
Iter 5240 | Time 4.5789(4.5507) | Bit/dim 1.1170(1.1490) | Steps 532(523.29) | Grad Norm 5.7972(9.6982) | Total Time 10.00(10.00)
Iter 5250 | Time 4.5793(4.5770) | Bit/dim 1.1131(1.1412) | Steps 526(526.02) | Grad Norm 6.2449(8.0225) | Total Time 10.00(10.00)
Iter 5260 | Time 4.7554(4.6093) | Bit/dim 1.1014(1.1339) | Steps 538(529.18) | Grad Norm 4.1384(6.8755) | Total Time 10.00(10.00)
Iter 5270 | Time 4.8658(4.6322) | Bit/dim 1.0985(1.1253) | Steps 550(531.21) | Grad Norm 0.5550(6.0129) | Total Time 10.00(10.00)
Iter 5280 | Time 4.7305(4.6317) | Bit/dim 1.1396(1.1245) | Steps 550(532.85) | Grad Norm 21.3693(7.2601) | Total Time 10.00(10.00)
Iter 5290 | Time 4.7530(4.6608) | Bit/dim 1.1064(1.1221) | Steps 538(535.11) | Grad Norm 7.6645(7.4548) | Total Time 10.00(10.00)
Iter 5300 | Time 4.2342(4.6662) | Bit/dim 1.1845(1.1287) | Steps 514(536.78) | Grad Norm 15.3577(9.5850) | Total Time 10.00(10.00)
Iter 5310 | Time 4.4743(4.6323) | Bit/dim 1.1634(1.1427) | Steps 508(533.39) | Grad Norm 9.4431(10.7501) | Total Time 10.00(10.00)
Iter 5320 | Time 4.4715(4.5940) | Bit/dim 1.2327(1.1465) | Steps 508(528.30) | Grad Norm 12.3285(11.1645) | Total Time 10.00(10.00)
Iter 5330 | Time 4.2195(4.5748) | Bit/dim 1.1643(1.1527) | Steps 508(527.23) | Grad Norm 13.6452(11.8946) | Total Time 10.00(10.00)
Iter 5340 | Time 4.1900(4.5387) | Bit/dim 1.1680(1.1524) | Steps 508(525.78) | Grad Norm 11.4037(11.8110) | Total Time 10.00(10.00)
Iter 5350 | Time 4.4284(4.5223) | Bit/dim 1.1341(1.1483) | Steps 526(524.44) | Grad Norm 3.7740(10.9927) | Total Time 10.00(10.00)
Iter 5360 | Time 4.6021(4.5589) | Bit/dim 1.0963(1.1399) | Steps 532(527.33) | Grad Norm 4.5684(9.4212) | Total Time 10.00(10.00)
Iter 5370 | Time 4.7547(4.6001) | Bit/dim 1.1522(1.1339) | Steps 538(530.12) | Grad Norm 4.7772(7.9164) | Total Time 10.00(10.00)
Iter 5380 | Time 4.6374(4.6179) | Bit/dim 1.1010(1.1264) | Steps 538(532.01) | Grad Norm 1.2577(6.5661) | Total Time 10.00(10.00)
Iter 5390 | Time 4.7601(4.6210) | Bit/dim 1.0936(1.1221) | Steps 550(533.02) | Grad Norm 3.1548(5.3814) | Total Time 10.00(10.00)
validating...
Epoch 0018 | Time 60.4329, Bit/dim 1.1042
===> Using batch size 200. Total 300 iterations/epoch.
Iter 5400 | Time 4.7696(4.6340) | Bit/dim 1.1078(1.1181) | Steps 538(534.47) | Grad Norm 1.9928(4.7562) | Total Time 10.00(10.00)
Iter 5410 | Time 4.6885(4.6560) | Bit/dim 1.0737(1.1141) | Steps 544(536.55) | Grad Norm 4.1196(4.0920) | Total Time 10.00(10.00)
Iter 5420 | Time 4.7669(4.6557) | Bit/dim 1.1289(1.1104) | Steps 538(536.70) | Grad Norm 2.1448(3.4642) | Total Time 10.00(10.00)
Iter 5430 | Time 4.6956(4.6585) | Bit/dim 1.1048(1.1104) | Steps 544(537.99) | Grad Norm 3.0302(3.2431) | Total Time 10.00(10.00)
Iter 5440 | Time 4.7280(4.6675) | Bit/dim 1.1415(1.1095) | Steps 550(539.30) | Grad Norm 2.5496(3.1512) | Total Time 10.00(10.00)
Iter 5450 | Time 4.7071(4.6820) | Bit/dim 1.1146(1.1099) | Steps 544(541.16) | Grad Norm 4.7320(3.4153) | Total Time 10.00(10.00)
Iter 5460 | Time 4.6797(4.6923) | Bit/dim 1.1174(1.1103) | Steps 544(542.11) | Grad Norm 1.5877(3.4496) | Total Time 10.00(10.00)
Iter 5470 | Time 4.6953(4.6888) | Bit/dim 1.1018(1.1081) | Steps 544(542.24) | Grad Norm 5.4318(3.0841) | Total Time 10.00(10.00)
Iter 5480 | Time 5.4566(4.7053) | Bit/dim 1.1928(1.1189) | Steps 598(543.30) | Grad Norm 50.4113(6.6194) | Total Time 10.00(10.00)
Iter 5490 | Time 4.1863(4.6478) | Bit/dim 1.2507(1.1582) | Steps 502(538.26) | Grad Norm 6.2155(7.5618) | Total Time 10.00(10.00)
Iter 5500 | Time 4.4827(4.6338) | Bit/dim 1.1295(1.1611) | Steps 514(535.73) | Grad Norm 7.1028(7.3524) | Total Time 10.00(10.00)
Iter 5510 | Time 4.7464(4.6013) | Bit/dim 1.2214(1.1752) | Steps 544(532.79) | Grad Norm 7.2748(9.3482) | Total Time 10.00(10.00)
Iter 5520 | Time 4.4244(4.5713) | Bit/dim 1.1302(1.1728) | Steps 526(531.38) | Grad Norm 9.5871(9.3367) | Total Time 10.00(10.00)
Iter 5530 | Time 4.5302(4.5437) | Bit/dim 1.1543(1.1631) | Steps 526(529.98) | Grad Norm 1.5670(8.6615) | Total Time 10.00(10.00)
Iter 5540 | Time 4.2476(4.5142) | Bit/dim 1.2039(1.1597) | Steps 514(529.14) | Grad Norm 14.5096(10.3278) | Total Time 10.00(10.00)
Iter 5550 | Time 4.7743(4.5334) | Bit/dim 1.1277(1.1542) | Steps 544(531.08) | Grad Norm 2.0192(10.2249) | Total Time 10.00(10.00)
Iter 5560 | Time 4.5912(4.5268) | Bit/dim 1.1410(1.1522) | Steps 532(531.41) | Grad Norm 10.1096(11.0693) | Total Time 10.00(10.00)
Iter 5570 | Time 4.2372(4.5128) | Bit/dim 1.1268(1.1457) | Steps 514(531.06) | Grad Norm 9.2927(10.9182) | Total Time 10.00(10.00)
Iter 5580 | Time 5.0996(4.5275) | Bit/dim 1.1594(1.1439) | Steps 568(532.04) | Grad Norm 29.9809(11.5425) | Total Time 10.00(10.00)
Iter 5590 | Time 4.5979(4.5312) | Bit/dim 1.1195(1.1401) | Steps 532(532.34) | Grad Norm 9.0970(11.5186) | Total Time 10.00(10.00)
Iter 5600 | Time 4.2439(4.5167) | Bit/dim 1.1472(1.1392) | Steps 514(531.81) | Grad Norm 10.0851(11.6107) | Total Time 10.00(10.00)
Iter 5610 | Time 4.2359(4.5132) | Bit/dim 1.1557(1.1394) | Steps 514(531.80) | Grad Norm 7.3734(11.0727) | Total Time 10.00(10.00)
Iter 5620 | Time 4.5530(4.5194) | Bit/dim 1.1405(1.1346) | Steps 526(531.02) | Grad Norm 3.4608(9.8251) | Total Time 10.00(10.00)
Iter 5630 | Time 4.5973(4.5413) | Bit/dim 1.1318(1.1301) | Steps 532(531.32) | Grad Norm 3.6037(8.6870) | Total Time 10.00(10.00)
Iter 5640 | Time 4.2342(4.5633) | Bit/dim 1.0797(1.1251) | Steps 514(531.59) | Grad Norm 6.3472(7.4576) | Total Time 10.00(10.00)
Iter 5650 | Time 4.2388(4.5527) | Bit/dim 1.1042(1.1238) | Steps 514(531.18) | Grad Norm 8.4835(8.0171) | Total Time 10.00(10.00)
Iter 5660 | Time 4.6002(4.5773) | Bit/dim 1.0963(1.1201) | Steps 532(532.05) | Grad Norm 1.6317(6.6950) | Total Time 10.00(10.00)
Iter 5670 | Time 4.8147(4.6004) | Bit/dim 1.0756(1.1163) | Steps 544(532.82) | Grad Norm 5.6125(5.6353) | Total Time 10.00(10.00)
Iter 5680 | Time 4.6430(4.6025) | Bit/dim 1.1504(1.1128) | Steps 538(532.81) | Grad Norm 2.4440(4.6958) | Total Time 10.00(10.00)
Iter 5690 | Time 4.7599(4.6096) | Bit/dim 1.1152(1.1108) | Steps 538(533.08) | Grad Norm 2.6008(3.9803) | Total Time 10.00(10.00)
validating...
Epoch 0019 | Time 62.0088, Bit/dim 1.0988
===> Using batch size 200. Total 300 iterations/epoch.
Iter 5700 | Time 4.6556(4.6118) | Bit/dim 1.1335(1.1079) | Steps 538(533.41) | Grad Norm 3.2549(3.8916) | Total Time 10.00(10.00)
Iter 5710 | Time 4.8767(4.6371) | Bit/dim 1.1137(1.1108) | Steps 538(534.94) | Grad Norm 8.4889(4.3314) | Total Time 10.00(10.00)
Iter 5720 | Time 4.2420(4.6459) | Bit/dim 1.1604(1.1119) | Steps 514(535.67) | Grad Norm 18.1885(5.9396) | Total Time 10.00(10.00)
Iter 5730 | Time 4.5205(4.6465) | Bit/dim 1.3256(1.1402) | Steps 544(536.60) | Grad Norm 7.7590(8.7007) | Total Time 10.00(10.00)
Iter 5740 | Time 4.8785(4.6103) | Bit/dim 1.1196(1.1574) | Steps 532(532.72) | Grad Norm 3.3577(8.0220) | Total Time 10.00(10.00)
Iter 5750 | Time 4.8375(4.5774) | Bit/dim 1.1190(1.1532) | Steps 550(530.74) | Grad Norm 33.3758(8.8248) | Total Time 10.00(10.00)
Iter 5760 | Time 4.2345(4.5456) | Bit/dim 1.1986(1.1581) | Steps 520(530.31) | Grad Norm 12.3509(10.3785) | Total Time 10.00(10.00)
Iter 5770 | Time 4.3894(4.5486) | Bit/dim 1.1462(1.1570) | Steps 526(532.52) | Grad Norm 7.6902(10.3221) | Total Time 10.00(10.00)
Iter 5780 | Time 4.6103(4.5562) | Bit/dim 1.0958(1.1488) | Steps 538(533.40) | Grad Norm 7.4864(9.8995) | Total Time 10.00(10.00)
Iter 5790 | Time 4.8090(4.5292) | Bit/dim 1.1102(1.1412) | Steps 544(531.65) | Grad Norm 20.2421(10.2121) | Total Time 10.00(10.00)
Iter 5800 | Time 4.6873(4.5497) | Bit/dim 1.1172(1.1347) | Steps 544(532.60) | Grad Norm 7.7772(9.9630) | Total Time 10.00(10.00)
Iter 5810 | Time 4.5844(4.5554) | Bit/dim 1.1029(1.1280) | Steps 532(532.12) | Grad Norm 5.4462(9.2482) | Total Time 10.00(10.00)
Iter 5820 | Time 4.7242(4.5935) | Bit/dim 1.1226(1.1224) | Steps 532(532.58) | Grad Norm 1.9080(7.5476) | Total Time 10.00(10.00)
Iter 5830 | Time 4.6025(4.6050) | Bit/dim 1.0975(1.1172) | Steps 532(532.72) | Grad Norm 1.3346(6.0016) | Total Time 10.00(10.00)
Iter 5840 | Time 4.5527(4.6020) | Bit/dim 1.1101(1.1126) | Steps 526(532.22) | Grad Norm 1.6130(4.9380) | Total Time 10.00(10.00)
Iter 5850 | Time 4.6778(4.6191) | Bit/dim 1.0758(1.1096) | Steps 526(532.72) | Grad Norm 4.1469(4.3354) | Total Time 10.00(10.00)
Iter 5860 | Time 4.6682(4.6354) | Bit/dim 1.1066(1.1102) | Steps 526(533.82) | Grad Norm 7.0940(4.4518) | Total Time 10.00(10.00)
Iter 5870 | Time 4.6893(4.6642) | Bit/dim 1.0926(1.1051) | Steps 532(534.73) | Grad Norm 0.8888(4.2778) | Total Time 10.00(10.00)
Iter 5880 | Time 4.7069(4.6764) | Bit/dim 1.0934(1.1052) | Steps 544(535.63) | Grad Norm 0.8634(4.3436) | Total Time 10.00(10.00)
Iter 5890 | Time 4.5493(4.6585) | Bit/dim 1.0743(1.1048) | Steps 526(535.21) | Grad Norm 2.2927(4.9845) | Total Time 10.00(10.00)
Iter 5900 | Time 5.1382(4.6544) | Bit/dim 1.2242(1.1327) | Steps 574(536.50) | Grad Norm 48.8972(8.4633) | Total Time 10.00(10.00)
Iter 5910 | Time 4.6797(4.6401) | Bit/dim 1.1755(1.1643) | Steps 526(536.49) | Grad Norm 5.6836(8.1416) | Total Time 10.00(10.00)
Iter 5920 | Time 4.8948(4.6440) | Bit/dim 1.1397(1.1639) | Steps 562(538.06) | Grad Norm 3.7100(7.0583) | Total Time 10.00(10.00)
Iter 5930 | Time 4.6342(4.6181) | Bit/dim 1.1206(1.1525) | Steps 538(537.17) | Grad Norm 4.2642(6.1777) | Total Time 10.00(10.00)
Iter 5940 | Time 4.5560(4.6109) | Bit/dim 1.1068(1.1406) | Steps 526(535.01) | Grad Norm 1.5001(5.1018) | Total Time 10.00(10.00)
Iter 5950 | Time 4.6130(4.6278) | Bit/dim 1.0873(1.1306) | Steps 532(535.16) | Grad Norm 6.0794(4.8359) | Total Time 10.00(10.00)
Iter 5960 | Time 4.6577(4.6586) | Bit/dim 1.1128(1.1251) | Steps 538(536.38) | Grad Norm 9.6114(4.8866) | Total Time 10.00(10.00)
Iter 5970 | Time 4.8207(4.6755) | Bit/dim 1.1251(1.1205) | Steps 544(537.38) | Grad Norm 11.4708(5.3316) | Total Time 10.00(10.00)
Iter 5980 | Time 4.5335(4.6576) | Bit/dim 1.3397(1.1457) | Steps 544(537.60) | Grad Norm 6.9805(7.6310) | Total Time 10.00(10.00)
Iter 5990 | Time 4.5062(4.6382) | Bit/dim 1.1289(1.1631) | Steps 538(538.23) | Grad Norm 3.1224(7.4121) | Total Time 10.00(10.00)
validating...
Epoch 0020 | Time 62.3036, Bit/dim 1.1423
===> Using batch size 200. Total 300 iterations/epoch.
Iter 6000 | Time 5.0476(4.6232) | Bit/dim 1.1505(1.1578) | Steps 562(539.19) | Grad Norm 29.6523(8.1193) | Total Time 10.00(10.00)
Iter 6010 | Time 4.7603(4.6167) | Bit/dim 1.2268(1.1850) | Steps 556(540.87) | Grad Norm 6.5567(9.9267) | Total Time 10.00(10.00)
Iter 6020 | Time 4.5175(4.6105) | Bit/dim 1.1726(1.1842) | Steps 538(541.07) | Grad Norm 2.6206(8.5361) | Total Time 10.00(10.00)
Iter 6030 | Time 4.4502(4.5793) | Bit/dim 1.1096(1.1690) | Steps 526(538.94) | Grad Norm 1.3192(6.9367) | Total Time 10.00(10.00)
Iter 6040 | Time 4.4486(4.5537) | Bit/dim 1.1406(1.1564) | Steps 526(536.50) | Grad Norm 6.2102(6.2625) | Total Time 10.00(10.00)
Iter 6050 | Time 4.4391(4.5453) | Bit/dim 1.2435(1.1521) | Steps 532(535.56) | Grad Norm 12.6878(8.0094) | Total Time 10.00(10.00)
Iter 6060 | Time 4.4466(4.5448) | Bit/dim 1.1135(1.1516) | Steps 526(535.62) | Grad Norm 6.9179(8.7559) | Total Time 10.00(10.00)
Iter 6070 | Time 4.4539(4.5423) | Bit/dim 1.1597(1.1472) | Steps 526(535.37) | Grad Norm 12.7333(9.6260) | Total Time 10.00(10.00)
Iter 6080 | Time 4.3417(4.5379) | Bit/dim 1.1382(1.1500) | Steps 532(536.09) | Grad Norm 10.6265(10.4320) | Total Time 10.00(10.00)
Iter 6090 | Time 4.6218(4.5355) | Bit/dim 1.1195(1.1451) | Steps 538(535.66) | Grad Norm 2.4714(9.3551) | Total Time 10.00(10.00)
Iter 6100 | Time 4.2991(4.5262) | Bit/dim 1.1179(1.1360) | Steps 520(534.58) | Grad Norm 4.5790(8.0755) | Total Time 10.00(10.00)
Iter 6110 | Time 4.5982(4.5345) | Bit/dim 1.1332(1.1277) | Steps 532(534.02) | Grad Norm 0.8947(6.5901) | Total Time 10.00(10.00)
Iter 6120 | Time 4.7548(4.5510) | Bit/dim 1.1147(1.1179) | Steps 538(533.67) | Grad Norm 1.1153(5.5363) | Total Time 10.00(10.00)
Iter 6130 | Time 4.9038(4.5580) | Bit/dim 1.1424(1.1161) | Steps 562(534.61) | Grad Norm 4.7103(6.4877) | Total Time 10.00(10.00)
Iter 6140 | Time 4.6107(4.5718) | Bit/dim 1.1722(1.1394) | Steps 532(537.41) | Grad Norm 11.1749(8.8665) | Total Time 10.00(10.00)
Iter 6150 | Time 4.7778(4.5857) | Bit/dim 1.1251(1.1399) | Steps 544(538.66) | Grad Norm 10.3857(9.3009) | Total Time 10.00(10.00)
Iter 6160 | Time 4.4680(4.5913) | Bit/dim 1.0942(1.1324) | Steps 532(538.16) | Grad Norm 2.6153(7.8367) | Total Time 10.00(10.00)
Iter 6170 | Time 4.7109(4.5642) | Bit/dim 1.1283(1.1261) | Steps 550(536.73) | Grad Norm 12.4980(8.3679) | Total Time 10.00(10.00)
Iter 6180 | Time 4.6281(4.5386) | Bit/dim 1.1275(1.1197) | Steps 538(535.17) | Grad Norm 10.9585(7.6020) | Total Time 10.00(10.00)
Iter 6190 | Time 4.9358(4.5234) | Bit/dim 1.1149(1.1173) | Steps 562(534.32) | Grad Norm 22.7633(8.7690) | Total Time 10.00(10.00)
Iter 6200 | Time 4.8209(4.5566) | Bit/dim 1.2291(1.1405) | Steps 562(538.17) | Grad Norm 9.0429(10.0505) | Total Time 10.00(10.00)
Iter 6210 | Time 4.6179(4.5850) | Bit/dim 1.1609(1.1446) | Steps 550(538.96) | Grad Norm 18.0865(9.9809) | Total Time 10.00(10.00)
Iter 6220 | Time 4.2748(4.5352) | Bit/dim 1.1001(1.1419) | Steps 520(535.81) | Grad Norm 7.9927(9.6938) | Total Time 10.00(10.00)
Iter 6230 | Time 4.3075(4.5228) | Bit/dim 1.1087(1.1338) | Steps 526(535.50) | Grad Norm 5.6897(9.0898) | Total Time 10.00(10.00)
Iter 6240 | Time 4.3073(4.5310) | Bit/dim 1.1184(1.1252) | Steps 526(535.54) | Grad Norm 5.0745(8.0545) | Total Time 10.00(10.00)
Iter 6250 | Time 4.2784(4.5174) | Bit/dim 1.0876(1.1197) | Steps 526(534.94) | Grad Norm 8.3857(8.1496) | Total Time 10.00(10.00)
Iter 6260 | Time 4.9588(4.5429) | Bit/dim 1.0799(1.1156) | Steps 556(536.85) | Grad Norm 2.2075(7.9998) | Total Time 10.00(10.00)
Iter 6270 | Time 4.2876(4.5357) | Bit/dim 1.1185(1.1143) | Steps 526(537.11) | Grad Norm 12.1294(9.2865) | Total Time 10.00(10.00)
Iter 6280 | Time 4.2929(4.5562) | Bit/dim 1.1074(1.1135) | Steps 526(539.14) | Grad Norm 9.9532(9.3625) | Total Time 10.00(10.00)
Iter 6290 | Time 4.2932(4.5626) | Bit/dim 1.1420(1.1133) | Steps 520(538.89) | Grad Norm 11.9446(9.8836) | Total Time 10.00(10.00)
validating...
Epoch 0021 | Time 59.2249, Bit/dim 1.1276
===> Using batch size 200. Total 300 iterations/epoch.
Iter 6300 | Time 4.3007(4.5554) | Bit/dim 1.1225(1.1233) | Steps 526(538.62) | Grad Norm 10.0485(11.0288) | Total Time 10.00(10.00)
Iter 6310 | Time 4.6657(4.5423) | Bit/dim 1.0920(1.1235) | Steps 544(538.35) | Grad Norm 8.4404(11.0317) | Total Time 10.00(10.00)
Iter 6320 | Time 4.4601(4.5405) | Bit/dim 1.1133(1.1213) | Steps 532(537.52) | Grad Norm 7.9101(11.0685) | Total Time 10.00(10.00)
Iter 6330 | Time 4.3358(4.5317) | Bit/dim 1.1230(1.1211) | Steps 526(536.34) | Grad Norm 8.1760(10.8812) | Total Time 10.00(10.00)
Iter 6340 | Time 4.6580(4.5333) | Bit/dim 1.0771(1.1154) | Steps 544(536.59) | Grad Norm 9.4155(9.8690) | Total Time 10.00(10.00)
Iter 6350 | Time 4.6143(4.5269) | Bit/dim 1.1294(1.1116) | Steps 538(535.95) | Grad Norm 12.4179(9.0461) | Total Time 10.00(10.00)
Iter 6360 | Time 4.6151(4.5208) | Bit/dim 1.1034(1.1092) | Steps 538(535.44) | Grad Norm 5.7385(8.8564) | Total Time 10.00(10.00)
Iter 6370 | Time 4.2871(4.5081) | Bit/dim 1.1114(1.1056) | Steps 520(534.02) | Grad Norm 9.8377(9.0954) | Total Time 10.00(10.00)
Iter 6380 | Time 4.7891(4.5303) | Bit/dim 1.0874(1.1021) | Steps 544(535.27) | Grad Norm 4.6355(9.0228) | Total Time 10.00(10.00)
Iter 6390 | Time 4.9041(4.5454) | Bit/dim 1.1240(1.1014) | Steps 556(536.45) | Grad Norm 24.9428(9.7114) | Total Time 10.00(10.00)
Iter 6400 | Time 4.4322(4.5488) | Bit/dim 1.1868(1.1139) | Steps 526(536.59) | Grad Norm 12.2526(10.9640) | Total Time 10.00(10.00)
Iter 6410 | Time 4.4328(4.5734) | Bit/dim 1.1201(1.1141) | Steps 526(537.39) | Grad Norm 7.6171(10.8337) | Total Time 10.00(10.00)
Iter 6420 | Time 5.0600(4.6543) | Bit/dim 1.1124(1.1154) | Steps 556(538.83) | Grad Norm 27.7400(11.0553) | Total Time 10.00(10.00)
Iter 6430 | Time 4.9127(4.6930) | Bit/dim 1.1340(1.1183) | Steps 544(540.32) | Grad Norm 7.2859(10.6764) | Total Time 10.00(10.00)
Iter 6440 | Time 4.9727(4.7118) | Bit/dim 1.0927(1.1193) | Steps 544(539.66) | Grad Norm 2.0069(10.2640) | Total Time 10.00(10.00)
Iter 6450 | Time 4.9780(4.7329) | Bit/dim 1.1385(1.1216) | Steps 550(539.82) | Grad Norm 9.6313(10.5614) | Total Time 10.00(10.00)
Iter 6460 | Time 4.4418(4.7123) | Bit/dim 1.1059(1.1170) | Steps 520(537.43) | Grad Norm 9.8061(10.0663) | Total Time 10.00(10.00)
Iter 6470 | Time 4.8336(4.7236) | Bit/dim 1.1387(1.1199) | Steps 550(537.53) | Grad Norm 19.4610(10.8682) | Total Time 10.00(10.00)
Iter 6480 | Time 4.5017(4.7292) | Bit/dim 1.0862(1.1169) | Steps 526(537.48) | Grad Norm 6.8692(10.2153) | Total Time 10.00(10.00)
Iter 6490 | Time 4.8177(4.7224) | Bit/dim 1.1144(1.1142) | Steps 538(536.97) | Grad Norm 4.5355(10.2237) | Total Time 10.00(10.00)
Iter 6500 | Time 4.6821(4.7224) | Bit/dim 1.0977(1.1120) | Steps 532(537.38) | Grad Norm 3.0244(10.2104) | Total Time 10.00(10.00)
Iter 6510 | Time 4.9078(4.7315) | Bit/dim 1.0917(1.1111) | Steps 544(536.65) | Grad Norm 6.4032(9.4580) | Total Time 10.00(10.00)
Iter 6520 | Time 4.8412(4.7254) | Bit/dim 1.0845(1.1076) | Steps 538(536.37) | Grad Norm 2.2260(8.8302) | Total Time 10.00(10.00)
Iter 6530 | Time 4.8323(4.7462) | Bit/dim 1.0774(1.1043) | Steps 538(536.30) | Grad Norm 8.8656(8.2855) | Total Time 10.00(10.00)
Iter 6540 | Time 4.5276(4.7820) | Bit/dim 1.1266(1.1037) | Steps 526(537.35) | Grad Norm 8.2802(7.7642) | Total Time 10.00(10.00)
Iter 6550 | Time 4.4091(4.7878) | Bit/dim 1.1204(1.1091) | Steps 514(537.33) | Grad Norm 11.5735(9.7194) | Total Time 10.00(10.00)
Iter 6560 | Time 4.4433(4.7708) | Bit/dim 1.1201(1.1195) | Steps 508(536.51) | Grad Norm 9.3415(10.4615) | Total Time 10.00(10.00)
Iter 6570 | Time 5.1567(4.7721) | Bit/dim 1.0933(1.1163) | Steps 544(535.40) | Grad Norm 2.6160(9.6730) | Total Time 10.00(10.00)
Iter 6580 | Time 4.5327(4.7508) | Bit/dim 1.1243(1.1147) | Steps 526(534.37) | Grad Norm 5.2526(8.9437) | Total Time 10.00(10.00)
Iter 6590 | Time 5.0168(4.7980) | Bit/dim 1.0524(1.1087) | Steps 544(536.01) | Grad Norm 2.2671(7.6582) | Total Time 10.00(10.00)
validating...
Epoch 0022 | Time 60.3275, Bit/dim 1.0864
===> Using batch size 200. Total 300 iterations/epoch.
Iter 6600 | Time 4.9271(4.8168) | Bit/dim 1.0982(1.1064) | Steps 544(536.88) | Grad Norm 3.4308(6.1544) | Total Time 10.00(10.00)
Iter 6610 | Time 5.0756(4.8461) | Bit/dim 1.1139(1.1007) | Steps 550(537.88) | Grad Norm 3.3049(5.0848) | Total Time 10.00(10.00)
Iter 6620 | Time 4.8588(4.8740) | Bit/dim 1.0796(1.0991) | Steps 538(538.90) | Grad Norm 2.8849(4.2787) | Total Time 10.00(10.00)
Iter 6630 | Time 5.0261(4.8895) | Bit/dim 1.0958(1.0957) | Steps 544(539.45) | Grad Norm 2.8405(3.7918) | Total Time 10.00(10.00)
Iter 6640 | Time 4.8939(4.8888) | Bit/dim 1.0800(1.0927) | Steps 544(540.01) | Grad Norm 2.7063(3.9658) | Total Time 10.00(10.00)
Iter 6650 | Time 4.4004(4.8211) | Bit/dim 1.1023(1.0905) | Steps 538(539.65) | Grad Norm 10.0359(4.9038) | Total Time 10.00(10.00)
Iter 6660 | Time 4.8053(4.8090) | Bit/dim 1.0966(1.0926) | Steps 544(540.49) | Grad Norm 7.0836(5.0690) | Total Time 10.00(10.00)
Iter 6670 | Time 4.6175(4.7771) | Bit/dim 1.3405(1.1116) | Steps 544(541.80) | Grad Norm 11.2820(8.1579) | Total Time 10.00(10.00)
Iter 6680 | Time 4.7946(4.7510) | Bit/dim 1.2530(1.1494) | Steps 544(542.62) | Grad Norm 9.0098(8.1802) | Total Time 10.00(10.00)
Iter 6690 | Time 4.6301(4.7463) | Bit/dim 1.1520(1.1486) | Steps 532(543.39) | Grad Norm 6.0563(7.5707) | Total Time 10.00(10.00)
Iter 6700 | Time 4.4706(4.6937) | Bit/dim 1.0928(1.1417) | Steps 532(541.09) | Grad Norm 1.7850(6.3456) | Total Time 10.00(10.00)
Iter 6710 | Time 4.6503(4.6950) | Bit/dim 1.1141(1.1332) | Steps 544(542.63) | Grad Norm 1.1838(5.2831) | Total Time 10.00(10.00)
Iter 6720 | Time 4.5030(4.6763) | Bit/dim 1.1033(1.1208) | Steps 538(542.78) | Grad Norm 4.3981(4.5234) | Total Time 10.00(10.00)
Iter 6730 | Time 4.6624(4.6620) | Bit/dim 1.0593(1.1099) | Steps 544(542.75) | Grad Norm 3.7413(4.6770) | Total Time 10.00(10.00)
Iter 6740 | Time 4.7704(4.6733) | Bit/dim 1.1758(1.1197) | Steps 574(544.90) | Grad Norm 8.5641(6.5044) | Total Time 10.00(10.00)
Iter 6750 | Time 5.0025(4.6847) | Bit/dim 1.2518(1.1426) | Steps 562(548.16) | Grad Norm 8.0961(8.8830) | Total Time 10.00(10.00)
Iter 6760 | Time 4.8438(4.7080) | Bit/dim 1.1146(1.1510) | Steps 556(548.75) | Grad Norm 5.3639(8.8810) | Total Time 10.00(10.00)
Iter 6770 | Time 4.4786(4.6972) | Bit/dim 1.0877(1.1426) | Steps 532(546.91) | Grad Norm 8.2151(7.9979) | Total Time 10.00(10.00)
Iter 6780 | Time 4.6489(4.6790) | Bit/dim 1.0971(1.1327) | Steps 544(545.85) | Grad Norm 1.5516(6.6002) | Total Time 10.00(10.00)
Iter 6790 | Time 4.6236(4.6721) | Bit/dim 1.0801(1.1213) | Steps 538(544.36) | Grad Norm 2.4754(5.2276) | Total Time 10.00(10.00)
Iter 6800 | Time 4.6656(4.6472) | Bit/dim 1.0761(1.1126) | Steps 544(543.37) | Grad Norm 1.0645(4.3620) | Total Time 10.00(10.00)
Iter 6810 | Time 4.5235(4.6246) | Bit/dim 1.0821(1.1068) | Steps 544(542.63) | Grad Norm 4.2250(3.9468) | Total Time 10.00(10.00)
Iter 6820 | Time 4.6417(4.6325) | Bit/dim 1.0943(1.0995) | Steps 538(541.61) | Grad Norm 2.6591(3.6764) | Total Time 10.00(10.00)
Iter 6830 | Time 4.7331(4.6491) | Bit/dim 1.1034(1.0973) | Steps 556(542.73) | Grad Norm 5.2460(4.0177) | Total Time 10.00(10.00)
Iter 6840 | Time 4.6753(4.6563) | Bit/dim 1.0974(1.0954) | Steps 550(542.96) | Grad Norm 8.4319(4.1057) | Total Time 10.00(10.00)
Iter 6850 | Time 4.9532(4.6767) | Bit/dim 1.3389(1.1146) | Steps 580(545.98) | Grad Norm 8.8439(6.4507) | Total Time 10.00(10.00)
Iter 6860 | Time 4.8486(4.7218) | Bit/dim 1.1183(1.1365) | Steps 556(550.29) | Grad Norm 6.9509(7.4925) | Total Time 10.00(10.00)
Iter 6870 | Time 4.6360(4.7115) | Bit/dim 1.1992(1.1406) | Steps 550(549.77) | Grad Norm 12.9002(8.6912) | Total Time 10.00(10.00)
Iter 6880 | Time 4.8711(4.6948) | Bit/dim 1.1560(1.1511) | Steps 550(549.20) | Grad Norm 4.2960(9.6294) | Total Time 10.00(10.00)
Iter 6890 | Time 4.5321(4.7044) | Bit/dim 1.1460(1.1446) | Steps 538(549.86) | Grad Norm 5.1042(8.6055) | Total Time 10.00(10.00)
validating...
Epoch 0023 | Time 59.1473, Bit/dim 1.0940
===> Using batch size 200. Total 300 iterations/epoch.
Iter 6900 | Time 4.5247(4.6788) | Bit/dim 1.1212(1.1341) | Steps 544(547.70) | Grad Norm 5.2581(7.5464) | Total Time 10.00(10.00)
Iter 6910 | Time 4.4949(4.6523) | Bit/dim 1.0754(1.1268) | Steps 538(545.24) | Grad Norm 7.2022(7.5561) | Total Time 10.00(10.00)
Iter 6920 | Time 4.5511(4.6330) | Bit/dim 1.1196(1.1183) | Steps 538(544.46) | Grad Norm 8.9157(8.0875) | Total Time 10.00(10.00)
Iter 6930 | Time 4.4554(4.6216) | Bit/dim 1.1290(1.1136) | Steps 532(544.11) | Grad Norm 11.6980(8.8962) | Total Time 10.00(10.00)
Iter 6940 | Time 4.7047(4.6316) | Bit/dim 1.1279(1.1248) | Steps 556(546.11) | Grad Norm 11.1348(9.9895) | Total Time 10.00(10.00)
Iter 6950 | Time 4.4631(4.6445) | Bit/dim 1.1000(1.1298) | Steps 532(547.92) | Grad Norm 8.5295(10.2305) | Total Time 10.00(10.00)
Iter 6960 | Time 4.8638(4.6519) | Bit/dim 1.0931(1.1269) | Steps 562(548.89) | Grad Norm 2.0849(9.1587) | Total Time 10.00(10.00)
Iter 6970 | Time 4.7360(4.6604) | Bit/dim 1.0999(1.1198) | Steps 562(549.41) | Grad Norm 3.8487(7.8999) | Total Time 10.00(10.00)
Iter 6980 | Time 4.6519(4.6564) | Bit/dim 1.1043(1.1115) | Steps 544(548.42) | Grad Norm 1.1836(6.3295) | Total Time 10.00(10.00)
Iter 6990 | Time 4.7201(4.6516) | Bit/dim 1.1028(1.1054) | Steps 556(548.86) | Grad Norm 1.8584(5.1182) | Total Time 10.00(10.00)
Iter 7000 | Time 4.6961(4.6617) | Bit/dim 1.0992(1.1032) | Steps 556(549.94) | Grad Norm 1.7095(4.2078) | Total Time 10.00(10.00)
Iter 7010 | Time 4.7097(4.6682) | Bit/dim 1.0830(1.1003) | Steps 556(551.59) | Grad Norm 0.6765(3.9051) | Total Time 10.00(10.00)
Iter 7020 | Time 4.7820(4.6660) | Bit/dim 1.0880(1.0955) | Steps 550(552.24) | Grad Norm 6.7843(3.9001) | Total Time 10.00(10.00)
Iter 7030 | Time 4.8703(4.6738) | Bit/dim 1.1091(1.0941) | Steps 562(552.25) | Grad Norm 6.7421(3.8734) | Total Time 10.00(10.00)
Iter 7040 | Time 4.7058(4.6861) | Bit/dim 1.3183(1.1064) | Steps 556(552.96) | Grad Norm 11.6906(6.9353) | Total Time 10.00(10.00)
Iter 7050 | Time 4.8062(4.7313) | Bit/dim 1.1705(1.1368) | Steps 550(554.83) | Grad Norm 5.3923(6.9604) | Total Time 10.00(10.00)
Iter 7060 | Time 4.8251(4.7500) | Bit/dim 1.1040(1.1368) | Steps 550(554.23) | Grad Norm 10.3787(6.9357) | Total Time 10.00(10.00)
Iter 7070 | Time 4.3330(4.7127) | Bit/dim 1.1860(1.1458) | Steps 532(552.91) | Grad Norm 12.7827(8.8974) | Total Time 10.00(10.00)
Iter 7080 | Time 4.6792(4.7181) | Bit/dim 1.1306(1.1494) | Steps 544(554.01) | Grad Norm 3.3366(9.0993) | Total Time 10.00(10.00)
Iter 7090 | Time 4.5262(4.7100) | Bit/dim 1.0933(1.1390) | Steps 544(553.87) | Grad Norm 6.1054(8.1947) | Total Time 10.00(10.00)
Iter 7100 | Time 4.3357(4.6778) | Bit/dim 1.0965(1.1289) | Steps 532(551.65) | Grad Norm 7.9476(7.8293) | Total Time 10.00(10.00)
Iter 7110 | Time 4.8412(4.6692) | Bit/dim 1.0761(1.1183) | Steps 556(550.01) | Grad Norm 4.7464(7.5274) | Total Time 10.00(10.00)
Iter 7120 | Time 4.6366(4.6665) | Bit/dim 1.1216(1.1134) | Steps 544(549.38) | Grad Norm 10.2006(8.0192) | Total Time 10.00(10.00)
Iter 7130 | Time 4.3860(4.6484) | Bit/dim 1.0875(1.1087) | Steps 544(548.92) | Grad Norm 8.1503(8.2823) | Total Time 10.00(10.00)
Iter 7140 | Time 4.7071(4.6777) | Bit/dim 1.1159(1.1028) | Steps 556(550.48) | Grad Norm 0.8902(6.8618) | Total Time 10.00(10.00)
Iter 7150 | Time 4.7206(4.6871) | Bit/dim 1.0291(1.0967) | Steps 562(552.30) | Grad Norm 1.6837(5.6868) | Total Time 10.00(10.00)
Iter 7160 | Time 4.5608(4.6951) | Bit/dim 1.0818(1.0916) | Steps 556(554.28) | Grad Norm 5.0707(4.8442) | Total Time 10.00(10.00)
Iter 7170 | Time 4.3788(4.7065) | Bit/dim 1.0694(1.0891) | Steps 544(554.82) | Grad Norm 6.9564(4.6637) | Total Time 10.00(10.00)
Iter 7180 | Time 4.8360(4.7016) | Bit/dim 1.1269(1.0875) | Steps 562(555.37) | Grad Norm 13.1139(5.4882) | Total Time 10.00(10.00)
Iter 7190 | Time 4.5600(4.6988) | Bit/dim 1.1147(1.1027) | Steps 550(556.16) | Grad Norm 10.1649(7.9319) | Total Time 10.00(10.00)
validating...
Epoch 0024 | Time 61.4117, Bit/dim 1.1901
===> Using batch size 200. Total 300 iterations/epoch.
Iter 7200 | Time 4.6479(4.7233) | Bit/dim 1.1662(1.1410) | Steps 538(557.04) | Grad Norm 3.6458(8.7293) | Total Time 10.00(10.00)
Iter 7210 | Time 5.0046(4.7758) | Bit/dim 1.1230(1.1469) | Steps 562(557.37) | Grad Norm 2.5162(7.1851) | Total Time 10.00(10.00)
Iter 7220 | Time 4.7055(4.7580) | Bit/dim 1.1160(1.1369) | Steps 556(555.14) | Grad Norm 9.8733(6.4159) | Total Time 10.00(10.00)
Iter 7230 | Time 4.7156(4.7483) | Bit/dim 1.2251(1.1433) | Steps 568(556.05) | Grad Norm 9.6559(8.6004) | Total Time 10.00(10.00)
Iter 7240 | Time 4.5873(4.7287) | Bit/dim 1.1205(1.1438) | Steps 562(556.06) | Grad Norm 5.7298(8.3717) | Total Time 10.00(10.00)
Iter 7250 | Time 4.4811(4.7167) | Bit/dim 1.0743(1.1347) | Steps 538(554.72) | Grad Norm 8.5114(7.5426) | Total Time 10.00(10.00)
Iter 7260 | Time 4.4654(4.6884) | Bit/dim 1.0829(1.1248) | Steps 538(551.25) | Grad Norm 5.1024(7.1480) | Total Time 10.00(10.00)
Iter 7270 | Time 4.7053(4.6806) | Bit/dim 1.0746(1.1167) | Steps 556(550.04) | Grad Norm 9.9083(7.7258) | Total Time 10.00(10.00)
Iter 7280 | Time 4.9604(4.6936) | Bit/dim 1.1557(1.1147) | Steps 574(551.04) | Grad Norm 27.0477(9.1105) | Total Time 10.00(10.00)
Iter 7290 | Time 4.8686(4.7160) | Bit/dim 1.1243(1.1190) | Steps 562(553.44) | Grad Norm 9.1472(9.9568) | Total Time 10.00(10.00)
Iter 7300 | Time 4.4785(4.7095) | Bit/dim 1.1001(1.1213) | Steps 538(553.12) | Grad Norm 6.6699(9.6731) | Total Time 10.00(10.00)
Iter 7310 | Time 4.5505(4.6999) | Bit/dim 1.0686(1.1137) | Steps 550(551.40) | Grad Norm 6.6280(9.0064) | Total Time 10.00(10.00)
Iter 7320 | Time 4.6312(4.6807) | Bit/dim 1.0847(1.1078) | Steps 538(549.10) | Grad Norm 3.8511(8.2222) | Total Time 10.00(10.00)
Iter 7330 | Time 4.6850(4.6763) | Bit/dim 1.0818(1.1034) | Steps 556(549.54) | Grad Norm 6.9808(7.1590) | Total Time 10.00(10.00)
Iter 7340 | Time 4.7089(4.6978) | Bit/dim 1.0972(1.1019) | Steps 556(551.47) | Grad Norm 13.0934(8.2661) | Total Time 10.00(10.00)
Iter 7350 | Time 4.9833(4.7346) | Bit/dim 1.1430(1.1275) | Steps 556(555.28) | Grad Norm 4.2601(9.1833) | Total Time 10.00(10.00)
Iter 7360 | Time 4.6810(4.7464) | Bit/dim 1.1163(1.1310) | Steps 550(554.11) | Grad Norm 1.7981(7.7654) | Total Time 10.00(10.00)
Iter 7370 | Time 4.7840(4.7367) | Bit/dim 1.1224(1.1241) | Steps 544(551.28) | Grad Norm 4.3775(6.5634) | Total Time 10.00(10.00)
Iter 7380 | Time 4.7137(4.7289) | Bit/dim 1.0721(1.1157) | Steps 556(549.78) | Grad Norm 1.5897(5.8641) | Total Time 10.00(10.00)
Iter 7390 | Time 4.6420(4.7196) | Bit/dim 1.1198(1.1150) | Steps 550(551.75) | Grad Norm 12.4260(7.8391) | Total Time 10.00(10.00)
Iter 7400 | Time 4.8136(4.7351) | Bit/dim 1.1385(1.1317) | Steps 550(554.19) | Grad Norm 10.5037(8.6456) | Total Time 10.00(10.00)
Iter 7410 | Time 5.2037(4.7886) | Bit/dim 1.1032(1.1293) | Steps 574(556.51) | Grad Norm 1.7152(7.3214) | Total Time 10.00(10.00)
Iter 7420 | Time 4.4987(4.7729) | Bit/dim 1.1044(1.1218) | Steps 538(554.76) | Grad Norm 4.9765(6.1396) | Total Time 10.00(10.00)
Iter 7430 | Time 4.8427(4.7650) | Bit/dim 1.1102(1.1123) | Steps 562(555.27) | Grad Norm 5.8622(5.6950) | Total Time 10.00(10.00)
Iter 7440 | Time 4.7110(4.7792) | Bit/dim 1.1686(1.1088) | Steps 556(556.76) | Grad Norm 13.5288(7.2142) | Total Time 10.00(10.00)
Iter 7450 | Time 4.6999(4.7829) | Bit/dim 1.1095(1.1180) | Steps 556(559.64) | Grad Norm 9.2998(8.9844) | Total Time 10.00(10.00)
Iter 7460 | Time 4.7796(4.7848) | Bit/dim 1.0817(1.1151) | Steps 550(559.98) | Grad Norm 6.9302(8.8073) | Total Time 10.00(10.00)
Iter 7470 | Time 4.7524(4.8150) | Bit/dim 1.0975(1.1103) | Steps 556(559.55) | Grad Norm 8.4377(7.8368) | Total Time 10.00(10.00)
Iter 7480 | Time 4.7545(4.8416) | Bit/dim 1.1091(1.1058) | Steps 550(558.58) | Grad Norm 7.8877(6.9656) | Total Time 10.00(10.00)
Iter 7490 | Time 4.8589(4.8599) | Bit/dim 1.1182(1.0999) | Steps 550(557.74) | Grad Norm 7.7244(6.9066) | Total Time 10.00(10.00)
validating...
Epoch 0025 | Time 60.2068, Bit/dim 1.0775
===> Using batch size 200. Total 300 iterations/epoch.
Iter 7500 | Time 4.9529(4.8783) | Bit/dim 1.0767(1.0961) | Steps 556(557.57) | Grad Norm 4.8653(7.1789) | Total Time 10.00(10.00)
Iter 7510 | Time 5.1218(4.9132) | Bit/dim 1.1071(1.0942) | Steps 568(558.39) | Grad Norm 2.8810(6.4526) | Total Time 10.00(10.00)
Iter 7520 | Time 5.1748(4.9600) | Bit/dim 1.1722(1.1129) | Steps 568(561.11) | Grad Norm 6.9320(8.5051) | Total Time 10.00(10.00)
Iter 7530 | Time 5.5546(5.0174) | Bit/dim 1.1173(1.1212) | Steps 592(564.88) | Grad Norm 2.1170(8.3335) | Total Time 10.00(10.00)
Iter 7540 | Time 5.0315(5.0163) | Bit/dim 1.1016(1.1195) | Steps 562(564.41) | Grad Norm 5.7690(8.2472) | Total Time 10.00(10.00)
Iter 7550 | Time 5.1289(5.0097) | Bit/dim 1.0819(1.1149) | Steps 556(560.82) | Grad Norm 14.0919(8.0407) | Total Time 10.00(10.00)
Iter 7560 | Time 4.7834(5.0017) | Bit/dim 1.0787(1.1074) | Steps 550(558.48) | Grad Norm 10.0603(7.3083) | Total Time 10.00(10.00)
Iter 7570 | Time 5.0986(5.0031) | Bit/dim 1.1127(1.1025) | Steps 562(557.38) | Grad Norm 6.6084(7.6840) | Total Time 10.00(10.00)
Iter 7580 | Time 4.9896(5.0013) | Bit/dim 1.0802(1.0985) | Steps 562(558.12) | Grad Norm 6.5212(8.1337) | Total Time 10.00(10.00)
Iter 7590 | Time 4.9469(5.0028) | Bit/dim 1.0860(1.0987) | Steps 556(558.47) | Grad Norm 9.0232(8.7478) | Total Time 10.00(10.00)
Iter 7600 | Time 5.0055(5.0065) | Bit/dim 1.0994(1.1035) | Steps 556(560.07) | Grad Norm 2.5113(9.5237) | Total Time 10.00(10.00)
Iter 7610 | Time 5.4447(5.0202) | Bit/dim 1.0717(1.1033) | Steps 568(561.72) | Grad Norm 2.1716(8.9210) | Total Time 10.00(10.00)
Iter 7620 | Time 4.9384(4.9943) | Bit/dim 1.0843(1.1002) | Steps 562(559.37) | Grad Norm 6.5866(8.6573) | Total Time 10.00(10.00)
Iter 7630 | Time 5.1070(4.9977) | Bit/dim 1.1083(1.0946) | Steps 556(557.99) | Grad Norm 2.7893(7.5809) | Total Time 10.00(10.00)
Iter 7640 | Time 4.8851(4.9887) | Bit/dim 1.1056(1.0921) | Steps 550(556.84) | Grad Norm 2.0752(6.3970) | Total Time 10.00(10.00)
Iter 7650 | Time 5.0575(4.9864) | Bit/dim 1.0878(1.0894) | Steps 550(556.59) | Grad Norm 37.0589(7.1934) | Total Time 10.00(10.00)
Iter 7660 | Time 5.2538(5.0019) | Bit/dim 1.1055(1.1083) | Steps 592(558.90) | Grad Norm 3.2956(8.1697) | Total Time 10.00(10.00)
Iter 7670 | Time 5.0591(5.0464) | Bit/dim 1.0913(1.1062) | Steps 574(564.39) | Grad Norm 4.7741(7.5873) | Total Time 10.00(10.00)
Iter 7680 | Time 4.8819(5.0280) | Bit/dim 1.0847(1.1012) | Steps 550(562.90) | Grad Norm 12.4082(7.8649) | Total Time 10.00(10.00)
Iter 7690 | Time 5.0471(5.0102) | Bit/dim 1.0902(1.0992) | Steps 562(561.01) | Grad Norm 4.0978(8.2307) | Total Time 10.00(10.00)
Iter 7700 | Time 5.0587(5.0366) | Bit/dim 1.0450(1.0944) | Steps 562(561.85) | Grad Norm 1.7882(7.0898) | Total Time 10.00(10.00)
Iter 7710 | Time 4.9974(5.0449) | Bit/dim 1.2281(1.1032) | Steps 568(563.80) | Grad Norm 12.4159(9.2567) | Total Time 10.00(10.00)
Iter 7720 | Time 5.2154(5.0737) | Bit/dim 1.0946(1.1122) | Steps 586(567.16) | Grad Norm 4.8258(8.9776) | Total Time 10.00(10.00)
Iter 7730 | Time 4.9664(5.1013) | Bit/dim 1.0931(1.1102) | Steps 556(568.17) | Grad Norm 1.1042(7.4272) | Total Time 10.00(10.00)
Iter 7740 | Time 4.7276(5.0739) | Bit/dim 1.1323(1.1050) | Steps 538(564.25) | Grad Norm 13.8496(7.2561) | Total Time 10.00(10.00)
Iter 7750 | Time 5.1654(5.0613) | Bit/dim 1.0589(1.0984) | Steps 562(561.71) | Grad Norm 9.9476(7.4630) | Total Time 10.00(10.00)
Iter 7760 | Time 5.2655(5.0714) | Bit/dim 1.0680(1.0969) | Steps 568(562.37) | Grad Norm 3.0275(8.2912) | Total Time 10.00(10.00)
Iter 7770 | Time 5.1416(5.0658) | Bit/dim 1.1381(1.1133) | Steps 568(563.52) | Grad Norm 8.5637(9.2906) | Total Time 10.00(10.00)
Iter 7780 | Time 5.1691(5.0695) | Bit/dim 1.0493(1.1102) | Steps 574(566.32) | Grad Norm 1.9173(8.2789) | Total Time 10.00(10.00)
Iter 7790 | Time 4.9614(5.0084) | Bit/dim 1.1193(1.1066) | Steps 568(564.61) | Grad Norm 6.3714(7.9563) | Total Time 10.00(10.00)
validating...
Epoch 0026 | Time 59.1844, Bit/dim 1.0768
===> Using batch size 200. Total 300 iterations/epoch.
Iter 7800 | Time 4.7735(4.9466) | Bit/dim 1.0879(1.1007) | Steps 562(562.18) | Grad Norm 5.2365(7.4228) | Total Time 10.00(10.00)
Iter 7810 | Time 5.0459(4.9376) | Bit/dim 1.0710(1.0977) | Steps 568(562.76) | Grad Norm 3.1647(6.0946) | Total Time 10.00(10.00)
Iter 7820 | Time 4.7238(4.8927) | Bit/dim 1.0732(1.0941) | Steps 556(561.09) | Grad Norm 11.3982(6.0462) | Total Time 10.00(10.00)
Iter 7830 | Time 4.7543(4.8685) | Bit/dim 1.1616(1.1106) | Steps 568(562.04) | Grad Norm 12.7200(8.2319) | Total Time 10.00(10.00)
Iter 7840 | Time 5.1158(4.9086) | Bit/dim 1.1465(1.1238) | Steps 580(568.62) | Grad Norm 2.8675(8.8841) | Total Time 10.00(10.00)
Iter 7850 | Time 5.2347(4.9497) | Bit/dim 1.0923(1.1209) | Steps 580(570.26) | Grad Norm 1.7854(7.8634) | Total Time 10.00(10.00)
Iter 7860 | Time 4.8923(4.9485) | Bit/dim 1.0887(1.1117) | Steps 562(569.00) | Grad Norm 1.3741(6.2416) | Total Time 10.00(10.00)
Iter 7870 | Time 4.9119(4.9329) | Bit/dim 1.0549(1.1040) | Steps 562(567.04) | Grad Norm 2.8974(4.9807) | Total Time 10.00(10.00)
Iter 7880 | Time 4.9235(4.9247) | Bit/dim 1.0469(1.0944) | Steps 568(565.89) | Grad Norm 2.0987(4.1300) | Total Time 10.00(10.00)
Iter 7890 | Time 4.7344(4.9171) | Bit/dim 1.0375(1.0852) | Steps 556(564.84) | Grad Norm 3.3096(3.5784) | Total Time 10.00(10.00)
Iter 7900 | Time 4.8928(4.9044) | Bit/dim 1.0525(1.0804) | Steps 562(564.13) | Grad Norm 5.2538(3.7844) | Total Time 10.00(10.00)
Iter 7910 | Time 4.5840(4.8707) | Bit/dim 1.0838(1.0778) | Steps 550(562.87) | Grad Norm 8.5448(4.3278) | Total Time 10.00(10.00)
Iter 7920 | Time 5.0572(4.8898) | Bit/dim 1.0745(1.0744) | Steps 574(563.76) | Grad Norm 0.5652(3.7137) | Total Time 10.00(10.00)
Iter 7930 | Time 4.7527(4.8964) | Bit/dim 1.0611(1.0731) | Steps 562(564.39) | Grad Norm 6.5961(3.8958) | Total Time 10.00(10.00)
Iter 7940 | Time 4.7962(4.9084) | Bit/dim 1.3064(1.0890) | Steps 574(566.36) | Grad Norm 10.6828(6.9960) | Total Time 10.00(10.00)
Iter 7950 | Time 5.2231(4.9120) | Bit/dim 1.1781(1.1287) | Steps 580(568.97) | Grad Norm 4.1129(6.6489) | Total Time 10.00(10.00)
Iter 7960 | Time 5.1528(4.9450) | Bit/dim 1.1118(1.1294) | Steps 580(571.61) | Grad Norm 1.4703(6.0948) | Total Time 10.00(10.00)
Iter 7970 | Time 4.6490(4.8982) | Bit/dim 1.0909(1.1217) | Steps 562(569.46) | Grad Norm 15.1821(6.7330) | Total Time 10.00(10.00)
Iter 7980 | Time 4.9564(4.8797) | Bit/dim 1.0960(1.1132) | Steps 574(569.44) | Grad Norm 2.5138(7.1858) | Total Time 10.00(10.00)
Iter 7990 | Time 4.7603(4.8651) | Bit/dim 1.0719(1.1031) | Steps 562(568.84) | Grad Norm 8.0964(6.6046) | Total Time 10.00(10.00)
Iter 8000 | Time 5.1042(4.8872) | Bit/dim 1.2130(1.1115) | Steps 610(571.14) | Grad Norm 50.1356(9.4021) | Total Time 10.00(10.00)
Iter 8010 | Time 4.6775(4.8807) | Bit/dim 1.1758(1.1323) | Steps 550(571.43) | Grad Norm 4.5890(8.7136) | Total Time 10.00(10.00)
Iter 8020 | Time 5.1067(4.9012) | Bit/dim 1.0819(1.1273) | Steps 580(571.70) | Grad Norm 1.5281(7.0739) | Total Time 10.00(10.00)
Iter 8030 | Time 4.8687(4.9050) | Bit/dim 1.0654(1.1172) | Steps 562(570.62) | Grad Norm 2.3276(5.6805) | Total Time 10.00(10.00)
Iter 8040 | Time 4.9028(4.8933) | Bit/dim 1.0953(1.1094) | Steps 562(567.32) | Grad Norm 1.3674(4.6065) | Total Time 10.00(10.00)
Iter 8050 | Time 4.9462(4.9021) | Bit/dim 1.0841(1.1002) | Steps 574(567.70) | Grad Norm 0.8078(3.9360) | Total Time 10.00(10.00)
Iter 8060 | Time 4.8926(4.9188) | Bit/dim 1.0929(1.0950) | Steps 562(568.37) | Grad Norm 1.1271(3.5150) | Total Time 10.00(10.00)
Iter 8070 | Time 4.9506(4.9228) | Bit/dim 1.0888(1.0882) | Steps 580(569.28) | Grad Norm 0.9098(3.8054) | Total Time 10.00(10.00)
Iter 8080 | Time 5.0725(4.9206) | Bit/dim 1.1504(1.0882) | Steps 592(570.75) | Grad Norm 51.1799(6.1126) | Total Time 10.00(10.00)
Iter 8090 | Time 4.9750(4.9166) | Bit/dim 1.1803(1.1269) | Steps 592(573.77) | Grad Norm 17.3081(6.6102) | Total Time 10.00(10.00)
validating...
Epoch 0027 | Time 61.0393, Bit/dim 1.1029
===> Using batch size 200. Total 300 iterations/epoch.
Iter 8100 | Time 5.0086(4.9419) | Bit/dim 1.1380(1.1305) | Steps 568(576.26) | Grad Norm 1.7806(6.1742) | Total Time 10.00(10.00)
Iter 8110 | Time 4.9207(4.9610) | Bit/dim 1.0802(1.1223) | Steps 574(577.03) | Grad Norm 1.0938(5.5877) | Total Time 10.00(10.00)
Iter 8120 | Time 4.9031(4.9531) | Bit/dim 1.0804(1.1132) | Steps 568(575.33) | Grad Norm 1.2654(4.4861) | Total Time 10.00(10.00)
Iter 8130 | Time 4.9611(4.9450) | Bit/dim 1.0966(1.1055) | Steps 574(574.41) | Grad Norm 3.3726(3.7387) | Total Time 10.00(10.00)
Iter 8140 | Time 4.9481(4.9378) | Bit/dim 1.0693(1.0973) | Steps 568(573.51) | Grad Norm 2.8088(3.5056) | Total Time 10.00(10.00)
Iter 8150 | Time 4.7672(4.9059) | Bit/dim 1.0857(1.0937) | Steps 574(572.36) | Grad Norm 11.4537(4.6836) | Total Time 10.00(10.00)
Iter 8160 | Time 4.8270(4.8898) | Bit/dim 1.1260(1.1081) | Steps 580(573.42) | Grad Norm 7.2787(7.1452) | Total Time 10.00(10.00)
Iter 8170 | Time 5.2177(4.9185) | Bit/dim 1.1645(1.1355) | Steps 592(578.70) | Grad Norm 5.0595(7.8930) | Total Time 10.00(10.00)
Iter 8180 | Time 4.9031(4.9601) | Bit/dim 1.1066(1.1339) | Steps 568(577.03) | Grad Norm 2.6530(6.8596) | Total Time 10.00(10.00)
Iter 8190 | Time 5.0823(4.9663) | Bit/dim 1.0809(1.1221) | Steps 574(574.50) | Grad Norm 3.3964(5.8902) | Total Time 10.00(10.00)
Iter 8200 | Time 4.9144(4.9615) | Bit/dim 1.0791(1.1130) | Steps 562(573.10) | Grad Norm 1.0805(4.8336) | Total Time 10.00(10.00)
Iter 8210 | Time 4.9318(4.9479) | Bit/dim 1.1083(1.1044) | Steps 574(571.51) | Grad Norm 1.0105(3.8930) | Total Time 10.00(10.00)
Iter 8220 | Time 4.8389(4.9307) | Bit/dim 1.0870(1.0965) | Steps 556(568.95) | Grad Norm 1.4687(3.7355) | Total Time 10.00(10.00)
Iter 8230 | Time 4.8780(4.9196) | Bit/dim 1.0606(1.0908) | Steps 568(568.39) | Grad Norm 5.6155(3.6747) | Total Time 10.00(10.00)
Iter 8240 | Time 5.0205(4.9107) | Bit/dim 1.1317(1.0905) | Steps 586(569.15) | Grad Norm 48.6172(5.9516) | Total Time 10.00(10.00)
Iter 8250 | Time 4.9097(4.9112) | Bit/dim 1.1612(1.1251) | Steps 568(571.44) | Grad Norm 7.0089(6.6420) | Total Time 10.00(10.00)
Iter 8260 | Time 4.9610(4.9691) | Bit/dim 1.0743(1.1221) | Steps 580(576.19) | Grad Norm 4.3899(6.1830) | Total Time 10.00(10.00)
Iter 8270 | Time 4.7784(4.9441) | Bit/dim 1.1045(1.1148) | Steps 562(574.78) | Grad Norm 8.2502(6.5350) | Total Time 10.00(10.00)
Iter 8280 | Time 4.7910(4.9299) | Bit/dim 1.0784(1.1054) | Steps 568(573.34) | Grad Norm 5.4767(6.3879) | Total Time 10.00(10.00)
Iter 8290 | Time 5.3865(4.9402) | Bit/dim 1.1708(1.1108) | Steps 622(575.27) | Grad Norm 37.1341(8.7060) | Total Time 10.00(10.00)
Iter 8300 | Time 4.9569(4.9283) | Bit/dim 1.0984(1.1171) | Steps 580(576.88) | Grad Norm 6.2736(8.8379) | Total Time 10.00(10.00)
Iter 8310 | Time 4.7808(4.9303) | Bit/dim 1.0838(1.1107) | Steps 568(576.61) | Grad Norm 5.5537(8.3034) | Total Time 10.00(10.00)
Iter 8320 | Time 4.7586(4.9531) | Bit/dim 1.0908(1.0991) | Steps 562(575.56) | Grad Norm 4.1827(7.0427) | Total Time 10.00(10.00)
Iter 8330 | Time 4.9398(4.9596) | Bit/dim 1.0714(1.0929) | Steps 568(574.01) | Grad Norm 1.0225(5.7687) | Total Time 10.00(10.00)
Iter 8340 | Time 4.9455(4.9499) | Bit/dim 1.0600(1.0865) | Steps 574(573.21) | Grad Norm 2.8142(4.7688) | Total Time 10.00(10.00)
Iter 8350 | Time 4.9452(4.9348) | Bit/dim 1.1090(1.0828) | Steps 580(573.24) | Grad Norm 11.8741(5.0661) | Total Time 10.00(10.00)
Iter 8360 | Time 4.8167(4.9164) | Bit/dim 1.1208(1.0947) | Steps 580(573.88) | Grad Norm 9.9201(7.5485) | Total Time 10.00(10.00)
Iter 8370 | Time 5.1364(4.9368) | Bit/dim 1.0627(1.0915) | Steps 592(576.86) | Grad Norm 9.1885(7.6114) | Total Time 10.00(10.00)
Iter 8380 | Time 4.9573(4.9511) | Bit/dim 1.0764(1.0884) | Steps 580(578.39) | Grad Norm 0.8384(7.1970) | Total Time 10.00(10.00)
Iter 8390 | Time 5.1418(4.9585) | Bit/dim 1.0975(1.0840) | Steps 586(579.14) | Grad Norm 13.1859(7.2582) | Total Time 10.00(10.00)
validating...
Epoch 0028 | Time 60.7167, Bit/dim 1.0689
===> Using batch size 200. Total 300 iterations/epoch.
Iter 8400 | Time 4.7903(4.9436) | Bit/dim 1.0716(1.0794) | Steps 574(578.75) | Grad Norm 6.6329(7.1200) | Total Time 10.00(10.00)
Iter 8410 | Time 5.1153(4.9304) | Bit/dim 1.0662(1.0798) | Steps 586(578.28) | Grad Norm 1.8551(7.9887) | Total Time 10.00(10.00)
Iter 8420 | Time 4.7985(4.9254) | Bit/dim 1.1266(1.0864) | Steps 580(578.98) | Grad Norm 9.6085(9.0088) | Total Time 10.00(10.00)
Iter 8430 | Time 4.8961(4.9424) | Bit/dim 1.1396(1.0938) | Steps 574(580.17) | Grad Norm 8.2428(9.8212) | Total Time 10.00(10.00)
Iter 8440 | Time 5.0917(4.9577) | Bit/dim 1.0583(1.0949) | Steps 580(580.27) | Grad Norm 3.9817(8.7234) | Total Time 10.00(10.00)
Iter 8450 | Time 4.9339(4.9680) | Bit/dim 1.0916(1.0908) | Steps 574(579.68) | Grad Norm 3.8228(7.1786) | Total Time 10.00(10.00)
Iter 8460 | Time 4.8687(4.9697) | Bit/dim 1.0801(1.0855) | Steps 568(578.78) | Grad Norm 1.6095(5.6952) | Total Time 10.00(10.00)
Iter 8470 | Time 4.9449(4.9684) | Bit/dim 1.0792(1.0829) | Steps 580(578.79) | Grad Norm 0.8565(4.8587) | Total Time 10.00(10.00)
Iter 8480 | Time 4.7100(4.9443) | Bit/dim 1.0963(1.0941) | Steps 580(578.83) | Grad Norm 16.5209(6.9257) | Total Time 10.00(10.00)
Iter 8490 | Time 4.7703(4.9538) | Bit/dim 1.1420(1.1003) | Steps 568(581.14) | Grad Norm 11.0435(8.3413) | Total Time 10.00(10.00)
Iter 8500 | Time 4.7846(4.9448) | Bit/dim 1.0774(1.1055) | Steps 568(581.76) | Grad Norm 6.4440(8.6199) | Total Time 10.00(10.00)
Iter 8510 | Time 4.7906(4.9523) | Bit/dim 1.0711(1.1017) | Steps 574(581.46) | Grad Norm 6.4695(8.1655) | Total Time 10.00(10.00)
Iter 8520 | Time 4.9904(4.9613) | Bit/dim 1.0604(1.0935) | Steps 586(581.99) | Grad Norm 4.6929(7.2679) | Total Time 10.00(10.00)
Iter 8530 | Time 4.9550(4.9846) | Bit/dim 1.0828(1.0871) | Steps 586(583.05) | Grad Norm 0.7185(6.1834) | Total Time 10.00(10.00)
Iter 8540 | Time 5.0797(4.9765) | Bit/dim 1.0795(1.0841) | Steps 580(582.02) | Grad Norm 20.5179(6.6707) | Total Time 10.00(10.00)
Iter 8550 | Time 4.8003(4.9471) | Bit/dim 1.0869(1.0842) | Steps 580(581.09) | Grad Norm 4.6690(7.4575) | Total Time 10.00(10.00)
Iter 8560 | Time 4.9972(4.9543) | Bit/dim 1.1315(1.0865) | Steps 592(581.98) | Grad Norm 11.7485(8.4522) | Total Time 10.00(10.00)
Iter 8570 | Time 4.9631(4.9551) | Bit/dim 1.1437(1.0945) | Steps 586(582.63) | Grad Norm 8.4446(9.4949) | Total Time 10.00(10.00)
Iter 8580 | Time 4.9566(4.9639) | Bit/dim 1.0812(1.0938) | Steps 580(582.19) | Grad Norm 6.6197(9.0086) | Total Time 10.00(10.00)
Iter 8590 | Time 5.1228(4.9698) | Bit/dim 1.0989(1.0929) | Steps 586(581.94) | Grad Norm 4.7289(8.0001) | Total Time 10.00(10.00)
Iter 8600 | Time 4.9175(4.9844) | Bit/dim 1.0877(1.0879) | Steps 580(582.22) | Grad Norm 2.6565(7.0264) | Total Time 10.00(10.00)
Iter 8610 | Time 5.0717(4.9841) | Bit/dim 1.0922(1.0830) | Steps 580(581.46) | Grad Norm 3.4652(6.3099) | Total Time 10.00(10.00)
Iter 8620 | Time 4.7589(4.9528) | Bit/dim 1.1069(1.0810) | Steps 568(578.64) | Grad Norm 10.9939(6.6965) | Total Time 10.00(10.00)
Iter 8630 | Time 4.9503(4.9619) | Bit/dim 1.1452(1.0946) | Steps 592(580.17) | Grad Norm 9.8696(8.5705) | Total Time 10.00(10.00)
Iter 8640 | Time 4.9609(4.9574) | Bit/dim 1.0655(1.0951) | Steps 580(580.42) | Grad Norm 5.5197(8.3898) | Total Time 10.00(10.00)
Iter 8650 | Time 4.8676(4.9755) | Bit/dim 1.0930(1.0922) | Steps 580(581.37) | Grad Norm 4.1083(7.1798) | Total Time 10.00(10.00)
Iter 8660 | Time 4.9345(4.9895) | Bit/dim 1.0414(1.0847) | Steps 580(582.07) | Grad Norm 0.7468(6.0239) | Total Time 10.00(10.00)
Iter 8670 | Time 4.7553(4.9791) | Bit/dim 1.0864(1.0821) | Steps 574(581.12) | Grad Norm 7.6161(5.4116) | Total Time 10.00(10.00)
Iter 8680 | Time 4.8158(4.9687) | Bit/dim 1.0947(1.0834) | Steps 586(581.52) | Grad Norm 8.0934(6.9509) | Total Time 10.00(10.00)
Iter 8690 | Time 5.1487(4.9876) | Bit/dim 1.1499(1.0945) | Steps 604(585.02) | Grad Norm 7.4344(8.7019) | Total Time 10.00(10.00)
validating...
Epoch 0029 | Time 60.8941, Bit/dim 1.0896
===> Using batch size 200. Total 300 iterations/epoch.
Iter 8700 | Time 5.0275(4.9804) | Bit/dim 1.0779(1.0967) | Steps 592(584.60) | Grad Norm 5.5856(8.1473) | Total Time 10.00(10.00)
Iter 8710 | Time 5.0650(4.9856) | Bit/dim 1.0735(1.0935) | Steps 580(584.48) | Grad Norm 4.7591(7.2612) | Total Time 10.00(10.00)
Iter 8720 | Time 4.9429(4.9848) | Bit/dim 1.0696(1.0882) | Steps 574(583.31) | Grad Norm 1.2186(6.5305) | Total Time 10.00(10.00)
Iter 8730 | Time 5.0657(5.0046) | Bit/dim 1.0390(1.0828) | Steps 580(583.18) | Grad Norm 3.6683(6.0248) | Total Time 10.00(10.00)
Iter 8740 | Time 5.0281(4.9918) | Bit/dim 1.0620(1.0773) | Steps 574(581.36) | Grad Norm 5.1007(5.8047) | Total Time 10.00(10.00)
Iter 8750 | Time 4.8154(4.9775) | Bit/dim 1.1035(1.0812) | Steps 580(581.30) | Grad Norm 10.2406(7.2684) | Total Time 10.00(10.00)
Iter 8760 | Time 4.9845(4.9906) | Bit/dim 1.0628(1.0827) | Steps 586(582.74) | Grad Norm 8.6414(8.1156) | Total Time 10.00(10.00)
Iter 8770 | Time 5.0769(4.9868) | Bit/dim 1.1009(1.0832) | Steps 586(583.23) | Grad Norm 4.2792(8.1216) | Total Time 10.00(10.00)
Iter 8780 | Time 4.8087(4.9959) | Bit/dim 1.0609(1.0818) | Steps 580(582.78) | Grad Norm 8.4898(8.2944) | Total Time 10.00(10.00)
Iter 8790 | Time 4.9093(4.9940) | Bit/dim 1.0392(1.0798) | Steps 574(582.65) | Grad Norm 1.7266(8.0184) | Total Time 10.00(10.00)
Iter 8800 | Time 4.8116(4.9879) | Bit/dim 1.0674(1.0772) | Steps 586(582.10) | Grad Norm 6.7581(7.6818) | Total Time 10.00(10.00)
Iter 8810 | Time 4.7937(4.9635) | Bit/dim 1.0905(1.0745) | Steps 580(581.08) | Grad Norm 7.5786(7.3451) | Total Time 10.00(10.00)
Iter 8820 | Time 5.0844(4.9708) | Bit/dim 1.0310(1.0723) | Steps 580(582.13) | Grad Norm 3.1905(6.8808) | Total Time 10.00(10.00)
Iter 8830 | Time 5.0984(4.9764) | Bit/dim 1.1184(1.0732) | Steps 592(582.13) | Grad Norm 10.8080(7.1774) | Total Time 10.00(10.00)
Iter 8840 | Time 4.9676(4.9647) | Bit/dim 1.1637(1.0897) | Steps 586(582.50) | Grad Norm 10.6682(8.6947) | Total Time 10.00(10.00)
Iter 8850 | Time 5.1259(4.9845) | Bit/dim 1.0869(1.0930) | Steps 586(584.50) | Grad Norm 2.6787(8.7621) | Total Time 10.00(10.00)
Iter 8860 | Time 5.1429(5.0029) | Bit/dim 1.1070(1.0935) | Steps 592(585.54) | Grad Norm 3.4171(7.6075) | Total Time 10.00(10.00)
Iter 8870 | Time 5.1335(5.0280) | Bit/dim 1.0983(1.0888) | Steps 592(585.68) | Grad Norm 2.6902(6.2490) | Total Time 10.00(10.00)
Iter 8880 | Time 4.9803(5.0276) | Bit/dim 1.0723(1.0816) | Steps 586(585.77) | Grad Norm 1.4786(5.1146) | Total Time 10.00(10.00)
Iter 8890 | Time 4.9561(5.0085) | Bit/dim 1.0468(1.0784) | Steps 580(584.50) | Grad Norm 1.4249(4.1569) | Total Time 10.00(10.00)
Iter 8900 | Time 4.9431(4.9922) | Bit/dim 1.0773(1.0752) | Steps 580(583.02) | Grad Norm 3.9969(3.7851) | Total Time 10.00(10.00)
Iter 8910 | Time 4.9284(4.9820) | Bit/dim 1.0612(1.0727) | Steps 574(583.07) | Grad Norm 2.7308(4.1896) | Total Time 10.00(10.00)
Iter 8920 | Time 4.9839(4.9747) | Bit/dim 1.1068(1.0894) | Steps 592(584.62) | Grad Norm 12.6671(6.5649) | Total Time 10.00(10.00)
Iter 8930 | Time 4.9642(5.0033) | Bit/dim 1.0766(1.0891) | Steps 586(586.90) | Grad Norm 4.0075(6.0716) | Total Time 10.00(10.00)
Iter 8940 | Time 4.8217(5.0036) | Bit/dim 1.0804(1.0833) | Steps 580(586.10) | Grad Norm 14.9082(6.5217) | Total Time 10.00(10.00)
Iter 8950 | Time 4.8195(4.9946) | Bit/dim 1.1030(1.0872) | Steps 586(585.21) | Grad Norm 8.6122(8.2074) | Total Time 10.00(10.00)
Iter 8960 | Time 5.1518(4.9891) | Bit/dim 1.0830(1.0870) | Steps 598(585.76) | Grad Norm 3.8019(8.4869) | Total Time 10.00(10.00)
Iter 8970 | Time 5.1469(5.0010) | Bit/dim 1.0761(1.0834) | Steps 592(586.31) | Grad Norm 1.4336(7.9933) | Total Time 10.00(10.00)
Iter 8980 | Time 4.8092(5.0122) | Bit/dim 1.0689(1.0807) | Steps 580(586.81) | Grad Norm 5.4270(6.9764) | Total Time 10.00(10.00)
Iter 8990 | Time 4.9274(5.0104) | Bit/dim 1.0663(1.0742) | Steps 580(586.40) | Grad Norm 1.3850(5.7576) | Total Time 10.00(10.00)
validating...
Epoch 0030 | Time 61.4176, Bit/dim 1.0535
===> Using batch size 200. Total 300 iterations/epoch.
Iter 9000 | Time 4.9255(5.0000) | Bit/dim 1.0764(1.0693) | Steps 580(585.19) | Grad Norm 3.3977(4.8793) | Total Time 10.00(10.00)
Iter 9010 | Time 5.1503(4.9968) | Bit/dim 1.0255(1.0660) | Steps 598(585.41) | Grad Norm 5.0849(4.4605) | Total Time 10.00(10.00)
Iter 9020 | Time 5.1412(4.9897) | Bit/dim 1.0495(1.0641) | Steps 598(585.80) | Grad Norm 5.4883(4.5103) | Total Time 10.00(10.00)
Iter 9030 | Time 4.7776(4.9730) | Bit/dim 1.0457(1.0661) | Steps 574(585.45) | Grad Norm 7.9430(5.7651) | Total Time 10.00(10.00)
Iter 9040 | Time 4.7106(4.9481) | Bit/dim 1.1936(1.1013) | Steps 562(585.50) | Grad Norm 4.9149(7.0943) | Total Time 10.00(10.00)
Iter 9050 | Time 5.2562(4.9980) | Bit/dim 1.1063(1.1088) | Steps 592(586.57) | Grad Norm 3.6220(6.3431) | Total Time 10.00(10.00)
Iter 9060 | Time 5.0741(5.0081) | Bit/dim 1.0881(1.1023) | Steps 586(585.03) | Grad Norm 5.3385(5.8109) | Total Time 10.00(10.00)
Iter 9070 | Time 5.1615(5.0118) | Bit/dim 1.0629(1.0928) | Steps 598(585.28) | Grad Norm 5.7965(5.4215) | Total Time 10.00(10.00)
Iter 9080 | Time 5.2191(5.0115) | Bit/dim 1.0615(1.0886) | Steps 610(586.60) | Grad Norm 9.4913(6.3874) | Total Time 10.00(10.00)
Iter 9090 | Time 5.0192(5.0188) | Bit/dim 1.0915(1.0942) | Steps 592(587.28) | Grad Norm 4.4620(7.4972) | Total Time 10.00(10.00)
Iter 9100 | Time 4.8777(5.0006) | Bit/dim 1.1154(1.0944) | Steps 592(588.48) | Grad Norm 5.9835(7.8196) | Total Time 10.00(10.00)
Iter 9110 | Time 4.9883(4.9977) | Bit/dim 1.0756(1.0928) | Steps 586(588.20) | Grad Norm 4.7286(7.9132) | Total Time 10.00(10.00)
Iter 9120 | Time 4.8402(5.0080) | Bit/dim 1.0380(1.0863) | Steps 580(588.13) | Grad Norm 5.1005(8.6081) | Total Time 10.00(10.00)
Iter 9130 | Time 4.8574(5.0126) | Bit/dim 1.0220(1.0812) | Steps 592(589.68) | Grad Norm 6.5882(8.2258) | Total Time 10.00(10.00)
Iter 9140 | Time 4.7931(5.0193) | Bit/dim 1.0630(1.0769) | Steps 580(590.08) | Grad Norm 8.6077(7.4887) | Total Time 10.00(10.00)
Iter 9150 | Time 5.0968(5.0210) | Bit/dim 1.0603(1.0715) | Steps 586(589.63) | Grad Norm 17.0325(7.3288) | Total Time 10.00(10.00)
Iter 9160 | Time 4.9109(5.0008) | Bit/dim 1.0773(1.0740) | Steps 580(588.29) | Grad Norm 5.9358(8.0381) | Total Time 10.00(10.00)
Iter 9170 | Time 5.0107(4.9960) | Bit/dim 1.0820(1.0790) | Steps 592(589.03) | Grad Norm 7.3767(8.9558) | Total Time 10.00(10.00)
Iter 9180 | Time 5.2522(5.0024) | Bit/dim 1.0800(1.0816) | Steps 586(588.54) | Grad Norm 5.1727(8.5850) | Total Time 10.00(10.00)
Iter 9190 | Time 4.8963(5.0173) | Bit/dim 1.0688(1.0788) | Steps 574(588.37) | Grad Norm 2.9902(7.5936) | Total Time 10.00(10.00)
Iter 9200 | Time 4.9675(5.0311) | Bit/dim 1.0254(1.0742) | Steps 580(589.31) | Grad Norm 7.4179(6.6082) | Total Time 10.00(10.00)
Iter 9210 | Time 4.7941(5.0167) | Bit/dim 1.0499(1.0723) | Steps 574(588.15) | Grad Norm 9.8471(7.0094) | Total Time 10.00(10.00)
Iter 9220 | Time 4.8198(4.9987) | Bit/dim 1.0500(1.0703) | Steps 586(587.97) | Grad Norm 7.6391(7.9596) | Total Time 10.00(10.00)
Iter 9230 | Time 5.0131(4.9989) | Bit/dim 1.0989(1.0735) | Steps 592(588.23) | Grad Norm 18.1244(8.8827) | Total Time 10.00(10.00)
Iter 9240 | Time 5.0198(5.0040) | Bit/dim 1.0627(1.0703) | Steps 592(587.80) | Grad Norm 3.0143(7.7880) | Total Time 10.00(10.00)
Iter 9250 | Time 5.0375(5.0223) | Bit/dim 1.0238(1.0675) | Steps 598(589.13) | Grad Norm 7.6731(6.7693) | Total Time 10.00(10.00)
Iter 9260 | Time 5.1263(5.0372) | Bit/dim 1.0735(1.0682) | Steps 592(590.28) | Grad Norm 10.2843(7.1448) | Total Time 10.00(10.00)
Iter 9270 | Time 5.0777(5.0406) | Bit/dim 1.0684(1.0661) | Steps 604(590.70) | Grad Norm 9.8639(7.4680) | Total Time 10.00(10.00)
Iter 9280 | Time 4.9300(5.0333) | Bit/dim 1.0871(1.0673) | Steps 586(590.88) | Grad Norm 2.1029(7.9032) | Total Time 10.00(10.00)
Iter 9290 | Time 5.2252(5.0090) | Bit/dim 1.0735(1.0768) | Steps 592(588.84) | Grad Norm 2.1527(8.5405) | Total Time 10.00(10.00)
validating...
Epoch 0031 | Time 61.4451, Bit/dim 1.0648
===> Using batch size 200. Total 300 iterations/epoch.
Iter 9300 | Time 4.9038(5.0004) | Bit/dim 1.0671(1.0778) | Steps 580(587.99) | Grad Norm 4.5646(8.0898) | Total Time 10.00(10.00)
Iter 9310 | Time 5.1381(5.0261) | Bit/dim 1.0879(1.0761) | Steps 592(590.00) | Grad Norm 3.3693(6.8393) | Total Time 10.00(10.00)
Iter 9320 | Time 5.1546(5.0469) | Bit/dim 1.0418(1.0689) | Steps 598(590.78) | Grad Norm 3.0894(5.8954) | Total Time 10.00(10.00)
Iter 9330 | Time 4.8165(5.0237) | Bit/dim 1.0590(1.0680) | Steps 592(591.02) | Grad Norm 10.3190(6.9409) | Total Time 10.00(10.00)
Iter 9340 | Time 4.8794(5.0140) | Bit/dim 1.0751(1.0685) | Steps 592(592.39) | Grad Norm 6.9861(7.2826) | Total Time 10.00(10.00)
Iter 9350 | Time 5.0014(5.0141) | Bit/dim 1.0642(1.0682) | Steps 592(592.52) | Grad Norm 6.5075(7.3684) | Total Time 10.00(10.00)
Iter 9360 | Time 5.0642(5.0250) | Bit/dim 1.0541(1.0649) | Steps 604(593.88) | Grad Norm 3.8367(7.0942) | Total Time 10.00(10.00)
Iter 9370 | Time 5.0267(5.0446) | Bit/dim 1.0302(1.0592) | Steps 598(595.08) | Grad Norm 1.8826(5.8925) | Total Time 10.00(10.00)
Iter 9380 | Time 4.8476(5.0378) | Bit/dim 1.0510(1.0558) | Steps 586(593.94) | Grad Norm 6.0331(5.6021) | Total Time 10.00(10.00)
Iter 9390 | Time 4.9298(5.0204) | Bit/dim 1.0500(1.0543) | Steps 580(592.89) | Grad Norm 2.0251(5.7325) | Total Time 10.00(10.00)
Iter 9400 | Time 4.9773(5.0021) | Bit/dim 1.1093(1.0719) | Steps 586(591.75) | Grad Norm 7.8897(7.5228) | Total Time 10.00(10.00)
Iter 9410 | Time 5.2538(5.0218) | Bit/dim 1.1000(1.0726) | Steps 598(591.77) | Grad Norm 6.5201(7.2583) | Total Time 10.00(10.00)
Iter 9420 | Time 4.8553(4.9999) | Bit/dim 1.0766(1.0717) | Steps 586(590.64) | Grad Norm 7.8240(7.4863) | Total Time 10.00(10.00)
Iter 9430 | Time 5.1461(5.0220) | Bit/dim 1.0637(1.0691) | Steps 592(590.66) | Grad Norm 4.2532(7.0381) | Total Time 10.00(10.00)
Iter 9440 | Time 5.1719(5.0336) | Bit/dim 1.0930(1.0690) | Steps 598(591.36) | Grad Norm 25.4926(7.7031) | Total Time 10.00(10.00)
Iter 9450 | Time 4.9605(5.0280) | Bit/dim 1.0675(1.0776) | Steps 586(592.21) | Grad Norm 1.8971(8.8391) | Total Time 10.00(10.00)
Iter 9460 | Time 5.1743(5.0345) | Bit/dim 1.0946(1.0785) | Steps 604(591.93) | Grad Norm 5.1911(8.0263) | Total Time 10.00(10.00)
Iter 9470 | Time 5.1676(5.0240) | Bit/dim 1.0387(1.0735) | Steps 598(591.34) | Grad Norm 1.0979(7.3113) | Total Time 10.00(10.00)
Iter 9480 | Time 5.1533(5.0328) | Bit/dim 1.0790(1.0717) | Steps 592(591.50) | Grad Norm 8.6882(7.3324) | Total Time 10.00(10.00)
Iter 9490 | Time 5.1838(5.0445) | Bit/dim 1.0757(1.0692) | Steps 604(593.35) | Grad Norm 3.1757(6.8992) | Total Time 10.00(10.00)
Iter 9500 | Time 5.0152(5.0397) | Bit/dim 1.1464(1.0731) | Steps 598(593.46) | Grad Norm 12.0294(8.6222) | Total Time 10.00(10.00)
Iter 9510 | Time 4.7608(5.0330) | Bit/dim 1.0506(1.0704) | Steps 574(592.54) | Grad Norm 5.2317(8.1898) | Total Time 10.00(10.00)
Iter 9520 | Time 5.1718(5.0376) | Bit/dim 1.0823(1.0699) | Steps 604(593.08) | Grad Norm 1.4786(6.9181) | Total Time 10.00(10.00)
Iter 9530 | Time 5.3209(5.0575) | Bit/dim 1.0757(1.0667) | Steps 610(595.49) | Grad Norm 17.8764(6.7923) | Total Time 10.00(10.00)
Iter 9540 | Time 5.1075(5.0701) | Bit/dim 1.0749(1.0695) | Steps 592(597.42) | Grad Norm 9.0895(8.0462) | Total Time 10.00(10.00)
Iter 9550 | Time 4.8302(5.0857) | Bit/dim 1.1112(1.0764) | Steps 592(598.19) | Grad Norm 7.3396(8.8141) | Total Time 10.00(10.00)
Iter 9560 | Time 4.8763(5.0657) | Bit/dim 1.0644(1.0778) | Steps 598(598.24) | Grad Norm 6.3774(8.8015) | Total Time 10.00(10.00)
Iter 9570 | Time 5.1177(5.0630) | Bit/dim 1.0489(1.0768) | Steps 592(597.66) | Grad Norm 3.1887(8.0224) | Total Time 10.00(10.00)
Iter 9580 | Time 5.1594(5.0735) | Bit/dim 1.0694(1.0735) | Steps 598(597.18) | Grad Norm 1.1798(6.7484) | Total Time 10.00(10.00)
Iter 9590 | Time 5.2428(5.0729) | Bit/dim 1.0878(1.0682) | Steps 616(597.64) | Grad Norm 10.1914(6.4593) | Total Time 10.00(10.00)
validating...
Epoch 0032 | Time 61.7292, Bit/dim 1.1261
===> Using batch size 200. Total 300 iterations/epoch.
Iter 9600 | Time 4.5616(5.0573) | Bit/dim 1.1514(1.0800) | Steps 562(596.65) | Grad Norm 7.4032(8.0768) | Total Time 10.00(10.00)
Iter 9610 | Time 5.0241(5.0615) | Bit/dim 1.0448(1.0799) | Steps 598(596.68) | Grad Norm 1.8935(7.5331) | Total Time 10.00(10.00)
Iter 9620 | Time 5.1528(5.0748) | Bit/dim 1.0611(1.0769) | Steps 598(598.54) | Grad Norm 1.7394(6.4499) | Total Time 10.00(10.00)
Iter 9630 | Time 5.0829(5.0816) | Bit/dim 1.0605(1.0718) | Steps 610(600.29) | Grad Norm 6.2534(5.9178) | Total Time 10.00(10.00)
Iter 9640 | Time 5.2053(5.0693) | Bit/dim 1.0606(1.0686) | Steps 616(600.96) | Grad Norm 9.1240(7.2293) | Total Time 10.00(10.00)
Iter 9650 | Time 4.8216(5.0621) | Bit/dim 1.0957(1.0709) | Steps 592(601.36) | Grad Norm 6.2678(8.1340) | Total Time 10.00(10.00)
Iter 9660 | Time 5.0420(5.0518) | Bit/dim 1.0423(1.0672) | Steps 598(600.38) | Grad Norm 3.7360(7.3700) | Total Time 10.00(10.00)
Iter 9670 | Time 4.8564(5.0508) | Bit/dim 1.0540(1.0609) | Steps 592(600.32) | Grad Norm 7.1989(6.5032) | Total Time 10.00(10.00)
Iter 9680 | Time 4.8345(5.0417) | Bit/dim 1.0795(1.0605) | Steps 592(599.43) | Grad Norm 9.5468(6.4673) | Total Time 10.00(10.00)
Iter 9690 | Time 5.4072(5.0463) | Bit/dim 1.0816(1.0637) | Steps 610(600.09) | Grad Norm 21.1326(8.0239) | Total Time 10.00(10.00)
Iter 9700 | Time 5.0224(5.0672) | Bit/dim 1.1178(1.0681) | Steps 604(601.56) | Grad Norm 9.6037(8.7062) | Total Time 10.00(10.00)
Iter 9710 | Time 5.0126(5.0738) | Bit/dim 1.1219(1.0721) | Steps 598(601.58) | Grad Norm 8.4834(8.7143) | Total Time 10.00(10.00)
Iter 9720 | Time 5.0279(5.0923) | Bit/dim 1.0561(1.0715) | Steps 592(601.54) | Grad Norm 1.4006(7.6151) | Total Time 10.00(10.00)
Iter 9730 | Time 5.1791(5.1081) | Bit/dim 0.9983(1.0648) | Steps 610(602.89) | Grad Norm 2.7392(6.2660) | Total Time 10.00(10.00)
Iter 9740 | Time 4.9523(5.0902) | Bit/dim 1.0338(1.0620) | Steps 586(601.84) | Grad Norm 1.1434(5.4698) | Total Time 10.00(10.00)
Iter 9750 | Time 5.0030(5.0842) | Bit/dim 1.1346(1.0687) | Steps 598(601.94) | Grad Norm 11.2909(7.4923) | Total Time 10.00(10.00)
Iter 9760 | Time 5.3490(5.0878) | Bit/dim 1.1080(1.0835) | Steps 604(603.35) | Grad Norm 22.1641(8.6056) | Total Time 10.00(10.00)
Iter 9770 | Time 5.2019(5.0898) | Bit/dim 1.0653(1.0825) | Steps 610(603.41) | Grad Norm 1.2602(7.5653) | Total Time 10.00(10.00)
Iter 9780 | Time 5.0442(5.0871) | Bit/dim 1.0798(1.0761) | Steps 604(602.11) | Grad Norm 2.6586(6.3528) | Total Time 10.00(10.00)
Iter 9790 | Time 5.1344(5.1071) | Bit/dim 1.0342(1.0696) | Steps 604(603.50) | Grad Norm 1.3810(5.2763) | Total Time 10.00(10.00)
Iter 9800 | Time 4.8341(5.1017) | Bit/dim 1.0935(1.0683) | Steps 586(603.11) | Grad Norm 6.0744(6.0173) | Total Time 10.00(10.00)
Iter 9810 | Time 4.8196(5.0913) | Bit/dim 1.1105(1.0781) | Steps 586(602.55) | Grad Norm 6.2343(7.5258) | Total Time 10.00(10.00)
Iter 9820 | Time 4.8801(5.0819) | Bit/dim 1.0818(1.0816) | Steps 598(600.94) | Grad Norm 5.3634(7.6709) | Total Time 10.00(10.00)
Iter 9830 | Time 5.1117(5.0710) | Bit/dim 1.0726(1.0780) | Steps 598(600.17) | Grad Norm 2.6691(6.6542) | Total Time 10.00(10.00)
Iter 9840 | Time 5.1777(5.0941) | Bit/dim 1.0400(1.0738) | Steps 610(601.84) | Grad Norm 2.4613(5.3586) | Total Time 10.00(10.00)
Iter 9850 | Time 5.0773(5.0873) | Bit/dim 1.0468(1.0680) | Steps 604(602.02) | Grad Norm 1.3899(4.3040) | Total Time 10.00(10.00)
Iter 9860 | Time 5.1475(5.0804) | Bit/dim 1.0642(1.0626) | Steps 622(604.23) | Grad Norm 8.4693(4.3283) | Total Time 10.00(10.00)
Iter 9870 | Time 5.1823(5.1173) | Bit/dim 1.0755(1.0636) | Steps 610(608.27) | Grad Norm 10.1361(6.1917) | Total Time 10.00(10.00)
Iter 9880 | Time 5.0995(5.1099) | Bit/dim 1.0933(1.0752) | Steps 616(609.30) | Grad Norm 8.2403(7.5161) | Total Time 10.00(10.00)
Iter 9890 | Time 5.0566(5.1138) | Bit/dim 1.0369(1.0762) | Steps 604(609.61) | Grad Norm 5.5287(7.2752) | Total Time 10.00(10.00)
validating...
Epoch 0033 | Time 61.9123, Bit/dim 1.0502
===> Using batch size 200. Total 300 iterations/epoch.
Iter 9900 | Time 5.0734(5.1098) | Bit/dim 1.0527(1.0714) | Steps 610(608.11) | Grad Norm 2.3723(6.1173) | Total Time 10.00(10.00)
Iter 9910 | Time 5.1598(5.1301) | Bit/dim 1.0282(1.0662) | Steps 616(609.20) | Grad Norm 2.3295(5.0035) | Total Time 10.00(10.00)
Iter 9920 | Time 5.3561(5.1325) | Bit/dim 1.0496(1.0615) | Steps 634(610.08) | Grad Norm 4.3639(4.3299) | Total Time 10.00(10.00)
Iter 9930 | Time 5.0441(5.1244) | Bit/dim 1.0475(1.0590) | Steps 610(610.78) | Grad Norm 2.1136(3.8935) | Total Time 10.00(10.00)
Iter 9940 | Time 4.9132(5.1040) | Bit/dim 1.0249(1.0536) | Steps 604(610.05) | Grad Norm 8.7913(4.3408) | Total Time 10.00(10.00)
Iter 9950 | Time 5.1557(5.1089) | Bit/dim 1.0393(1.0495) | Steps 622(612.22) | Grad Norm 10.4814(4.7865) | Total Time 10.00(10.00)
Iter 9960 | Time 5.1668(5.1092) | Bit/dim 1.1044(1.0740) | Steps 604(610.75) | Grad Norm 5.2433(6.3635) | Total Time 10.00(10.00)
Iter 9970 | Time 5.1422(5.1003) | Bit/dim 1.1342(1.0914) | Steps 604(608.16) | Grad Norm 4.5631(6.9691) | Total Time 10.00(10.00)
Iter 9980 | Time 5.0590(5.0960) | Bit/dim 1.0582(1.0890) | Steps 580(603.82) | Grad Norm 1.5215(6.4786) | Total Time 10.00(10.00)
Iter 9990 | Time 5.1116(5.0866) | Bit/dim 1.0702(1.0840) | Steps 598(601.69) | Grad Norm 2.4950(5.8564) | Total Time 10.00(10.00)
Iter 10000 | Time 5.0033(5.0779) | Bit/dim 1.0715(1.0768) | Steps 598(600.93) | Grad Norm 3.9844(5.5985) | Total Time 10.00(10.00)
Iter 10010 | Time 4.9316(5.0727) | Bit/dim 1.1052(1.0870) | Steps 604(599.64) | Grad Norm 8.9409(7.5441) | Total Time 10.00(10.00)
Iter 10020 | Time 4.8516(5.0624) | Bit/dim 1.0814(1.0923) | Steps 598(600.11) | Grad Norm 4.3052(8.1155) | Total Time 10.00(10.00)
Iter 10030 | Time 5.3475(5.0839) | Bit/dim 1.0697(1.0886) | Steps 610(602.13) | Grad Norm 4.6362(7.1735) | Total Time 10.00(10.00)
Iter 10040 | Time 5.1438(5.1008) | Bit/dim 1.0278(1.0794) | Steps 598(602.48) | Grad Norm 2.1755(5.8707) | Total Time 10.00(10.00)
Iter 10050 | Time 5.1635(5.1084) | Bit/dim 1.0473(1.0711) | Steps 610(603.56) | Grad Norm 1.6834(4.9769) | Total Time 10.00(10.00)
Iter 10060 | Time 4.9928(5.1283) | Bit/dim 1.0802(1.0672) | Steps 604(604.82) | Grad Norm 5.8597(6.3232) | Total Time 10.00(10.00)
Iter 10070 | Time 4.8249(5.1258) | Bit/dim 1.0529(1.0757) | Steps 592(605.11) | Grad Norm 7.0444(7.7733) | Total Time 10.00(10.00)
Iter 10080 | Time 4.8968(5.1154) | Bit/dim 1.0877(1.0751) | Steps 604(605.05) | Grad Norm 6.3208(7.5786) | Total Time 10.00(10.00)
Iter 10090 | Time 5.1641(5.1144) | Bit/dim 1.0259(1.0721) | Steps 604(606.18) | Grad Norm 1.3531(6.5032) | Total Time 10.00(10.00)
Iter 10100 | Time 5.1733(5.1154) | Bit/dim 1.0545(1.0652) | Steps 604(605.74) | Grad Norm 1.0613(5.3211) | Total Time 10.00(10.00)
Iter 10110 | Time 5.0448(5.0895) | Bit/dim 1.0297(1.0588) | Steps 604(604.81) | Grad Norm 6.4122(4.8335) | Total Time 10.00(10.00)
Iter 10120 | Time 5.4266(5.1227) | Bit/dim 1.1138(1.0628) | Steps 628(607.23) | Grad Norm 10.0824(6.9164) | Total Time 10.00(10.00)
Iter 10130 | Time 5.3647(5.1158) | Bit/dim 1.0801(1.0705) | Steps 622(607.85) | Grad Norm 5.8499(7.8911) | Total Time 10.00(10.00)
Iter 10140 | Time 5.1387(5.1235) | Bit/dim 1.0787(1.0675) | Steps 616(609.01) | Grad Norm 8.8552(7.3034) | Total Time 10.00(10.00)
Iter 10150 | Time 5.1544(5.1313) | Bit/dim 1.0470(1.0640) | Steps 604(609.25) | Grad Norm 1.9152(6.1221) | Total Time 10.00(10.00)
Iter 10160 | Time 5.0716(5.1278) | Bit/dim 1.0521(1.0613) | Steps 610(609.29) | Grad Norm 1.0919(4.9669) | Total Time 10.00(10.00)
Iter 10170 | Time 5.1772(5.1347) | Bit/dim 1.0364(1.0554) | Steps 616(611.40) | Grad Norm 2.4427(4.1296) | Total Time 10.00(10.00)
Iter 10180 | Time 5.0553(5.1150) | Bit/dim 1.0227(1.0509) | Steps 604(609.92) | Grad Norm 4.0004(3.9553) | Total Time 10.00(10.00)
Iter 10190 | Time 5.0624(5.1035) | Bit/dim 1.0390(1.0504) | Steps 610(610.26) | Grad Norm 3.5231(3.6855) | Total Time 10.00(10.00)
validating...
Epoch 0034 | Time 62.2908, Bit/dim 1.0385
===> Using batch size 200. Total 300 iterations/epoch.
Iter 10200 | Time 5.0679(5.0937) | Bit/dim 1.0505(1.0498) | Steps 610(610.96) | Grad Norm 4.7150(4.2306) | Total Time 10.00(10.00)
Iter 10210 | Time 5.0964(5.0869) | Bit/dim 1.0639(1.0474) | Steps 616(610.05) | Grad Norm 5.5395(4.1524) | Total Time 10.00(10.00)
Iter 10220 | Time 4.9466(5.1002) | Bit/dim 1.1282(1.0667) | Steps 604(611.10) | Grad Norm 10.4182(5.9564) | Total Time 10.00(10.00)
Iter 10230 | Time 5.1956(5.1071) | Bit/dim 1.0987(1.0753) | Steps 610(611.38) | Grad Norm 2.0838(6.5198) | Total Time 10.00(10.00)
Iter 10240 | Time 4.9795(5.0948) | Bit/dim 1.0648(1.0732) | Steps 604(609.50) | Grad Norm 3.8223(5.8606) | Total Time 10.00(10.00)
Iter 10250 | Time 5.0801(5.0785) | Bit/dim 1.0737(1.0670) | Steps 592(606.88) | Grad Norm 4.2323(5.6629) | Total Time 10.00(10.00)
Iter 10260 | Time 4.9211(5.0877) | Bit/dim 1.0790(1.0740) | Steps 598(607.33) | Grad Norm 4.6627(7.3645) | Total Time 10.00(10.00)
Iter 10270 | Time 5.3529(5.1331) | Bit/dim 1.0428(1.0773) | Steps 634(611.15) | Grad Norm 4.3932(7.8251) | Total Time 10.00(10.00)
Iter 10280 | Time 5.1826(5.1298) | Bit/dim 1.0838(1.0739) | Steps 610(610.89) | Grad Norm 2.8907(6.9686) | Total Time 10.00(10.00)
Iter 10290 | Time 5.0932(5.1347) | Bit/dim 1.0403(1.0687) | Steps 610(611.22) | Grad Norm 3.4340(6.1354) | Total Time 10.00(10.00)
Iter 10300 | Time 5.0923(5.1198) | Bit/dim 1.0654(1.0665) | Steps 610(610.49) | Grad Norm 6.7717(6.0420) | Total Time 10.00(10.00)
Iter 10310 | Time 5.4773(5.1615) | Bit/dim 1.1269(1.0728) | Steps 628(614.16) | Grad Norm 21.3407(8.0421) | Total Time 10.00(10.00)
Iter 10320 | Time 5.1946(5.1829) | Bit/dim 1.0373(1.0704) | Steps 628(617.24) | Grad Norm 1.7190(7.4298) | Total Time 10.00(10.00)
Iter 10330 | Time 5.2136(5.1821) | Bit/dim 1.0430(1.0663) | Steps 616(617.86) | Grad Norm 1.1992(6.3905) | Total Time 10.00(10.00)
Iter 10340 | Time 4.9466(5.1668) | Bit/dim 1.0443(1.0601) | Steps 610(616.99) | Grad Norm 7.6744(5.7492) | Total Time 10.00(10.00)
Iter 10350 | Time 5.4143(5.1847) | Bit/dim 1.0697(1.0578) | Steps 628(617.04) | Grad Norm 9.4072(6.8940) | Total Time 10.00(10.00)
Iter 10360 | Time 5.0789(5.2049) | Bit/dim 1.1010(1.0645) | Steps 616(618.26) | Grad Norm 10.1911(8.2326) | Total Time 10.00(10.00)
Iter 10370 | Time 5.0657(5.1726) | Bit/dim 1.0602(1.0706) | Steps 604(615.89) | Grad Norm 12.5685(8.3735) | Total Time 10.00(10.00)
Iter 10380 | Time 5.3282(5.1638) | Bit/dim 1.0482(1.0688) | Steps 634(615.60) | Grad Norm 0.9744(7.2951) | Total Time 10.00(10.00)
Iter 10390 | Time 5.0245(5.1535) | Bit/dim 1.0275(1.0637) | Steps 604(614.24) | Grad Norm 1.2475(5.9671) | Total Time 10.00(10.00)
Iter 10400 | Time 5.1460(5.1389) | Bit/dim 1.0405(1.0591) | Steps 604(612.25) | Grad Norm 1.6089(4.9629) | Total Time 10.00(10.00)
Iter 10410 | Time 5.2509(5.1565) | Bit/dim 1.0334(1.0542) | Steps 622(613.49) | Grad Norm 2.1418(4.4868) | Total Time 10.00(10.00)
Iter 10420 | Time 5.1434(5.1572) | Bit/dim 1.0224(1.0494) | Steps 622(614.72) | Grad Norm 6.1105(4.1307) | Total Time 10.00(10.00)
Iter 10430 | Time 5.4270(5.1819) | Bit/dim 1.0866(1.0502) | Steps 628(617.14) | Grad Norm 13.0796(5.1976) | Total Time 10.00(10.00)
Iter 10440 | Time 5.1935(5.1777) | Bit/dim 1.1431(1.0772) | Steps 610(617.40) | Grad Norm 5.7183(6.7212) | Total Time 10.00(10.00)
Iter 10450 | Time 5.1991(5.1705) | Bit/dim 1.0907(1.0825) | Steps 610(616.16) | Grad Norm 4.6474(6.8663) | Total Time 10.00(10.00)
Iter 10460 | Time 5.1070(5.2007) | Bit/dim 1.0246(1.0784) | Steps 616(619.08) | Grad Norm 2.6734(5.8225) | Total Time 10.00(10.00)
Iter 10470 | Time 5.2135(5.1919) | Bit/dim 1.0469(1.0703) | Steps 616(619.49) | Grad Norm 4.0521(5.0589) | Total Time 10.00(10.00)
Iter 10480 | Time 5.4422(5.1806) | Bit/dim 1.0277(1.0658) | Steps 634(618.33) | Grad Norm 8.6728(5.4561) | Total Time 10.00(10.00)
Iter 10490 | Time 5.4531(5.2089) | Bit/dim 1.0545(1.0660) | Steps 634(619.48) | Grad Norm 8.3436(6.6765) | Total Time 10.00(10.00)
validating...
Epoch 0035 | Time 62.1391, Bit/dim 1.0719
===> Using batch size 200. Total 300 iterations/epoch.
Iter 10500 | Time 5.0440(5.2180) | Bit/dim 1.1010(1.0689) | Steps 610(619.72) | Grad Norm 6.2804(7.6041) | Total Time 10.00(10.00)
Iter 10510 | Time 5.3353(5.2263) | Bit/dim 1.0894(1.0681) | Steps 634(621.55) | Grad Norm 3.3305(7.4467) | Total Time 10.00(10.00)
Iter 10520 | Time 5.1397(5.2141) | Bit/dim 1.0566(1.0660) | Steps 622(621.79) | Grad Norm 2.1057(6.2301) | Total Time 10.00(10.00)
Iter 10530 | Time 5.2732(5.2118) | Bit/dim 1.0481(1.0608) | Steps 622(622.41) | Grad Norm 2.4640(5.2100) | Total Time 10.00(10.00)
Iter 10540 | Time 5.5153(5.2391) | Bit/dim 1.0685(1.0584) | Steps 640(624.18) | Grad Norm 10.3763(5.9031) | Total Time 10.00(10.00)
Iter 10550 | Time 5.2330(5.2175) | Bit/dim 1.0921(1.0777) | Steps 616(622.99) | Grad Norm 7.0023(7.3153) | Total Time 10.00(10.00)
Iter 10560 | Time 5.2534(5.2107) | Bit/dim 1.0639(1.0783) | Steps 616(622.55) | Grad Norm 7.9032(6.9126) | Total Time 10.00(10.00)
Iter 10570 | Time 5.1582(5.2083) | Bit/dim 1.0603(1.0720) | Steps 622(622.29) | Grad Norm 4.0012(6.0197) | Total Time 10.00(10.00)
Iter 10580 | Time 5.2748(5.2174) | Bit/dim 1.0628(1.0680) | Steps 622(622.49) | Grad Norm 1.3946(4.9915) | Total Time 10.00(10.00)
Iter 10590 | Time 5.1477(5.2270) | Bit/dim 1.0231(1.0614) | Steps 622(623.47) | Grad Norm 1.3175(4.2086) | Total Time 10.00(10.00)
Iter 10600 | Time 5.0255(5.2299) | Bit/dim 1.0769(1.0576) | Steps 622(624.30) | Grad Norm 4.1556(4.1558) | Total Time 10.00(10.00)
Iter 10610 | Time 5.2077(5.2254) | Bit/dim 1.0490(1.0502) | Steps 628(624.36) | Grad Norm 7.8147(4.2096) | Total Time 10.00(10.00)
Iter 10620 | Time 5.5605(5.2562) | Bit/dim 1.1436(1.0547) | Steps 652(626.29) | Grad Norm 9.9456(5.6636) | Total Time 10.00(10.00)
Iter 10630 | Time 5.2608(5.2233) | Bit/dim 1.1140(1.0756) | Steps 622(624.04) | Grad Norm 5.2403(6.9191) | Total Time 10.00(10.00)
Iter 10640 | Time 5.1839(5.2439) | Bit/dim 1.0842(1.0820) | Steps 628(624.27) | Grad Norm 6.2128(7.4813) | Total Time 10.00(10.00)
Iter 10650 | Time 5.2378(5.2623) | Bit/dim 1.1022(1.0822) | Steps 622(626.23) | Grad Norm 3.5087(6.6381) | Total Time 10.00(10.00)
Iter 10660 | Time 4.9265(5.2411) | Bit/dim 1.0431(1.0757) | Steps 604(625.53) | Grad Norm 6.0410(5.9932) | Total Time 10.00(10.00)
Iter 10670 | Time 4.9493(5.2051) | Bit/dim 1.0330(1.0655) | Steps 610(623.40) | Grad Norm 9.5657(5.7468) | Total Time 10.00(10.00)
Iter 10680 | Time 5.6411(5.2410) | Bit/dim 1.0346(1.0597) | Steps 640(625.53) | Grad Norm 15.5072(6.6305) | Total Time 10.00(10.00)
Iter 10690 | Time 5.1785(5.2712) | Bit/dim 1.0580(1.0578) | Steps 622(627.47) | Grad Norm 3.8730(7.0074) | Total Time 10.00(10.00)
Iter 10700 | Time 5.4169(5.2931) | Bit/dim 1.0858(1.0611) | Steps 634(627.75) | Grad Norm 2.7871(7.8158) | Total Time 10.00(10.00)
Iter 10710 | Time 5.0384(5.2592) | Bit/dim 1.0182(1.0593) | Steps 622(626.51) | Grad Norm 4.3942(6.9439) | Total Time 10.00(10.00)
Iter 10720 | Time 5.3504(5.2393) | Bit/dim 1.0593(1.0548) | Steps 634(625.13) | Grad Norm 2.7614(5.8661) | Total Time 10.00(10.00)
Iter 10730 | Time 5.6751(5.2237) | Bit/dim 1.0745(1.0524) | Steps 640(622.66) | Grad Norm 18.7353(6.0854) | Total Time 10.00(10.00)
Iter 10740 | Time 5.2900(5.2565) | Bit/dim 1.1023(1.0629) | Steps 622(623.84) | Grad Norm 12.0894(7.3234) | Total Time 10.00(10.00)
Iter 10750 | Time 5.1460(5.2477) | Bit/dim 1.0482(1.0658) | Steps 622(623.49) | Grad Norm 2.6265(7.1620) | Total Time 10.00(10.00)
Iter 10760 | Time 5.0391(5.2325) | Bit/dim 1.0687(1.0617) | Steps 622(622.46) | Grad Norm 6.6577(6.5634) | Total Time 10.00(10.00)
Iter 10770 | Time 5.0660(5.2119) | Bit/dim 1.0677(1.0597) | Steps 610(621.83) | Grad Norm 1.5387(5.7188) | Total Time 10.00(10.00)
Iter 10780 | Time 5.3227(5.2236) | Bit/dim 1.0312(1.0551) | Steps 628(623.54) | Grad Norm 5.9642(5.2929) | Total Time 10.00(10.00)
Iter 10790 | Time 5.4799(5.2616) | Bit/dim 1.0417(1.0519) | Steps 640(626.30) | Grad Norm 1.4021(5.6871) | Total Time 10.00(10.00)
validating...
Epoch 0036 | Time 62.3243, Bit/dim 1.0340
===> Using batch size 200. Total 300 iterations/epoch.
Iter 10800 | Time 5.2148(5.2732) | Bit/dim 1.0518(1.0489) | Steps 634(628.13) | Grad Norm 3.2854(5.0359) | Total Time 10.00(10.00)
Iter 10810 | Time 5.2152(5.2608) | Bit/dim 1.0218(1.0463) | Steps 616(628.20) | Grad Norm 2.5792(4.8510) | Total Time 10.00(10.00)
Iter 10820 | Time 4.9465(5.2759) | Bit/dim 1.0850(1.0636) | Steps 610(628.23) | Grad Norm 4.0333(6.4996) | Total Time 10.00(10.00)
Iter 10830 | Time 5.3620(5.2857) | Bit/dim 1.0492(1.0656) | Steps 634(627.83) | Grad Norm 9.4766(6.6463) | Total Time 10.00(10.00)
Iter 10840 | Time 5.3852(5.2524) | Bit/dim 1.0634(1.0634) | Steps 622(625.43) | Grad Norm 1.1903(5.9223) | Total Time 10.00(10.00)
Iter 10850 | Time 5.3056(5.2383) | Bit/dim 1.0447(1.0578) | Steps 628(624.64) | Grad Norm 4.6432(5.2768) | Total Time 10.00(10.00)
Iter 10860 | Time 5.1495(5.2549) | Bit/dim 1.1099(1.0583) | Steps 616(624.04) | Grad Norm 32.1483(6.9218) | Total Time 10.00(10.00)
Iter 10870 | Time 5.5766(5.2821) | Bit/dim 1.0771(1.0625) | Steps 634(626.24) | Grad Norm 18.6345(7.8604) | Total Time 10.00(10.00)
Iter 10880 | Time 5.2893(5.2717) | Bit/dim 1.0585(1.0614) | Steps 628(625.15) | Grad Norm 2.5101(7.1217) | Total Time 10.00(10.00)
Iter 10890 | Time 5.1254(5.2591) | Bit/dim 1.0406(1.0604) | Steps 622(624.82) | Grad Norm 5.4981(6.5367) | Total Time 10.00(10.00)
Iter 10900 | Time 5.1961(5.2311) | Bit/dim 1.0134(1.0542) | Steps 628(623.98) | Grad Norm 3.7280(5.9779) | Total Time 10.00(10.00)
Iter 10910 | Time 5.2331(5.2442) | Bit/dim 1.0632(1.0510) | Steps 634(626.27) | Grad Norm 4.1801(5.1268) | Total Time 10.00(10.00)
Iter 10920 | Time 5.3473(5.2500) | Bit/dim 1.0524(1.0463) | Steps 634(626.92) | Grad Norm 1.2955(4.6240) | Total Time 10.00(10.00)
Iter 10930 | Time 5.4142(5.2742) | Bit/dim 1.0256(1.0441) | Steps 640(629.03) | Grad Norm 6.0031(4.9056) | Total Time 10.00(10.00)
Iter 10940 | Time 5.5857(5.3230) | Bit/dim 1.1113(1.0448) | Steps 652(632.07) | Grad Norm 11.6029(6.6002) | Total Time 10.00(10.00)
Iter 10950 | Time 5.5863(5.3292) | Bit/dim 1.0751(1.0612) | Steps 634(631.82) | Grad Norm 2.9064(6.8556) | Total Time 10.00(10.00)
Iter 10960 | Time 5.3768(5.3230) | Bit/dim 1.0451(1.0654) | Steps 640(631.42) | Grad Norm 3.9024(6.3098) | Total Time 10.00(10.00)
Iter 10970 | Time 5.3428(5.2795) | Bit/dim 1.0566(1.0623) | Steps 634(628.08) | Grad Norm 4.0755(6.1618) | Total Time 10.00(10.00)
Iter 10980 | Time 5.7406(5.2794) | Bit/dim 1.0616(1.0564) | Steps 652(626.16) | Grad Norm 17.6379(6.2312) | Total Time 10.00(10.00)
Iter 10990 | Time 5.5004(5.3050) | Bit/dim 1.0715(1.0592) | Steps 640(627.16) | Grad Norm 7.1405(7.2134) | Total Time 10.00(10.00)
Iter 11000 | Time 5.5623(5.3295) | Bit/dim 1.0757(1.0593) | Steps 646(629.94) | Grad Norm 22.6480(7.9727) | Total Time 10.00(10.00)
Iter 11010 | Time 5.4741(5.3589) | Bit/dim 1.0747(1.0605) | Steps 634(631.67) | Grad Norm 6.5794(8.0537) | Total Time 10.00(10.00)
Iter 11020 | Time 5.2792(5.3465) | Bit/dim 1.0765(1.0587) | Steps 628(631.62) | Grad Norm 4.2636(7.0386) | Total Time 10.00(10.00)
Iter 11030 | Time 5.0736(5.3286) | Bit/dim 1.0229(1.0544) | Steps 628(631.64) | Grad Norm 3.1115(5.9771) | Total Time 10.00(10.00)
Iter 11040 | Time 5.2635(5.3058) | Bit/dim 1.0437(1.0491) | Steps 622(631.20) | Grad Norm 1.6337(5.3909) | Total Time 10.00(10.00)
Iter 11050 | Time 5.4676(5.3337) | Bit/dim 1.1258(1.0550) | Steps 634(632.36) | Grad Norm 10.1028(7.6863) | Total Time 10.00(10.00)
Iter 11060 | Time 5.0344(5.3105) | Bit/dim 1.0831(1.0709) | Steps 628(632.47) | Grad Norm 5.3660(7.8159) | Total Time 10.00(10.00)
Iter 11070 | Time 5.3299(5.3262) | Bit/dim 1.0740(1.0710) | Steps 634(633.48) | Grad Norm 4.6208(7.4998) | Total Time 10.00(10.00)
Iter 11080 | Time 5.1458(5.3020) | Bit/dim 1.0749(1.0663) | Steps 622(632.17) | Grad Norm 2.5822(6.2227) | Total Time 10.00(10.00)
Iter 11090 | Time 5.1607(5.2825) | Bit/dim 1.0517(1.0586) | Steps 628(630.55) | Grad Norm 1.7207(5.0848) | Total Time 10.00(10.00)
validating...
Epoch 0037 | Time 62.5294, Bit/dim 1.0332
===> Using batch size 200. Total 300 iterations/epoch.
Iter 11100 | Time 5.3253(5.2813) | Bit/dim 1.0298(1.0520) | Steps 628(630.81) | Grad Norm 1.6392(4.5148) | Total Time 10.00(10.00)
Iter 11110 | Time 5.2838(5.2696) | Bit/dim 1.0390(1.0480) | Steps 628(630.33) | Grad Norm 2.5653(4.1191) | Total Time 10.00(10.00)
Iter 11120 | Time 5.5146(5.3192) | Bit/dim 1.2117(1.0567) | Steps 646(633.19) | Grad Norm 7.0678(6.0324) | Total Time 10.00(10.00)
Iter 11130 | Time 5.6357(5.3207) | Bit/dim 1.0695(1.0674) | Steps 640(632.55) | Grad Norm 5.2339(6.4611) | Total Time 10.00(10.00)
Iter 11140 | Time 4.9660(5.2977) | Bit/dim 1.0807(1.0681) | Steps 610(629.63) | Grad Norm 9.5979(6.1107) | Total Time 10.00(10.00)
Iter 11150 | Time 5.3942(5.2993) | Bit/dim 1.0252(1.0655) | Steps 640(630.17) | Grad Norm 4.9204(5.7347) | Total Time 10.00(10.00)
Iter 11160 | Time 5.1933(5.2672) | Bit/dim 1.0323(1.0579) | Steps 610(626.67) | Grad Norm 2.5324(5.1916) | Total Time 10.00(10.00)
Iter 11170 | Time 5.2759(5.2948) | Bit/dim 1.0289(1.0539) | Steps 634(628.67) | Grad Norm 4.2560(5.8169) | Total Time 10.00(10.00)
Iter 11180 | Time 5.3992(5.3495) | Bit/dim 1.0437(1.0604) | Steps 646(633.09) | Grad Norm 5.3720(7.0989) | Total Time 10.00(10.00)
Iter 11190 | Time 5.2620(5.3414) | Bit/dim 1.0706(1.0573) | Steps 640(632.46) | Grad Norm 4.6378(6.5129) | Total Time 10.00(10.00)
Iter 11200 | Time 5.5292(5.3377) | Bit/dim 1.0260(1.0543) | Steps 640(632.34) | Grad Norm 1.5269(5.5016) | Total Time 10.00(10.00)
Iter 11210 | Time 5.5733(5.3505) | Bit/dim 1.0856(1.0528) | Steps 640(632.56) | Grad Norm 21.9178(5.7670) | Total Time 10.00(10.00)
Iter 11220 | Time 5.4741(5.3740) | Bit/dim 1.0473(1.0554) | Steps 640(633.99) | Grad Norm 7.2488(7.1197) | Total Time 10.00(10.00)
Iter 11230 | Time 5.2018(5.3946) | Bit/dim 1.0553(1.0537) | Steps 628(635.04) | Grad Norm 3.9948(7.1052) | Total Time 10.00(10.00)
Iter 11240 | Time 5.2352(5.3639) | Bit/dim 1.0652(1.0528) | Steps 634(634.65) | Grad Norm 3.7795(6.3503) | Total Time 10.00(10.00)
Iter 11250 | Time 5.0426(5.3353) | Bit/dim 1.0226(1.0500) | Steps 628(634.82) | Grad Norm 1.8580(5.5862) | Total Time 10.00(10.00)
Iter 11260 | Time 5.2246(5.3172) | Bit/dim 1.0751(1.0476) | Steps 634(634.57) | Grad Norm 1.9246(4.6249) | Total Time 10.00(10.00)
Iter 11270 | Time 5.2193(5.3204) | Bit/dim 1.0302(1.0445) | Steps 634(635.39) | Grad Norm 3.1186(4.4575) | Total Time 10.00(10.00)
Iter 11280 | Time 5.1962(5.3362) | Bit/dim 1.0482(1.0530) | Steps 628(636.92) | Grad Norm 3.9139(5.9323) | Total Time 10.00(10.00)
Iter 11290 | Time 5.2058(5.3435) | Bit/dim 1.0963(1.0759) | Steps 616(635.95) | Grad Norm 5.3185(7.3218) | Total Time 10.00(10.00)
Iter 11300 | Time 5.3653(5.3273) | Bit/dim 1.1043(1.0789) | Steps 634(635.06) | Grad Norm 7.9216(6.8240) | Total Time 10.00(10.00)
Iter 11310 | Time 5.2845(5.2948) | Bit/dim 1.0301(1.0732) | Steps 628(632.43) | Grad Norm 3.1311(5.9621) | Total Time 10.00(10.00)
Iter 11320 | Time 5.0903(5.2687) | Bit/dim 1.0637(1.0660) | Steps 610(629.66) | Grad Norm 2.4316(5.2024) | Total Time 10.00(10.00)
Iter 11330 | Time 5.3355(5.2894) | Bit/dim 1.0306(1.0575) | Steps 628(628.64) | Grad Norm 9.6388(5.4446) | Total Time 10.00(10.00)
Iter 11340 | Time 5.6486(5.3220) | Bit/dim 1.0587(1.0551) | Steps 640(629.95) | Grad Norm 19.2844(6.1871) | Total Time 10.00(10.00)
Iter 11350 | Time 5.2346(5.3458) | Bit/dim 1.0645(1.0670) | Steps 634(633.51) | Grad Norm 3.2062(7.2427) | Total Time 10.00(10.00)
Iter 11360 | Time 5.3324(5.3502) | Bit/dim 1.0775(1.0698) | Steps 634(634.43) | Grad Norm 3.4035(6.8457) | Total Time 10.00(10.00)
Iter 11370 | Time 5.2347(5.3364) | Bit/dim 1.0053(1.0634) | Steps 640(635.24) | Grad Norm 5.0153(5.9729) | Total Time 10.00(10.00)
Iter 11380 | Time 5.0545(5.3110) | Bit/dim 1.0234(1.0553) | Steps 610(633.52) | Grad Norm 1.6922(5.0922) | Total Time 10.00(10.00)
Iter 11390 | Time 5.4481(5.3292) | Bit/dim 1.0548(1.0531) | Steps 640(633.84) | Grad Norm 25.0796(5.8806) | Total Time 10.00(10.00)
validating...
Epoch 0038 | Time 66.9750, Bit/dim 1.0679
===> Using batch size 200. Total 300 iterations/epoch.
Iter 11400 | Time 5.1106(5.3408) | Bit/dim 1.0866(1.0612) | Steps 616(635.14) | Grad Norm 3.3044(6.9275) | Total Time 10.00(10.00)
Iter 11410 | Time 5.3445(5.3335) | Bit/dim 1.0850(1.0677) | Steps 646(635.44) | Grad Norm 5.4416(7.2925) | Total Time 10.00(10.00)
Iter 11420 | Time 5.2693(5.3325) | Bit/dim 1.0643(1.0641) | Steps 622(634.73) | Grad Norm 5.0528(6.5147) | Total Time 10.00(10.00)
Iter 11430 | Time 5.1916(5.3292) | Bit/dim 1.0560(1.0574) | Steps 628(635.24) | Grad Norm 1.3280(5.3412) | Total Time 10.00(10.00)
Iter 11440 | Time 5.1961(5.3082) | Bit/dim 1.0241(1.0512) | Steps 622(633.10) | Grad Norm 2.1626(4.4326) | Total Time 10.00(10.00)
Iter 11450 | Time 5.2398(5.2802) | Bit/dim 1.0019(1.0442) | Steps 616(629.27) | Grad Norm 1.0586(3.7043) | Total Time 10.00(10.00)
Iter 11460 | Time 5.5732(5.2754) | Bit/dim 1.0275(1.0434) | Steps 646(628.73) | Grad Norm 5.6164(3.4560) | Total Time 10.00(10.00)
Iter 11470 | Time 5.4198(5.3045) | Bit/dim 1.0512(1.0437) | Steps 634(629.92) | Grad Norm 32.8436(5.2735) | Total Time 10.00(10.00)
Iter 11480 | Time 5.3393(5.3220) | Bit/dim 1.1608(1.0635) | Steps 640(631.71) | Grad Norm 6.3296(6.6382) | Total Time 10.00(10.00)
Iter 11490 | Time 5.3417(5.3092) | Bit/dim 1.0581(1.0659) | Steps 634(629.77) | Grad Norm 2.9687(6.2875) | Total Time 10.00(10.00)
Iter 11500 | Time 5.1452(5.3034) | Bit/dim 1.0569(1.0608) | Steps 628(630.56) | Grad Norm 2.1211(5.2218) | Total Time 10.00(10.00)
Iter 11510 | Time 5.0723(5.2756) | Bit/dim 1.0197(1.0542) | Steps 616(629.60) | Grad Norm 1.5114(4.3915) | Total Time 10.00(10.00)
Iter 11520 | Time 5.5876(5.2680) | Bit/dim 1.0400(1.0475) | Steps 652(628.97) | Grad Norm 9.6738(4.3907) | Total Time 10.00(10.00)
Iter 11530 | Time 5.6430(5.3098) | Bit/dim 1.1128(1.0596) | Steps 658(633.58) | Grad Norm 32.5565(6.5267) | Total Time 10.00(10.00)
Iter 11540 | Time 5.3356(5.3297) | Bit/dim 1.0588(1.0663) | Steps 640(635.76) | Grad Norm 4.8222(6.4235) | Total Time 10.00(10.00)
Iter 11550 | Time 5.3403(5.3201) | Bit/dim 1.0379(1.0644) | Steps 634(635.23) | Grad Norm 1.3756(5.3986) | Total Time 10.00(10.00)
Iter 11560 | Time 5.2991(5.3135) | Bit/dim 1.0551(1.0595) | Steps 640(634.53) | Grad Norm 1.4578(4.3914) | Total Time 10.00(10.00)
Iter 11570 | Time 5.3546(5.3386) | Bit/dim 1.0204(1.0528) | Steps 634(635.22) | Grad Norm 2.4071(3.6154) | Total Time 10.00(10.00)
Iter 11580 | Time 5.1520(5.3258) | Bit/dim 1.0215(1.0467) | Steps 640(634.76) | Grad Norm 3.0201(3.2534) | Total Time 10.00(10.00)
Iter 11590 | Time 5.2917(5.3325) | Bit/dim 1.0592(1.0465) | Steps 622(634.54) | Grad Norm 29.5966(4.5109) | Total Time 10.00(10.00)
Iter 11600 | Time 5.0135(5.3128) | Bit/dim 1.1553(1.0684) | Steps 628(634.22) | Grad Norm 6.4252(6.2439) | Total Time 10.00(10.00)
Iter 11610 | Time 5.3419(5.3211) | Bit/dim 1.0599(1.0744) | Steps 640(635.54) | Grad Norm 9.1679(6.3583) | Total Time 10.00(10.00)
Iter 11620 | Time 5.1850(5.3208) | Bit/dim 1.0446(1.0700) | Steps 634(636.14) | Grad Norm 3.0672(5.5507) | Total Time 10.00(10.00)
Iter 11630 | Time 5.2426(5.3167) | Bit/dim 1.0538(1.0639) | Steps 640(636.54) | Grad Norm 4.8600(4.9531) | Total Time 10.00(10.00)
Iter 11640 | Time 5.5141(5.3157) | Bit/dim 1.0546(1.0595) | Steps 646(634.89) | Grad Norm 6.2221(5.7150) | Total Time 10.00(10.00)
Iter 11650 | Time 5.4107(5.3114) | Bit/dim 1.0202(1.0504) | Steps 640(634.66) | Grad Norm 0.7325(4.8784) | Total Time 10.00(10.00)
Iter 11660 | Time 5.0363(5.3159) | Bit/dim 1.0234(1.0455) | Steps 622(635.57) | Grad Norm 3.5804(4.8100) | Total Time 10.00(10.00)
Iter 11670 | Time 5.6373(5.3306) | Bit/dim 1.0073(1.0405) | Steps 658(636.91) | Grad Norm 5.6352(4.3616) | Total Time 10.00(10.00)
Iter 11680 | Time 5.5771(5.3568) | Bit/dim 1.1549(1.0465) | Steps 658(639.57) | Grad Norm 9.6664(6.0218) | Total Time 10.00(10.00)
Iter 11690 | Time 5.4254(5.3558) | Bit/dim 1.0952(1.0687) | Steps 646(639.75) | Grad Norm 5.0925(6.9168) | Total Time 10.00(10.00)
validating...
Epoch 0039 | Time 64.6678, Bit/dim 1.0622
===> Using batch size 200. Total 300 iterations/epoch.
Iter 11700 | Time 5.4293(5.3769) | Bit/dim 1.0571(1.0693) | Steps 652(641.25) | Grad Norm 2.6909(6.4131) | Total Time 10.00(10.00)
Iter 11710 | Time 5.3692(5.3666) | Bit/dim 1.0492(1.0631) | Steps 634(639.98) | Grad Norm 1.6839(5.3772) | Total Time 10.00(10.00)
Iter 11720 | Time 5.3978(5.3578) | Bit/dim 1.0382(1.0560) | Steps 640(639.19) | Grad Norm 0.9248(4.3288) | Total Time 10.00(10.00)
Iter 11730 | Time 5.2112(5.3386) | Bit/dim 1.0280(1.0472) | Steps 628(637.17) | Grad Norm 5.2783(3.8658) | Total Time 10.00(10.00)
Iter 11740 | Time 5.3211(5.3215) | Bit/dim 1.0485(1.0432) | Steps 640(636.97) | Grad Norm 6.9873(3.9396) | Total Time 10.00(10.00)
Iter 11750 | Time 5.5333(5.3541) | Bit/dim 1.0696(1.0450) | Steps 646(638.18) | Grad Norm 8.3253(5.7042) | Total Time 10.00(10.00)
Iter 11760 | Time 5.1493(5.3519) | Bit/dim 1.0984(1.0668) | Steps 652(639.57) | Grad Norm 2.2089(6.5116) | Total Time 10.00(10.00)
Iter 11770 | Time 5.2884(5.3479) | Bit/dim 1.0489(1.0685) | Steps 628(639.28) | Grad Norm 2.3702(6.0056) | Total Time 10.00(10.00)
Iter 11780 | Time 5.2373(5.3366) | Bit/dim 1.0725(1.0643) | Steps 622(639.25) | Grad Norm 1.0592(5.1183) | Total Time 10.00(10.00)
Iter 11790 | Time 5.1329(5.3135) | Bit/dim 1.0162(1.0557) | Steps 622(637.88) | Grad Norm 3.6901(4.6230) | Total Time 10.00(10.00)
Iter 11800 | Time 5.4313(5.3454) | Bit/dim 1.0791(1.0558) | Steps 634(638.08) | Grad Norm 10.3700(5.8754) | Total Time 10.00(10.00)
Iter 11810 | Time 5.4638(5.3679) | Bit/dim 1.1402(1.0602) | Steps 640(638.51) | Grad Norm 7.5501(7.1229) | Total Time 10.00(10.00)
Iter 11820 | Time 5.3963(5.3680) | Bit/dim 1.0472(1.0628) | Steps 646(638.60) | Grad Norm 3.9326(7.1892) | Total Time 10.00(10.00)
Iter 11830 | Time 5.1294(5.3625) | Bit/dim 1.0629(1.0597) | Steps 640(639.83) | Grad Norm 2.9099(6.1885) | Total Time 10.00(10.00)
Iter 11840 | Time 5.4005(5.3614) | Bit/dim 1.0517(1.0535) | Steps 640(639.73) | Grad Norm 1.3539(5.2405) | Total Time 10.00(10.00)
Iter 11850 | Time 5.5182(5.3618) | Bit/dim 1.0632(1.0513) | Steps 640(639.64) | Grad Norm 10.0307(5.4055) | Total Time 10.00(10.00)
Iter 11860 | Time 5.1493(5.3529) | Bit/dim 1.0507(1.0632) | Steps 622(638.85) | Grad Norm 5.0696(6.4925) | Total Time 10.00(10.00)
Iter 11870 | Time 5.2823(5.3682) | Bit/dim 1.0917(1.0671) | Steps 640(640.16) | Grad Norm 4.8883(6.6073) | Total Time 10.00(10.00)
Iter 11880 | Time 5.3329(5.3584) | Bit/dim 1.0328(1.0632) | Steps 652(640.65) | Grad Norm 4.1764(5.9180) | Total Time 10.00(10.00)
Iter 11890 | Time 5.2259(5.3616) | Bit/dim 1.0586(1.0582) | Steps 628(641.37) | Grad Norm 5.6290(5.5103) | Total Time 10.00(10.00)
Iter 11900 | Time 5.1643(5.3441) | Bit/dim 1.0315(1.0510) | Steps 622(639.98) | Grad Norm 2.7554(4.7879) | Total Time 10.00(10.00)
Iter 11910 | Time 5.4510(5.3557) | Bit/dim 1.1046(1.0502) | Steps 640(640.08) | Grad Norm 10.0367(5.6803) | Total Time 10.00(10.00)
Iter 11920 | Time 5.3043(5.3613) | Bit/dim 1.0724(1.0629) | Steps 646(640.98) | Grad Norm 3.6344(6.7456) | Total Time 10.00(10.00)
Iter 11930 | Time 5.2561(5.3561) | Bit/dim 1.0504(1.0662) | Steps 646(641.40) | Grad Norm 3.8641(6.5636) | Total Time 10.00(10.00)
Iter 11940 | Time 5.4472(5.3544) | Bit/dim 1.0320(1.0622) | Steps 646(641.86) | Grad Norm 1.6694(5.4281) | Total Time 10.00(10.00)
Iter 11950 | Time 5.1665(5.3619) | Bit/dim 0.9991(1.0554) | Steps 622(641.29) | Grad Norm 1.5906(4.3904) | Total Time 10.00(10.00)
Iter 11960 | Time 5.4892(5.3577) | Bit/dim 1.0346(1.0516) | Steps 646(640.93) | Grad Norm 0.9333(3.5614) | Total Time 10.00(10.00)
Iter 11970 | Time 5.2055(5.3496) | Bit/dim 1.0113(1.0452) | Steps 634(640.74) | Grad Norm 8.3988(3.7572) | Total Time 10.00(10.00)
Iter 11980 | Time 5.3141(5.3522) | Bit/dim 1.0803(1.0564) | Steps 640(640.38) | Grad Norm 7.7024(5.6750) | Total Time 10.00(10.00)
Iter 11990 | Time 5.2403(5.3651) | Bit/dim 1.0785(1.0640) | Steps 640(641.43) | Grad Norm 1.9406(6.0152) | Total Time 10.00(10.00)
validating...
Epoch 0040 | Time 65.3218, Bit/dim 1.0449
===> Using batch size 200. Total 300 iterations/epoch.
Iter 12000 | Time 5.4664(5.3725) | Bit/dim 1.0755(1.0634) | Steps 658(643.75) | Grad Norm 3.5494(5.9216) | Total Time 10.00(10.00)
Iter 12010 | Time 5.4865(5.3895) | Bit/dim 1.0276(1.0564) | Steps 652(645.53) | Grad Norm 2.7879(4.9452) | Total Time 10.00(10.00)
Iter 12020 | Time 5.6652(5.3732) | Bit/dim 1.0353(1.0506) | Steps 664(645.20) | Grad Norm 9.0555(4.6157) | Total Time 10.00(10.00)
Iter 12030 | Time 5.3905(5.3948) | Bit/dim 1.0831(1.0517) | Steps 652(645.69) | Grad Norm 7.1767(5.9673) | Total Time 10.00(10.00)
Iter 12040 | Time 5.4403(5.3930) | Bit/dim 1.0549(1.0561) | Steps 658(644.95) | Grad Norm 4.5602(6.1268) | Total Time 10.00(10.00)
Iter 12050 | Time 5.3267(5.3782) | Bit/dim 1.0709(1.0538) | Steps 652(644.30) | Grad Norm 2.8191(5.1742) | Total Time 10.00(10.00)
Iter 12060 | Time 5.4104(5.3723) | Bit/dim 1.0512(1.0487) | Steps 646(643.86) | Grad Norm 0.9698(4.2614) | Total Time 10.00(10.00)
Iter 12070 | Time 5.2698(5.3558) | Bit/dim 1.0439(1.0451) | Steps 640(642.69) | Grad Norm 1.0316(3.5952) | Total Time 10.00(10.00)
Iter 12080 | Time 5.3080(5.3613) | Bit/dim 1.0437(1.0404) | Steps 646(643.37) | Grad Norm 4.7189(3.9589) | Total Time 10.00(10.00)
Iter 12090 | Time 5.2344(5.3692) | Bit/dim 1.0540(1.0531) | Steps 640(642.77) | Grad Norm 4.8693(5.5598) | Total Time 10.00(10.00)
Iter 12100 | Time 5.6747(5.3774) | Bit/dim 1.0344(1.0533) | Steps 664(643.97) | Grad Norm 8.1628(5.5637) | Total Time 10.00(10.00)
Iter 12110 | Time 5.3592(5.3887) | Bit/dim 1.0134(1.0480) | Steps 652(646.36) | Grad Norm 3.4836(5.6522) | Total Time 10.00(10.00)
Iter 12120 | Time 5.4622(5.4150) | Bit/dim 1.0135(1.0415) | Steps 658(648.04) | Grad Norm 7.0289(6.2130) | Total Time 10.00(10.00)
Iter 12130 | Time 5.3688(5.4355) | Bit/dim 1.0512(1.0414) | Steps 640(649.93) | Grad Norm 7.4341(6.4141) | Total Time 10.00(10.00)
Iter 12140 | Time 5.2722(5.4381) | Bit/dim 1.0440(1.0393) | Steps 634(649.31) | Grad Norm 8.1604(6.6383) | Total Time 10.00(10.00)
Iter 12150 | Time 5.5245(5.4444) | Bit/dim 1.0748(1.0390) | Steps 658(650.01) | Grad Norm 9.7176(6.4923) | Total Time 10.00(10.00)
Iter 12160 | Time 5.2640(5.4042) | Bit/dim 1.0435(1.0358) | Steps 640(648.23) | Grad Norm 4.0665(5.6946) | Total Time 10.00(10.00)
Iter 12170 | Time 5.5668(5.4189) | Bit/dim 1.0846(1.0376) | Steps 652(649.36) | Grad Norm 9.6618(6.2479) | Total Time 10.00(10.00)
Iter 12180 | Time 5.0638(5.4039) | Bit/dim 1.0932(1.0532) | Steps 634(648.22) | Grad Norm 8.6920(7.3064) | Total Time 10.00(10.00)
Iter 12190 | Time 5.2130(5.3871) | Bit/dim 1.0534(1.0548) | Steps 640(648.88) | Grad Norm 2.3203(6.4402) | Total Time 10.00(10.00)
Iter 12200 | Time 5.4436(5.3977) | Bit/dim 1.0323(1.0503) | Steps 646(649.66) | Grad Norm 0.9228(5.3482) | Total Time 10.00(10.00)
Iter 12210 | Time 5.2681(5.3986) | Bit/dim 1.0220(1.0447) | Steps 634(648.29) | Grad Norm 2.2366(4.3371) | Total Time 10.00(10.00)
Iter 12220 | Time 5.3869(5.3821) | Bit/dim 0.9834(1.0387) | Steps 640(647.71) | Grad Norm 2.3073(3.9381) | Total Time 10.00(10.00)
Iter 12230 | Time 5.5932(5.3847) | Bit/dim 0.9985(1.0330) | Steps 652(647.79) | Grad Norm 4.3589(3.4326) | Total Time 10.00(10.00)
Iter 12240 | Time 5.4997(5.3797) | Bit/dim 1.1353(1.0494) | Steps 658(646.40) | Grad Norm 5.8768(5.2284) | Total Time 10.00(10.00)
Iter 12250 | Time 5.3366(5.3893) | Bit/dim 1.0511(1.0640) | Steps 634(646.29) | Grad Norm 3.6030(5.9501) | Total Time 10.00(10.00)
Iter 12260 | Time 5.2616(5.3815) | Bit/dim 1.0179(1.0644) | Steps 646(646.13) | Grad Norm 4.5395(5.4868) | Total Time 10.00(10.00)
Iter 12270 | Time 5.4864(5.4177) | Bit/dim 1.0189(1.0589) | Steps 664(649.54) | Grad Norm 6.3231(5.4164) | Total Time 10.00(10.00)
Iter 12280 | Time 5.3451(5.4082) | Bit/dim 1.0308(1.0513) | Steps 640(648.05) | Grad Norm 9.0818(5.6599) | Total Time 10.00(10.00)
Iter 12290 | Time 5.6098(5.4316) | Bit/dim 1.1459(1.0555) | Steps 664(648.92) | Grad Norm 8.9633(7.0778) | Total Time 10.00(10.00)
validating...
Epoch 0041 | Time 70.3724, Bit/dim 1.0607
===> Using batch size 200. Total 300 iterations/epoch.
Iter 12300 | Time 5.5873(5.4380) | Bit/dim 1.0498(1.0624) | Steps 646(649.85) | Grad Norm 7.4895(7.3842) | Total Time 10.00(10.00)
Iter 12310 | Time 5.3204(5.4297) | Bit/dim 1.0421(1.0611) | Steps 658(650.76) | Grad Norm 3.6918(6.7377) | Total Time 10.00(10.00)
Iter 12320 | Time 5.3990(5.4039) | Bit/dim 1.0278(1.0545) | Steps 646(649.47) | Grad Norm 2.1954(5.5464) | Total Time 10.00(10.00)
Iter 12330 | Time 5.3679(5.3898) | Bit/dim 1.0143(1.0496) | Steps 634(648.45) | Grad Norm 1.4304(4.8123) | Total Time 10.00(10.00)
Iter 12340 | Time 5.5720(5.3957) | Bit/dim 1.0340(1.0437) | Steps 646(647.18) | Grad Norm 10.2930(5.4284) | Total Time 10.00(10.00)
Iter 12350 | Time 5.4258(5.4151) | Bit/dim 1.0598(1.0404) | Steps 646(647.71) | Grad Norm 7.1639(6.1543) | Total Time 10.00(10.00)
Iter 12360 | Time 5.1685(5.4294) | Bit/dim 1.0877(1.0455) | Steps 628(648.77) | Grad Norm 7.4394(6.9244) | Total Time 10.00(10.00)
Iter 12370 | Time 5.5835(5.4247) | Bit/dim 1.0674(1.0520) | Steps 658(648.86) | Grad Norm 5.0807(7.3138) | Total Time 10.00(10.00)
Iter 12380 | Time 5.2409(5.4120) | Bit/dim 1.0486(1.0507) | Steps 640(649.49) | Grad Norm 1.6346(6.3491) | Total Time 10.00(10.00)
Iter 12390 | Time 5.4138(5.4116) | Bit/dim 1.0214(1.0463) | Steps 646(651.03) | Grad Norm 1.0108(5.2559) | Total Time 10.00(10.00)
Iter 12400 | Time 5.6378(5.4076) | Bit/dim 1.0263(1.0411) | Steps 658(650.76) | Grad Norm 5.6565(4.8209) | Total Time 10.00(10.00)
Iter 12410 | Time 5.2634(5.4045) | Bit/dim 1.0257(1.0415) | Steps 634(648.66) | Grad Norm 4.0186(5.6874) | Total Time 10.00(10.00)
Iter 12420 | Time 5.2114(5.4018) | Bit/dim 1.0745(1.0570) | Steps 640(649.47) | Grad Norm 2.9020(6.6537) | Total Time 10.00(10.00)
Iter 12430 | Time 5.6231(5.3942) | Bit/dim 1.0421(1.0575) | Steps 658(651.24) | Grad Norm 2.0523(6.0351) | Total Time 10.00(10.00)
Iter 12440 | Time 5.2955(5.3913) | Bit/dim 1.0540(1.0522) | Steps 652(652.94) | Grad Norm 3.9231(5.2055) | Total Time 10.00(10.00)
Iter 12450 | Time 5.1312(5.3688) | Bit/dim 0.9858(1.0470) | Steps 640(651.14) | Grad Norm 3.3895(5.0371) | Total Time 10.00(10.00)
Iter 12460 | Time 5.4734(5.3854) | Bit/dim 1.0076(1.0405) | Steps 652(650.93) | Grad Norm 5.4899(5.4682) | Total Time 10.00(10.00)
Iter 12470 | Time 5.3754(5.3924) | Bit/dim 1.0464(1.0362) | Steps 658(651.17) | Grad Norm 4.2306(5.6646) | Total Time 10.00(10.00)
Iter 12480 | Time 5.4632(5.4292) | Bit/dim 1.0158(1.0366) | Steps 652(652.13) | Grad Norm 7.4079(6.1539) | Total Time 10.00(10.00)
Iter 12490 | Time 5.3474(5.4410) | Bit/dim 1.0656(1.0375) | Steps 664(653.77) | Grad Norm 9.6675(6.9704) | Total Time 10.00(10.00)
Iter 12500 | Time 5.3316(5.4367) | Bit/dim 1.0559(1.0455) | Steps 652(653.90) | Grad Norm 3.3812(6.8969) | Total Time 10.00(10.00)
Iter 12510 | Time 5.5054(5.4338) | Bit/dim 1.0408(1.0446) | Steps 664(654.88) | Grad Norm 3.6859(6.0935) | Total Time 10.00(10.00)
Iter 12520 | Time 5.3110(5.4200) | Bit/dim 1.0409(1.0411) | Steps 646(654.64) | Grad Norm 2.5611(5.2371) | Total Time 10.00(10.00)
Iter 12530 | Time 5.6021(5.4171) | Bit/dim 1.0295(1.0386) | Steps 652(652.43) | Grad Norm 18.7863(5.7544) | Total Time 10.00(10.00)
Iter 12540 | Time 5.8487(5.4164) | Bit/dim 1.0830(1.0482) | Steps 670(651.17) | Grad Norm 11.5554(6.8489) | Total Time 10.00(10.00)
Iter 12550 | Time 5.3978(5.4159) | Bit/dim 1.0402(1.0506) | Steps 664(651.98) | Grad Norm 3.5205(6.4470) | Total Time 10.00(10.00)
Iter 12560 | Time 5.3924(5.4296) | Bit/dim 1.0428(1.0482) | Steps 652(654.84) | Grad Norm 4.6164(5.9017) | Total Time 10.00(10.00)
Iter 12570 | Time 5.5315(5.4188) | Bit/dim 1.0091(1.0452) | Steps 658(655.17) | Grad Norm 9.8726(5.8592) | Total Time 10.00(10.00)
Iter 12580 | Time 5.8668(5.4413) | Bit/dim 1.0463(1.0406) | Steps 670(656.00) | Grad Norm 16.3011(5.9650) | Total Time 10.00(10.00)
Iter 12590 | Time 5.5634(5.4343) | Bit/dim 1.0863(1.0481) | Steps 646(654.74) | Grad Norm 15.9603(7.0295) | Total Time 10.00(10.00)
validating...
Epoch 0042 | Time 65.7408, Bit/dim 1.0382
===> Using batch size 200. Total 300 iterations/epoch.
Iter 12600 | Time 5.4083(5.4379) | Bit/dim 1.0359(1.0484) | Steps 640(653.81) | Grad Norm 3.0690(6.1947) | Total Time 10.00(10.00)
Iter 12610 | Time 5.3603(5.4143) | Bit/dim 1.0583(1.0449) | Steps 658(654.65) | Grad Norm 3.6454(5.5018) | Total Time 10.00(10.00)
Iter 12620 | Time 5.5321(5.4179) | Bit/dim 1.0275(1.0416) | Steps 658(654.29) | Grad Norm 3.7984(4.7367) | Total Time 10.00(10.00)
Iter 12630 | Time 5.3126(5.4255) | Bit/dim 1.0643(1.0416) | Steps 646(653.73) | Grad Norm 8.8801(6.1866) | Total Time 10.00(10.00)
Iter 12640 | Time 5.4508(5.4373) | Bit/dim 1.0451(1.0461) | Steps 670(654.79) | Grad Norm 3.0397(6.6476) | Total Time 10.00(10.00)
Iter 12650 | Time 5.3181(5.4580) | Bit/dim 1.0249(1.0437) | Steps 658(657.56) | Grad Norm 3.9345(6.2187) | Total Time 10.00(10.00)
Iter 12660 | Time 5.3038(5.4321) | Bit/dim 1.0382(1.0410) | Steps 646(657.28) | Grad Norm 1.7557(5.2652) | Total Time 10.00(10.00)
Iter 12670 | Time 5.1889(5.4276) | Bit/dim 1.0415(1.0350) | Steps 646(656.47) | Grad Norm 2.9324(4.2669) | Total Time 10.00(10.00)
Iter 12680 | Time 5.3715(5.4282) | Bit/dim 1.0021(1.0319) | Steps 652(655.24) | Grad Norm 0.7359(3.9885) | Total Time 10.00(10.00)
Iter 12690 | Time 5.3547(5.4383) | Bit/dim 1.0399(1.0282) | Steps 652(655.28) | Grad Norm 8.2328(4.5055) | Total Time 10.00(10.00)
Iter 12700 | Time 5.2410(5.4304) | Bit/dim 1.1475(1.0473) | Steps 646(654.46) | Grad Norm 5.7854(6.4233) | Total Time 10.00(10.00)
Iter 12710 | Time 5.4705(5.4225) | Bit/dim 1.0747(1.0588) | Steps 658(654.63) | Grad Norm 4.4413(6.0552) | Total Time 10.00(10.00)
Iter 12720 | Time 5.2542(5.4423) | Bit/dim 1.0271(1.0558) | Steps 658(656.49) | Grad Norm 1.6073(5.4343) | Total Time 10.00(10.00)
Iter 12730 | Time 5.3957(5.4365) | Bit/dim 0.9863(1.0499) | Steps 658(657.90) | Grad Norm 1.2770(4.4609) | Total Time 10.00(10.00)
Iter 12740 | Time 5.4044(5.4139) | Bit/dim 1.0505(1.0431) | Steps 658(657.17) | Grad Norm 3.7204(4.0399) | Total Time 10.00(10.00)
Iter 12750 | Time 5.4878(5.4327) | Bit/dim 1.0020(1.0378) | Steps 652(658.02) | Grad Norm 1.7751(4.0042) | Total Time 10.00(10.00)
Iter 12760 | Time 5.4736(5.4665) | Bit/dim 1.1105(1.0382) | Steps 658(659.37) | Grad Norm 9.5521(5.0405) | Total Time 10.00(10.00)
Iter 12770 | Time 5.6942(5.4900) | Bit/dim 1.0780(1.0428) | Steps 670(660.10) | Grad Norm 19.4365(6.1896) | Total Time 10.00(10.00)
Iter 12780 | Time 5.5073(5.5038) | Bit/dim 1.0632(1.0449) | Steps 664(660.97) | Grad Norm 7.9027(7.1308) | Total Time 10.00(10.00)
Iter 12790 | Time 5.4795(5.5184) | Bit/dim 1.0747(1.0492) | Steps 676(662.66) | Grad Norm 2.0269(6.9497) | Total Time 10.00(10.00)
Iter 12800 | Time 5.5390(5.5194) | Bit/dim 1.0031(1.0467) | Steps 676(664.08) | Grad Norm 4.1304(6.5569) | Total Time 10.00(10.00)
Iter 12810 | Time 5.2716(5.5201) | Bit/dim 1.0305(1.0435) | Steps 658(665.43) | Grad Norm 4.4326(6.0895) | Total Time 10.00(10.00)
Iter 12820 | Time 5.5537(5.5084) | Bit/dim 1.0228(1.0381) | Steps 670(665.09) | Grad Norm 1.7149(5.7197) | Total Time 10.00(10.00)
Iter 12830 | Time 5.8613(5.5154) | Bit/dim 1.0534(1.0334) | Steps 676(665.76) | Grad Norm 13.5401(5.4568) | Total Time 10.00(10.00)
Iter 12840 | Time 5.3862(5.5135) | Bit/dim 1.0940(1.0413) | Steps 664(665.53) | Grad Norm 8.9948(6.9495) | Total Time 10.00(10.00)
Iter 12850 | Time 5.6620(5.5049) | Bit/dim 1.0306(1.0461) | Steps 664(664.91) | Grad Norm 5.0513(7.0819) | Total Time 10.00(10.00)
Iter 12860 | Time 5.5938(5.5308) | Bit/dim 1.0444(1.0462) | Steps 682(667.22) | Grad Norm 2.8106(6.6187) | Total Time 10.00(10.00)
Iter 12870 | Time 5.3681(5.5200) | Bit/dim 1.0484(1.0433) | Steps 658(667.52) | Grad Norm 1.6669(5.5583) | Total Time 10.00(10.00)
Iter 12880 | Time 5.2677(5.4745) | Bit/dim 1.0373(1.0394) | Steps 658(665.14) | Grad Norm 3.5644(4.7541) | Total Time 10.00(10.00)
Iter 12890 | Time 5.3962(5.4662) | Bit/dim 1.0395(1.0359) | Steps 652(662.76) | Grad Norm 3.6710(4.2338) | Total Time 10.00(10.00)
validating...
Epoch 0043 | Time 69.4304, Bit/dim 1.0409
===> Using batch size 200. Total 300 iterations/epoch.
Iter 12900 | Time 5.3914(5.4715) | Bit/dim 1.0581(1.0364) | Steps 658(661.38) | Grad Norm 7.0637(4.9235) | Total Time 10.00(10.00)
Iter 12910 | Time 5.4055(5.4599) | Bit/dim 1.0623(1.0435) | Steps 652(659.56) | Grad Norm 5.9021(6.1990) | Total Time 10.00(10.00)
Iter 12920 | Time 5.3943(5.4711) | Bit/dim 1.0359(1.0434) | Steps 670(660.77) | Grad Norm 3.5658(5.9905) | Total Time 10.00(10.00)
Iter 12930 | Time 5.2811(5.4770) | Bit/dim 1.0364(1.0411) | Steps 658(663.26) | Grad Norm 2.9309(5.5958) | Total Time 10.00(10.00)
Iter 12940 | Time 5.2941(5.4830) | Bit/dim 1.0110(1.0386) | Steps 664(663.56) | Grad Norm 3.0360(5.3999) | Total Time 10.00(10.00)
Iter 12950 | Time 5.4342(5.4790) | Bit/dim 1.0821(1.0389) | Steps 652(662.33) | Grad Norm 9.2256(5.9480) | Total Time 10.00(10.00)
Iter 12960 | Time 5.3014(5.4757) | Bit/dim 1.0714(1.0437) | Steps 652(660.74) | Grad Norm 6.1474(6.4473) | Total Time 10.00(10.00)
Iter 12970 | Time 5.4133(5.4833) | Bit/dim 0.9983(1.0382) | Steps 658(662.23) | Grad Norm 3.4827(6.1127) | Total Time 10.00(10.00)
Iter 12980 | Time 5.4460(5.4865) | Bit/dim 1.0226(1.0382) | Steps 664(664.22) | Grad Norm 2.1190(5.2961) | Total Time 10.00(10.00)
Iter 12990 | Time 5.4878(5.4998) | Bit/dim 1.0731(1.0354) | Steps 664(664.63) | Grad Norm 10.3847(5.6113) | Total Time 10.00(10.00)
Iter 13000 | Time 5.3454(5.4828) | Bit/dim 1.0949(1.0536) | Steps 658(664.39) | Grad Norm 4.7917(6.6055) | Total Time 10.00(10.00)
Iter 13010 | Time 5.7024(5.4926) | Bit/dim 1.0579(1.0580) | Steps 682(665.26) | Grad Norm 7.4623(6.2425) | Total Time 10.00(10.00)
Iter 13020 | Time 5.4932(5.4739) | Bit/dim 1.0106(1.0538) | Steps 670(666.26) | Grad Norm 1.5089(5.1260) | Total Time 10.00(10.00)
Iter 13030 | Time 5.2553(5.4622) | Bit/dim 1.0073(1.0447) | Steps 658(666.11) | Grad Norm 2.9067(4.2249) | Total Time 10.00(10.00)
Iter 13040 | Time 5.5784(5.4396) | Bit/dim 1.0402(1.0387) | Steps 670(664.07) | Grad Norm 8.8682(4.1967) | Total Time 10.00(10.00)
Iter 13050 | Time 5.0928(5.4136) | Bit/dim 1.0485(1.0502) | Steps 634(661.07) | Grad Norm 9.1124(5.6470) | Total Time 10.00(10.00)
Iter 13060 | Time 5.2558(5.4220) | Bit/dim 1.0037(1.0536) | Steps 652(660.57) | Grad Norm 2.6995(5.4949) | Total Time 10.00(10.00)
Iter 13070 | Time 5.4830(5.4241) | Bit/dim 1.0312(1.0495) | Steps 664(662.90) | Grad Norm 3.6567(4.9350) | Total Time 10.00(10.00)
Iter 13080 | Time 5.6493(5.4385) | Bit/dim 1.0445(1.0431) | Steps 682(665.52) | Grad Norm 5.3584(4.7141) | Total Time 10.00(10.00)
Iter 13090 | Time 5.5167(5.4705) | Bit/dim 1.0772(1.0428) | Steps 670(666.98) | Grad Norm 9.7831(6.1857) | Total Time 10.00(10.00)
Iter 13100 | Time 5.4012(5.4816) | Bit/dim 1.0571(1.0524) | Steps 670(666.99) | Grad Norm 2.8118(6.7466) | Total Time 10.00(10.00)
Iter 13110 | Time 5.5791(5.4739) | Bit/dim 1.0519(1.0528) | Steps 682(666.92) | Grad Norm 3.5110(6.2496) | Total Time 10.00(10.00)
Iter 13120 | Time 5.1856(5.4536) | Bit/dim 1.0424(1.0481) | Steps 646(665.57) | Grad Norm 2.3666(5.2703) | Total Time 10.00(10.00)
Iter 13130 | Time 5.3991(5.4509) | Bit/dim 1.0057(1.0434) | Steps 658(663.94) | Grad Norm 1.7863(4.2530) | Total Time 10.00(10.00)
Iter 13140 | Time 5.5235(5.4460) | Bit/dim 1.0333(1.0381) | Steps 658(661.25) | Grad Norm 1.1700(3.9018) | Total Time 10.00(10.00)
Iter 13150 | Time 5.5615(5.4630) | Bit/dim 1.0658(1.0419) | Steps 664(661.72) | Grad Norm 11.5255(5.4094) | Total Time 10.00(10.00)
Iter 13160 | Time 5.8762(5.4783) | Bit/dim 1.0576(1.0428) | Steps 676(661.77) | Grad Norm 9.7762(5.8451) | Total Time 10.00(10.00)
Iter 13170 | Time 5.5107(5.4845) | Bit/dim 1.0401(1.0428) | Steps 670(662.11) | Grad Norm 6.5294(6.2382) | Total Time 10.00(10.00)
Iter 13180 | Time 5.2607(5.4914) | Bit/dim 1.0105(1.0385) | Steps 658(663.02) | Grad Norm 1.7377(6.0397) | Total Time 10.00(10.00)
Iter 13190 | Time 5.5750(5.5235) | Bit/dim 1.0171(1.0396) | Steps 676(665.74) | Grad Norm 6.7873(6.1197) | Total Time 10.00(10.00)
validating...
Epoch 0044 | Time 67.0330, Bit/dim 1.0283
===> Using batch size 200. Total 300 iterations/epoch.
Iter 13200 | Time 5.5281(5.5304) | Bit/dim 1.0150(1.0384) | Steps 670(666.98) | Grad Norm 5.2820(6.3554) | Total Time 10.00(10.00)
Iter 13210 | Time 5.5291(5.5288) | Bit/dim 1.0359(1.0349) | Steps 664(667.08) | Grad Norm 5.4188(5.6983) | Total Time 10.00(10.00)
Iter 13220 | Time 5.8442(5.5232) | Bit/dim 1.0098(1.0312) | Steps 676(667.12) | Grad Norm 10.6122(5.3715) | Total Time 10.00(10.00)
Iter 13230 | Time 5.8750(5.5292) | Bit/dim 1.0251(1.0315) | Steps 676(667.20) | Grad Norm 12.1548(5.9568) | Total Time 10.00(10.00)
Iter 13240 | Time 5.4197(5.5198) | Bit/dim 1.0098(1.0272) | Steps 670(667.93) | Grad Norm 4.2410(6.2568) | Total Time 10.00(10.00)
Iter 13250 | Time 5.4182(5.5075) | Bit/dim 1.0624(1.0382) | Steps 676(666.90) | Grad Norm 5.3746(7.0105) | Total Time 10.00(10.00)
Iter 13260 | Time 5.4078(5.5030) | Bit/dim 1.0626(1.0389) | Steps 670(667.68) | Grad Norm 1.9376(6.1988) | Total Time 10.00(10.00)
Iter 13270 | Time 5.4362(5.4702) | Bit/dim 1.0206(1.0374) | Steps 670(667.98) | Grad Norm 1.0201(5.1817) | Total Time 10.00(10.00)
Iter 13280 | Time 5.4234(5.4433) | Bit/dim 1.0350(1.0334) | Steps 664(664.99) | Grad Norm 1.3180(4.2622) | Total Time 10.00(10.00)
Iter 13290 | Time 5.5984(5.4310) | Bit/dim 1.0221(1.0319) | Steps 676(663.84) | Grad Norm 8.0171(3.9999) | Total Time 10.00(10.00)
Iter 13300 | Time 5.4336(5.4470) | Bit/dim 1.0670(1.0415) | Steps 682(664.21) | Grad Norm 6.9228(5.7196) | Total Time 10.00(10.00)
Iter 13310 | Time 5.2622(5.4344) | Bit/dim 1.0207(1.0413) | Steps 658(663.76) | Grad Norm 1.9088(5.3693) | Total Time 10.00(10.00)
Iter 13320 | Time 5.4100(5.4536) | Bit/dim 1.0235(1.0381) | Steps 658(666.08) | Grad Norm 1.1400(4.5577) | Total Time 10.00(10.00)
Iter 13330 | Time 5.3070(5.4384) | Bit/dim 1.0595(1.0333) | Steps 664(665.36) | Grad Norm 3.6074(3.8365) | Total Time 10.00(10.00)
Iter 13340 | Time 5.5723(5.4463) | Bit/dim 1.0117(1.0274) | Steps 664(664.63) | Grad Norm 6.1064(3.9487) | Total Time 10.00(10.00)
Iter 13350 | Time 5.3959(5.4482) | Bit/dim 1.1463(1.0382) | Steps 652(663.95) | Grad Norm 6.7160(5.6854) | Total Time 10.00(10.00)
Iter 13360 | Time 5.4088(5.4488) | Bit/dim 1.0291(1.0525) | Steps 658(663.54) | Grad Norm 4.2058(5.7664) | Total Time 10.00(10.00)
Iter 13370 | Time 5.5950(5.4555) | Bit/dim 1.0106(1.0496) | Steps 676(663.33) | Grad Norm 1.6470(4.8763) | Total Time 10.00(10.00)
Iter 13380 | Time 5.5735(5.4587) | Bit/dim 1.0484(1.0476) | Steps 670(664.18) | Grad Norm 1.3250(4.1230) | Total Time 10.00(10.00)
Iter 13390 | Time 5.4160(5.4595) | Bit/dim 1.0166(1.0426) | Steps 664(664.31) | Grad Norm 2.6152(3.3920) | Total Time 10.00(10.00)
Iter 13400 | Time 5.4154(5.4677) | Bit/dim 0.9941(1.0363) | Steps 664(665.01) | Grad Norm 2.0767(3.8580) | Total Time 10.00(10.00)
Iter 13410 | Time 5.3721(5.4827) | Bit/dim 1.0419(1.0376) | Steps 658(666.12) | Grad Norm 3.8850(5.3654) | Total Time 10.00(10.00)
Iter 13420 | Time 5.5627(5.4932) | Bit/dim 1.1018(1.0700) | Steps 676(668.31) | Grad Norm 3.0054(6.2153) | Total Time 10.00(10.00)
Iter 13430 | Time 5.6086(5.5007) | Bit/dim 1.0486(1.0710) | Steps 676(668.64) | Grad Norm 5.2219(5.6330) | Total Time 10.00(10.00)
Iter 13440 | Time 5.2965(5.4830) | Bit/dim 1.0429(1.0640) | Steps 658(667.90) | Grad Norm 1.2214(4.7249) | Total Time 10.00(10.00)
Iter 13450 | Time 5.4244(5.4747) | Bit/dim 1.0387(1.0548) | Steps 658(667.00) | Grad Norm 1.4232(3.9100) | Total Time 10.00(10.00)
Iter 13460 | Time 5.5396(5.4711) | Bit/dim 1.0109(1.0448) | Steps 664(666.22) | Grad Norm 5.7544(3.4652) | Total Time 10.00(10.00)
Iter 13470 | Time 5.4872(5.5040) | Bit/dim 1.0947(1.0459) | Steps 664(666.84) | Grad Norm 9.4544(5.1573) | Total Time 10.00(10.00)
Iter 13480 | Time 5.5229(5.5235) | Bit/dim 1.0682(1.0627) | Steps 664(668.04) | Grad Norm 3.9724(6.1871) | Total Time 10.00(10.00)
Iter 13490 | Time 5.8222(5.5339) | Bit/dim 1.0586(1.0640) | Steps 676(666.59) | Grad Norm 2.2625(5.5422) | Total Time 10.00(10.00)
validating...
Epoch 0045 | Time 71.1073, Bit/dim 1.0283
===> Using batch size 200. Total 300 iterations/epoch.
Iter 13500 | Time 5.4400(5.5578) | Bit/dim 1.0224(1.0572) | Steps 676(669.47) | Grad Norm 1.1746(4.7720) | Total Time 10.00(10.00)
Iter 13510 | Time 5.4708(5.5271) | Bit/dim 0.9974(1.0483) | Steps 670(669.18) | Grad Norm 1.1954(3.8968) | Total Time 10.00(10.00)
Iter 13520 | Time 5.4159(5.4795) | Bit/dim 1.0349(1.0417) | Steps 664(667.54) | Grad Norm 0.9594(3.4081) | Total Time 10.00(10.00)
Iter 13530 | Time 5.3804(5.4774) | Bit/dim 1.0320(1.0335) | Steps 664(667.14) | Grad Norm 3.6293(2.9318) | Total Time 10.00(10.00)
Iter 13540 | Time 5.5939(5.4793) | Bit/dim 1.0382(1.0397) | Steps 676(666.92) | Grad Norm 8.6605(5.1199) | Total Time 10.00(10.00)
Iter 13550 | Time 5.5296(5.5016) | Bit/dim 1.0687(1.0478) | Steps 682(669.90) | Grad Norm 2.7482(5.6115) | Total Time 10.00(10.00)
Iter 13560 | Time 5.8133(5.5352) | Bit/dim 1.0531(1.0498) | Steps 688(671.91) | Grad Norm 2.4284(5.2200) | Total Time 10.00(10.00)
Iter 13570 | Time 5.4919(5.5288) | Bit/dim 1.0241(1.0446) | Steps 670(672.26) | Grad Norm 1.4781(4.3943) | Total Time 10.00(10.00)
Iter 13580 | Time 5.4700(5.4966) | Bit/dim 1.0427(1.0387) | Steps 670(670.73) | Grad Norm 0.9304(3.8368) | Total Time 10.00(10.00)
Iter 13590 | Time 5.5126(5.4829) | Bit/dim 1.0942(1.0356) | Steps 670(669.42) | Grad Norm 10.0075(4.8884) | Total Time 10.00(10.00)
Iter 13600 | Time 5.5592(5.5041) | Bit/dim 1.0607(1.0487) | Steps 694(670.70) | Grad Norm 4.0892(5.5207) | Total Time 10.00(10.00)
Iter 13610 | Time 5.1760(5.4964) | Bit/dim 1.0245(1.0457) | Steps 670(670.17) | Grad Norm 2.6005(5.4203) | Total Time 10.00(10.00)
Iter 13620 | Time 5.3036(5.4908) | Bit/dim 1.0418(1.0408) | Steps 652(669.78) | Grad Norm 4.8142(5.1055) | Total Time 10.00(10.00)
Iter 13630 | Time 5.5755(5.5015) | Bit/dim 1.0427(1.0366) | Steps 670(669.97) | Grad Norm 5.3605(4.9693) | Total Time 10.00(10.00)
Iter 13640 | Time 5.4638(5.4995) | Bit/dim 1.0180(1.0319) | Steps 676(670.50) | Grad Norm 2.0273(5.0468) | Total Time 10.00(10.00)
Iter 13650 | Time 5.4326(5.4889) | Bit/dim 1.0560(1.0356) | Steps 670(669.77) | Grad Norm 7.6017(5.7099) | Total Time 10.00(10.00)
Iter 13660 | Time 5.5614(5.5009) | Bit/dim 1.0818(1.0551) | Steps 676(671.38) | Grad Norm 2.7277(6.4074) | Total Time 10.00(10.00)
Iter 13670 | Time 5.6608(5.4912) | Bit/dim 1.0844(1.0606) | Steps 676(670.12) | Grad Norm 4.9463(5.7816) | Total Time 10.00(10.00)
Iter 13680 | Time 5.2961(5.4937) | Bit/dim 1.0478(1.0545) | Steps 658(671.74) | Grad Norm 1.4218(4.7738) | Total Time 10.00(10.00)
Iter 13690 | Time 5.3238(5.4655) | Bit/dim 1.0444(1.0460) | Steps 664(670.67) | Grad Norm 0.9510(3.8735) | Total Time 10.00(10.00)
Iter 13700 | Time 5.9143(5.4885) | Bit/dim 1.0165(1.0422) | Steps 688(670.86) | Grad Norm 9.8050(4.5667) | Total Time 10.00(10.00)
Iter 13710 | Time 5.4046(5.5029) | Bit/dim 1.0296(1.0404) | Steps 670(671.69) | Grad Norm 5.8708(5.0089) | Total Time 10.00(10.00)
Iter 13720 | Time 5.5299(5.5166) | Bit/dim 1.0764(1.0403) | Steps 670(672.04) | Grad Norm 8.5079(6.2424) | Total Time 10.00(10.00)
Iter 13730 | Time 5.6274(5.5290) | Bit/dim 1.0398(1.0478) | Steps 676(673.75) | Grad Norm 3.0253(6.5903) | Total Time 10.00(10.00)
Iter 13740 | Time 5.6065(5.5181) | Bit/dim 1.0623(1.0491) | Steps 706(673.92) | Grad Norm 3.2268(5.9461) | Total Time 10.00(10.00)
Iter 13750 | Time 5.8177(5.5425) | Bit/dim 0.9929(1.0417) | Steps 682(675.36) | Grad Norm 1.0258(5.0141) | Total Time 10.00(10.00)
Iter 13760 | Time 5.2953(5.4910) | Bit/dim 0.9937(1.0347) | Steps 664(673.11) | Grad Norm 1.0397(4.0706) | Total Time 10.00(10.00)
Iter 13770 | Time 5.3183(5.4773) | Bit/dim 1.0260(1.0300) | Steps 664(671.81) | Grad Norm 3.5316(3.4638) | Total Time 10.00(10.00)
Iter 13780 | Time 5.5936(5.4509) | Bit/dim 1.0373(1.0271) | Steps 670(669.98) | Grad Norm 5.8430(3.3946) | Total Time 10.00(10.00)
Iter 13790 | Time 5.4665(5.4621) | Bit/dim 1.1200(1.0283) | Steps 664(669.67) | Grad Norm 8.6404(4.6114) | Total Time 10.00(10.00)
validating...
Epoch 0046 | Time 70.6495, Bit/dim 1.0907
===> Using batch size 200. Total 300 iterations/epoch.
Iter 13800 | Time 5.6626(5.4950) | Bit/dim 1.1528(1.0501) | Steps 694(671.84) | Grad Norm 4.4506(5.6909) | Total Time 10.00(10.00)
Iter 13810 | Time 5.7394(5.4887) | Bit/dim 1.0683(1.0539) | Steps 676(670.43) | Grad Norm 5.3399(5.3655) | Total Time 10.00(10.00)
Iter 13820 | Time 5.8422(5.5567) | Bit/dim 1.0032(1.0476) | Steps 688(673.93) | Grad Norm 1.1093(4.3874) | Total Time 10.00(10.00)
Iter 13830 | Time 5.3034(5.5231) | Bit/dim 1.0215(1.0392) | Steps 664(673.28) | Grad Norm 3.7801(3.8237) | Total Time 10.00(10.00)
Iter 13840 | Time 5.6349(5.5417) | Bit/dim 1.1118(1.0379) | Steps 688(673.70) | Grad Norm 8.7309(4.8518) | Total Time 10.00(10.00)
Iter 13850 | Time 5.6291(5.5467) | Bit/dim 1.0159(1.0398) | Steps 682(675.14) | Grad Norm 4.1635(5.1110) | Total Time 10.00(10.00)
Iter 13860 | Time 6.0003(5.5597) | Bit/dim 1.0323(1.0388) | Steps 694(675.75) | Grad Norm 10.3069(5.2961) | Total Time 10.00(10.00)
Iter 13870 | Time 5.9782(5.5482) | Bit/dim 1.0181(1.0325) | Steps 688(675.03) | Grad Norm 10.2913(5.0607) | Total Time 10.00(10.00)
Iter 13880 | Time 5.2294(5.5366) | Bit/dim 1.0385(1.0295) | Steps 652(674.46) | Grad Norm 3.6177(5.1193) | Total Time 10.00(10.00)
Iter 13890 | Time 5.2703(5.5019) | Bit/dim 1.0181(1.0257) | Steps 658(671.06) | Grad Norm 4.1601(4.5475) | Total Time 10.00(10.00)
Iter 13900 | Time 5.5420(5.4955) | Bit/dim 1.1142(1.0298) | Steps 676(670.39) | Grad Norm 8.8588(5.2783) | Total Time 10.00(10.00)
Iter 13910 | Time 5.3688(5.5052) | Bit/dim 1.0630(1.0448) | Steps 688(672.38) | Grad Norm 3.1891(6.0072) | Total Time 10.00(10.00)
Iter 13920 | Time 5.5538(5.4901) | Bit/dim 1.0425(1.0467) | Steps 694(672.47) | Grad Norm 2.1555(5.5627) | Total Time 10.00(10.00)
Iter 13930 | Time 5.4480(5.5407) | Bit/dim 1.0291(1.0426) | Steps 670(674.79) | Grad Norm 2.5821(4.7384) | Total Time 10.00(10.00)
Iter 13940 | Time 5.6053(5.5405) | Bit/dim 1.0294(1.0357) | Steps 688(674.68) | Grad Norm 7.5910(5.1947) | Total Time 10.00(10.00)
Iter 13950 | Time 5.8532(5.5664) | Bit/dim 1.0307(1.0345) | Steps 676(674.87) | Grad Norm 6.3918(5.2872) | Total Time 10.00(10.00)
Iter 13960 | Time 5.7737(5.5880) | Bit/dim 1.0277(1.0324) | Steps 682(675.70) | Grad Norm 2.4354(5.7488) | Total Time 10.00(10.00)
Iter 13970 | Time 5.4436(5.5475) | Bit/dim 1.0842(1.0427) | Steps 664(674.95) | Grad Norm 5.3962(6.5232) | Total Time 10.00(10.00)
Iter 13980 | Time 5.5153(5.5353) | Bit/dim 1.0484(1.0402) | Steps 676(674.16) | Grad Norm 2.2214(5.8972) | Total Time 10.00(10.00)
Iter 13990 | Time 5.2833(5.5203) | Bit/dim 1.0162(1.0375) | Steps 670(675.12) | Grad Norm 1.6892(4.9399) | Total Time 10.00(10.00)
Iter 14000 | Time 5.8856(5.5495) | Bit/dim 1.0238(1.0346) | Steps 682(675.87) | Grad Norm 8.2070(5.1100) | Total Time 10.00(10.00)
Iter 14010 | Time 5.4290(5.5528) | Bit/dim 1.0576(1.0301) | Steps 664(675.43) | Grad Norm 4.6882(5.4713) | Total Time 10.00(10.00)
Iter 14020 | Time 5.3856(5.5445) | Bit/dim 1.0718(1.0542) | Steps 664(675.84) | Grad Norm 4.6829(6.2020) | Total Time 10.00(10.00)
Iter 14030 | Time 5.2773(5.5320) | Bit/dim 1.0644(1.0595) | Steps 664(674.94) | Grad Norm 1.7716(5.5010) | Total Time 10.00(10.00)
Iter 14040 | Time 5.3767(5.5043) | Bit/dim 1.0372(1.0521) | Steps 676(674.35) | Grad Norm 0.9459(4.5027) | Total Time 10.00(10.00)
Iter 14050 | Time 5.4603(5.5046) | Bit/dim 1.0407(1.0436) | Steps 676(674.18) | Grad Norm 3.3800(3.7514) | Total Time 10.00(10.00)
Iter 14060 | Time 5.6824(5.5244) | Bit/dim 1.0258(1.0374) | Steps 688(673.94) | Grad Norm 9.6298(4.5796) | Total Time 10.00(10.00)
Iter 14070 | Time 5.5488(5.5343) | Bit/dim 1.0522(1.0569) | Steps 676(675.24) | Grad Norm 4.8636(5.7454) | Total Time 10.00(10.00)
Iter 14080 | Time 5.2705(5.5127) | Bit/dim 1.0610(1.0596) | Steps 652(674.15) | Grad Norm 2.6428(5.3929) | Total Time 10.00(10.00)
Iter 14090 | Time 5.3556(5.5234) | Bit/dim 1.0203(1.0528) | Steps 676(676.80) | Grad Norm 1.1788(4.5258) | Total Time 10.00(10.00)
validating...
Epoch 0047 | Time 70.4045, Bit/dim 1.0144
===> Using batch size 200. Total 300 iterations/epoch.
Iter 14100 | Time 5.4452(5.5039) | Bit/dim 0.9975(1.0425) | Steps 670(675.57) | Grad Norm 1.7248(3.7413) | Total Time 10.00(10.00)
Iter 14110 | Time 5.4166(5.4828) | Bit/dim 1.0238(1.0382) | Steps 670(673.97) | Grad Norm 1.0295(3.2217) | Total Time 10.00(10.00)
Iter 14120 | Time 5.4000(5.4500) | Bit/dim 1.0213(1.0329) | Steps 658(671.42) | Grad Norm 5.1913(3.2065) | Total Time 10.00(10.00)
Iter 14130 | Time 5.5489(5.4761) | Bit/dim 1.1556(1.0414) | Steps 682(672.65) | Grad Norm 6.3303(4.8987) | Total Time 10.00(10.00)
Iter 14140 | Time 5.9163(5.4889) | Bit/dim 1.0384(1.0490) | Steps 688(674.80) | Grad Norm 2.9569(4.9497) | Total Time 10.00(10.00)
Iter 14150 | Time 5.1959(5.4497) | Bit/dim 1.0051(1.0440) | Steps 670(673.28) | Grad Norm 1.8226(4.1266) | Total Time 10.00(10.00)
Iter 14160 | Time 5.4306(5.4592) | Bit/dim 1.0217(1.0379) | Steps 670(673.62) | Grad Norm 2.0683(3.4853) | Total Time 10.00(10.00)
Iter 14170 | Time 5.4963(5.4446) | Bit/dim 1.0186(1.0337) | Steps 676(672.56) | Grad Norm 4.2414(3.1466) | Total Time 10.00(10.00)
Iter 14180 | Time 5.5927(5.4322) | Bit/dim 1.0388(1.0326) | Steps 676(672.02) | Grad Norm 3.3241(3.0907) | Total Time 10.00(10.00)
Iter 14190 | Time 5.5766(5.4619) | Bit/dim 1.1911(1.0534) | Steps 706(674.36) | Grad Norm 4.3780(4.8982) | Total Time 10.00(10.00)
Iter 14200 | Time 5.5581(5.4559) | Bit/dim 1.0926(1.0667) | Steps 688(674.93) | Grad Norm 2.9410(4.7110) | Total Time 10.00(10.00)
Iter 14210 | Time 5.2932(5.4804) | Bit/dim 1.0380(1.0618) | Steps 664(675.12) | Grad Norm 1.5238(4.0674) | Total Time 10.00(10.00)
Iter 14220 | Time 5.5996(5.4869) | Bit/dim 1.0027(1.0531) | Steps 682(675.15) | Grad Norm 2.3471(3.5047) | Total Time 10.00(10.00)
Iter 14230 | Time 5.4036(5.4768) | Bit/dim 1.0141(1.0415) | Steps 670(673.13) | Grad Norm 4.5977(3.0614) | Total Time 10.00(10.00)
Iter 14240 | Time 5.6496(5.4867) | Bit/dim 1.0433(1.0364) | Steps 688(672.61) | Grad Norm 7.9706(3.4276) | Total Time 10.00(10.00)
Iter 14250 | Time 5.4709(5.4846) | Bit/dim 1.0866(1.0486) | Steps 676(672.49) | Grad Norm 8.2608(5.3903) | Total Time 10.00(10.00)
Iter 14260 | Time 5.4461(5.5116) | Bit/dim 1.0377(1.0521) | Steps 676(675.59) | Grad Norm 3.0870(5.8303) | Total Time 10.00(10.00)
Iter 14270 | Time 5.5156(5.5376) | Bit/dim 1.0088(1.0477) | Steps 682(678.75) | Grad Norm 2.6690(5.1681) | Total Time 10.00(10.00)
Iter 14280 | Time 5.2907(5.5194) | Bit/dim 1.0206(1.0414) | Steps 664(678.75) | Grad Norm 2.2424(4.3839) | Total Time 10.00(10.00)
Iter 14290 | Time 5.3253(5.4886) | Bit/dim 0.9991(1.0358) | Steps 664(675.80) | Grad Norm 1.1795(3.9916) | Total Time 10.00(10.00)
Iter 14300 | Time 5.7106(5.5069) | Bit/dim 1.0534(1.0379) | Steps 670(675.92) | Grad Norm 23.5441(5.7941) | Total Time 10.00(10.00)
Iter 14310 | Time 5.5412(5.4988) | Bit/dim 1.0641(1.0450) | Steps 682(674.44) | Grad Norm 4.2896(6.0606) | Total Time 10.00(10.00)
Iter 14320 | Time 5.3529(5.5014) | Bit/dim 1.0094(1.0419) | Steps 676(676.83) | Grad Norm 1.6688(5.3145) | Total Time 10.00(10.00)
Iter 14330 | Time 5.1753(5.4999) | Bit/dim 1.0119(1.0367) | Steps 664(677.66) | Grad Norm 2.6318(4.4443) | Total Time 10.00(10.00)
Iter 14340 | Time 5.7004(5.4920) | Bit/dim 1.0186(1.0306) | Steps 694(677.54) | Grad Norm 4.6025(4.2948) | Total Time 10.00(10.00)
Iter 14350 | Time 5.4141(5.5178) | Bit/dim 1.0116(1.0275) | Steps 676(678.07) | Grad Norm 7.2275(5.0191) | Total Time 10.00(10.00)
Iter 14360 | Time 5.5227(5.5248) | Bit/dim 1.0979(1.0360) | Steps 694(678.39) | Grad Norm 5.8216(6.1078) | Total Time 10.00(10.00)
Iter 14370 | Time 5.7590(5.5156) | Bit/dim 1.0626(1.0398) | Steps 676(678.28) | Grad Norm 3.0098(5.7425) | Total Time 10.00(10.00)
Iter 14380 | Time 5.3214(5.5210) | Bit/dim 1.0182(1.0360) | Steps 670(679.43) | Grad Norm 2.4735(4.8214) | Total Time 10.00(10.00)
Iter 14390 | Time 5.3328(5.5275) | Bit/dim 1.0345(1.0295) | Steps 676(681.00) | Grad Norm 2.6721(4.2844) | Total Time 10.00(10.00)
validating...
Epoch 0048 | Time 71.5852, Bit/dim 1.0168
===> Using batch size 200. Total 300 iterations/epoch.
Iter 14400 | Time 5.6161(5.5456) | Bit/dim 1.0199(1.0287) | Steps 682(681.08) | Grad Norm 3.4346(4.7550) | Total Time 10.00(10.00)
Iter 14410 | Time 5.5323(5.5651) | Bit/dim 1.0253(1.0351) | Steps 694(682.11) | Grad Norm 6.0102(6.0070) | Total Time 10.00(10.00)
Iter 14420 | Time 5.4378(5.5452) | Bit/dim 1.0254(1.0407) | Steps 688(681.54) | Grad Norm 3.2886(6.3388) | Total Time 10.00(10.00)
Iter 14430 | Time 5.3732(5.5696) | Bit/dim 1.0266(1.0398) | Steps 676(684.27) | Grad Norm 2.1391(5.3815) | Total Time 10.00(10.00)
Iter 14440 | Time 5.8591(5.5849) | Bit/dim 1.0266(1.0342) | Steps 694(685.22) | Grad Norm 2.4309(4.5320) | Total Time 10.00(10.00)
Iter 14450 | Time 5.8584(5.5854) | Bit/dim 1.0381(1.0326) | Steps 682(684.84) | Grad Norm 4.4008(3.9363) | Total Time 10.00(10.00)
Iter 14460 | Time 5.8826(5.6051) | Bit/dim 1.0193(1.0297) | Steps 682(683.71) | Grad Norm 7.5645(4.4598) | Total Time 10.00(10.00)
Iter 14470 | Time 5.5858(5.6073) | Bit/dim 1.0500(1.0297) | Steps 676(682.70) | Grad Norm 3.0612(4.8676) | Total Time 10.00(10.00)
Iter 14480 | Time 5.6142(5.5946) | Bit/dim 1.1251(1.0424) | Steps 700(683.11) | Grad Norm 5.5483(6.4553) | Total Time 10.00(10.00)
Iter 14490 | Time 5.3948(5.5798) | Bit/dim 1.0643(1.0519) | Steps 664(684.01) | Grad Norm 3.6544(5.9171) | Total Time 10.00(10.00)
Iter 14500 | Time 5.7009(5.5910) | Bit/dim 1.0519(1.0498) | Steps 694(685.17) | Grad Norm 1.6291(5.1910) | Total Time 10.00(10.00)
Iter 14510 | Time 5.3888(5.5707) | Bit/dim 1.0224(1.0429) | Steps 682(684.12) | Grad Norm 1.6124(4.3586) | Total Time 10.00(10.00)
Iter 14520 | Time 5.5873(5.5265) | Bit/dim 1.0095(1.0366) | Steps 670(679.52) | Grad Norm 5.6253(3.9998) | Total Time 10.00(10.00)
Iter 14530 | Time 5.4840(5.5463) | Bit/dim 1.0162(1.0354) | Steps 682(680.72) | Grad Norm 5.3233(5.1718) | Total Time 10.00(10.00)
Iter 14540 | Time 5.6513(5.5551) | Bit/dim 1.0371(1.0350) | Steps 688(680.63) | Grad Norm 1.6983(5.5392) | Total Time 10.00(10.00)
Iter 14550 | Time 5.3607(5.5360) | Bit/dim 1.0117(1.0326) | Steps 676(680.75) | Grad Norm 1.4548(5.1418) | Total Time 10.00(10.00)
Iter 14560 | Time 6.0008(5.5208) | Bit/dim 1.0045(1.0298) | Steps 694(680.34) | Grad Norm 7.6904(4.8248) | Total Time 10.00(10.00)
Iter 14570 | Time 5.5244(5.5448) | Bit/dim 1.0096(1.0260) | Steps 688(681.59) | Grad Norm 5.7792(4.9952) | Total Time 10.00(10.00)
Iter 14580 | Time 5.5219(5.5395) | Bit/dim 1.1261(1.0373) | Steps 688(681.57) | Grad Norm 5.6510(6.2942) | Total Time 10.00(10.00)
Iter 14590 | Time 5.6697(5.5421) | Bit/dim 1.0545(1.0450) | Steps 688(681.87) | Grad Norm 2.9544(5.9101) | Total Time 10.00(10.00)
Iter 14600 | Time 5.9083(5.5576) | Bit/dim 1.0221(1.0425) | Steps 712(685.30) | Grad Norm 3.4498(4.9942) | Total Time 10.00(10.00)
Iter 14610 | Time 5.2049(5.5568) | Bit/dim 1.0121(1.0341) | Steps 676(687.18) | Grad Norm 2.7370(4.2705) | Total Time 10.00(10.00)
Iter 14620 | Time 5.8453(5.5176) | Bit/dim 1.0251(1.0291) | Steps 694(684.47) | Grad Norm 1.5315(3.6467) | Total Time 10.00(10.00)
Iter 14630 | Time 5.8177(5.5571) | Bit/dim 1.0178(1.0255) | Steps 688(684.50) | Grad Norm 14.9845(4.4088) | Total Time 10.00(10.00)
Iter 14640 | Time 5.5048(5.5577) | Bit/dim 1.0097(1.0236) | Steps 688(683.97) | Grad Norm 4.7124(4.6804) | Total Time 10.00(10.00)
Iter 14650 | Time 5.5901(5.5628) | Bit/dim 1.0824(1.0257) | Steps 676(683.62) | Grad Norm 32.1586(6.2548) | Total Time 10.00(10.00)
Iter 14660 | Time 5.6057(5.5605) | Bit/dim 1.0581(1.0434) | Steps 688(685.08) | Grad Norm 3.2748(6.1800) | Total Time 10.00(10.00)
Iter 14670 | Time 5.8678(5.6148) | Bit/dim 1.0500(1.0448) | Steps 700(689.38) | Grad Norm 1.4159(5.2414) | Total Time 10.00(10.00)
Iter 14680 | Time 5.8467(5.6623) | Bit/dim 1.0185(1.0383) | Steps 694(692.05) | Grad Norm 1.7784(4.2781) | Total Time 10.00(10.00)
Iter 14690 | Time 5.8573(5.6303) | Bit/dim 0.9994(1.0326) | Steps 700(690.61) | Grad Norm 2.5631(3.6376) | Total Time 10.00(10.00)
validating...
Epoch 0049 | Time 71.7688, Bit/dim 1.0082
===> Using batch size 200. Total 300 iterations/epoch.
Iter 14700 | Time 5.3234(5.6095) | Bit/dim 1.0118(1.0262) | Steps 676(688.74) | Grad Norm 2.1690(3.0586) | Total Time 10.00(10.00)
Iter 14710 | Time 5.3491(5.5856) | Bit/dim 1.0197(1.0220) | Steps 670(685.53) | Grad Norm 2.1833(3.0946) | Total Time 10.00(10.00)
Iter 14720 | Time 5.6451(5.6006) | Bit/dim 1.1042(1.0290) | Steps 694(685.33) | Grad Norm 8.7989(5.3087) | Total Time 10.00(10.00)
Iter 14730 | Time 5.7185(5.6090) | Bit/dim 1.0925(1.0590) | Steps 706(690.10) | Grad Norm 2.4316(4.9228) | Total Time 10.00(10.00)
Iter 14740 | Time 5.8698(5.6500) | Bit/dim 1.0325(1.0632) | Steps 700(691.58) | Grad Norm 1.5217(4.2403) | Total Time 10.00(10.00)
Iter 14750 | Time 5.5358(5.6510) | Bit/dim 1.0548(1.0565) | Steps 694(692.20) | Grad Norm 1.6368(3.6458) | Total Time 10.00(10.00)
Iter 14760 | Time 5.3672(5.6474) | Bit/dim 1.0143(1.0462) | Steps 676(691.93) | Grad Norm 4.6577(3.5051) | Total Time 10.00(10.00)
Iter 14770 | Time 5.3626(5.6289) | Bit/dim 1.0033(1.0377) | Steps 676(689.67) | Grad Norm 5.9435(4.1734) | Total Time 10.00(10.00)
Iter 14780 | Time 5.3567(5.6160) | Bit/dim 0.9964(1.0338) | Steps 676(687.46) | Grad Norm 4.0174(4.8818) | Total Time 10.00(10.00)
Iter 14790 | Time 5.6653(5.5742) | Bit/dim 1.0027(1.0282) | Steps 688(684.49) | Grad Norm 2.4947(4.3734) | Total Time 10.00(10.00)
Iter 14800 | Time 5.3216(5.5862) | Bit/dim 1.0003(1.0222) | Steps 676(685.31) | Grad Norm 3.7463(3.9045) | Total Time 10.00(10.00)
Iter 14810 | Time 5.7019(5.5766) | Bit/dim 1.0203(1.0191) | Steps 694(683.40) | Grad Norm 0.9404(4.0402) | Total Time 10.00(10.00)
Iter 14820 | Time 5.9294(5.5789) | Bit/dim 1.0553(1.0267) | Steps 694(683.44) | Grad Norm 15.7227(5.5780) | Total Time 10.00(10.00)
Iter 14830 | Time 5.3977(5.5617) | Bit/dim 1.0241(1.0286) | Steps 676(682.82) | Grad Norm 5.8520(5.5370) | Total Time 10.00(10.00)
Iter 14840 | Time 5.5793(5.5856) | Bit/dim 1.0482(1.0333) | Steps 706(686.76) | Grad Norm 6.7660(6.2673) | Total Time 10.00(10.00)
Iter 14850 | Time 5.6701(5.6085) | Bit/dim 1.0587(1.0423) | Steps 688(689.18) | Grad Norm 5.7947(6.4750) | Total Time 10.00(10.00)
Iter 14860 | Time 5.9686(5.6000) | Bit/dim 1.0172(1.0405) | Steps 700(689.50) | Grad Norm 4.1830(5.5762) | Total Time 10.00(10.00)
Iter 14870 | Time 5.8327(5.6285) | Bit/dim 0.9989(1.0336) | Steps 694(691.09) | Grad Norm 1.7565(4.7970) | Total Time 10.00(10.00)
Iter 14880 | Time 5.7559(5.6376) | Bit/dim 1.0067(1.0269) | Steps 676(689.60) | Grad Norm 16.0656(4.7928) | Total Time 10.00(10.00)
Iter 14890 | Time 5.6464(5.5991) | Bit/dim 1.0695(1.0385) | Steps 682(688.29) | Grad Norm 3.1872(5.8393) | Total Time 10.00(10.00)
Iter 14900 | Time 5.7868(5.6107) | Bit/dim 1.0455(1.0415) | Steps 700(689.91) | Grad Norm 3.5118(5.6503) | Total Time 10.00(10.00)
Iter 14910 | Time 5.7654(5.6267) | Bit/dim 1.0240(1.0384) | Steps 712(693.18) | Grad Norm 2.6563(5.1405) | Total Time 10.00(10.00)
Iter 14920 | Time 5.9001(5.6387) | Bit/dim 1.0066(1.0336) | Steps 712(694.22) | Grad Norm 1.6065(4.3863) | Total Time 10.00(10.00)
Iter 14930 | Time 5.6977(5.6709) | Bit/dim 0.9834(1.0278) | Steps 694(694.36) | Grad Norm 1.7227(3.5132) | Total Time 10.00(10.00)
Iter 14940 | Time 5.3322(5.6865) | Bit/dim 1.0307(1.0251) | Steps 682(694.72) | Grad Norm 3.1482(3.3340) | Total Time 10.00(10.00)
Iter 14950 | Time 5.5460(5.6810) | Bit/dim 1.1731(1.0371) | Steps 682(694.65) | Grad Norm 5.5210(5.0024) | Total Time 10.00(10.00)
Iter 14960 | Time 5.5464(5.6464) | Bit/dim 1.0762(1.0550) | Steps 706(694.15) | Grad Norm 2.4630(4.6774) | Total Time 10.00(10.00)
Iter 14970 | Time 5.5070(5.6305) | Bit/dim 1.0235(1.0521) | Steps 688(692.61) | Grad Norm 3.6909(4.1757) | Total Time 10.00(10.00)
Iter 14980 | Time 5.4336(5.6272) | Bit/dim 1.0539(1.0477) | Steps 682(692.02) | Grad Norm 8.6522(5.0763) | Total Time 10.00(10.00)
Iter 14990 | Time 5.5337(5.6033) | Bit/dim 1.0639(1.0515) | Steps 700(690.84) | Grad Norm 2.9758(5.8063) | Total Time 10.00(10.00)
validating...
Epoch 0050 | Time 73.7067, Bit/dim 1.0240
===> Using batch size 200. Total 300 iterations/epoch.
Iter 15000 | Time 5.8874(5.5994) | Bit/dim 1.0632(1.0486) | Steps 706(691.15) | Grad Norm 3.0479(5.1369) | Total Time 10.00(10.00)
Iter 15010 | Time 5.4080(5.6114) | Bit/dim 1.0156(1.0403) | Steps 688(692.75) | Grad Norm 2.4311(4.5418) | Total Time 10.00(10.00)
Iter 15020 | Time 5.5239(5.5810) | Bit/dim 1.0358(1.0346) | Steps 688(691.09) | Grad Norm 1.5308(3.8957) | Total Time 10.00(10.00)
Iter 15030 | Time 5.3800(5.5421) | Bit/dim 0.9664(1.0291) | Steps 688(688.59) | Grad Norm 2.1722(3.4201) | Total Time 10.00(10.00)
Iter 15040 | Time 5.4475(5.5783) | Bit/dim 1.0140(1.0260) | Steps 688(689.69) | Grad Norm 4.8972(3.6335) | Total Time 10.00(10.00)
Iter 15050 | Time 5.8000(5.5989) | Bit/dim 1.0413(1.0318) | Steps 688(690.58) | Grad Norm 9.6664(4.8690) | Total Time 10.00(10.00)
Iter 15060 | Time 5.3186(5.6016) | Bit/dim 1.0250(1.0388) | Steps 688(692.30) | Grad Norm 4.0871(5.8477) | Total Time 10.00(10.00)
Iter 15070 | Time 5.6300(5.6150) | Bit/dim 1.0368(1.0370) | Steps 682(692.02) | Grad Norm 2.0053(5.3423) | Total Time 10.00(10.00)
Iter 15080 | Time 5.7079(5.6480) | Bit/dim 0.9862(1.0304) | Steps 694(693.78) | Grad Norm 3.7296(4.7634) | Total Time 10.00(10.00)
Iter 15090 | Time 5.9410(5.6924) | Bit/dim 1.0038(1.0261) | Steps 712(697.30) | Grad Norm 0.9516(3.8796) | Total Time 10.00(10.00)
Iter 15100 | Time 5.5843(5.7008) | Bit/dim 1.0595(1.0229) | Steps 688(697.21) | Grad Norm 9.2428(4.6050) | Total Time 10.00(10.00)
Iter 15110 | Time 5.4693(5.6598) | Bit/dim 1.1196(1.0406) | Steps 694(696.84) | Grad Norm 5.1020(5.7045) | Total Time 10.00(10.00)
Iter 15120 | Time 5.7037(5.6479) | Bit/dim 1.0282(1.0465) | Steps 700(697.30) | Grad Norm 2.2313(5.3396) | Total Time 10.00(10.00)
Iter 15130 | Time 5.6376(5.6803) | Bit/dim 1.0477(1.0427) | Steps 718(698.40) | Grad Norm 3.6795(4.7162) | Total Time 10.00(10.00)
Iter 15140 | Time 5.5946(5.6884) | Bit/dim 0.9885(1.0349) | Steps 694(697.20) | Grad Norm 2.8480(4.4138) | Total Time 10.00(10.00)
Iter 15150 | Time 5.7662(5.6705) | Bit/dim 1.0090(1.0281) | Steps 682(695.09) | Grad Norm 16.6778(4.4944) | Total Time 10.00(10.00)
Iter 15160 | Time 5.5515(5.6462) | Bit/dim 1.0245(1.0307) | Steps 694(695.04) | Grad Norm 5.2157(4.9599) | Total Time 10.00(10.00)
Iter 15170 | Time 5.6448(5.6383) | Bit/dim 1.0568(1.0374) | Steps 718(695.19) | Grad Norm 5.7464(6.0364) | Total Time 10.00(10.00)
Iter 15180 | Time 5.5423(5.6264) | Bit/dim 1.0331(1.0429) | Steps 688(695.70) | Grad Norm 6.0414(5.8922) | Total Time 10.00(10.00)
Iter 15190 | Time 5.8295(5.6597) | Bit/dim 1.0176(1.0385) | Steps 724(698.95) | Grad Norm 1.4876(5.1089) | Total Time 10.00(10.00)
Iter 15200 | Time 5.6918(5.6618) | Bit/dim 1.0167(1.0342) | Steps 688(699.54) | Grad Norm 5.2183(4.6631) | Total Time 10.00(10.00)
Iter 15210 | Time 5.7257(5.6786) | Bit/dim 1.0003(1.0254) | Steps 706(699.60) | Grad Norm 1.2342(3.9652) | Total Time 10.00(10.00)
Iter 15220 | Time 5.8696(5.6633) | Bit/dim 1.0220(1.0219) | Steps 688(697.36) | Grad Norm 11.8694(5.1525) | Total Time 10.00(10.00)
Iter 15230 | Time 5.3324(5.6624) | Bit/dim 1.0240(1.0232) | Steps 676(696.17) | Grad Norm 3.3703(5.5407) | Total Time 10.00(10.00)
Iter 15240 | Time 5.4153(5.6370) | Bit/dim 1.0244(1.0245) | Steps 700(695.61) | Grad Norm 5.6117(5.8340) | Total Time 10.00(10.00)
Iter 15250 | Time 5.5561(5.6361) | Bit/dim 1.0429(1.0257) | Steps 706(696.74) | Grad Norm 5.9222(6.4073) | Total Time 10.00(10.00)
Iter 15260 | Time 5.7061(5.6288) | Bit/dim 1.0026(1.0262) | Steps 700(696.77) | Grad Norm 2.5133(6.1313) | Total Time 10.00(10.00)
Iter 15270 | Time 5.8105(5.6627) | Bit/dim 1.0391(1.0251) | Steps 712(699.11) | Grad Norm 3.0466(5.2312) | Total Time 10.00(10.00)
Iter 15280 | Time 5.8173(5.6830) | Bit/dim 0.9984(1.0223) | Steps 718(701.53) | Grad Norm 2.1853(4.4972) | Total Time 10.00(10.00)
Iter 15290 | Time 5.7572(5.7276) | Bit/dim 1.0019(1.0215) | Steps 712(704.79) | Grad Norm 0.8038(3.7611) | Total Time 10.00(10.00)
validating...
Epoch 0051 | Time 72.0179, Bit/dim 1.0135
===> Using batch size 200. Total 300 iterations/epoch.
Iter 15300 | Time 5.7982(5.7488) | Bit/dim 1.0075(1.0180) | Steps 694(705.62) | Grad Norm 15.4465(3.9656) | Total Time 10.00(10.00)
Iter 15310 | Time 5.4509(5.7074) | Bit/dim 1.0533(1.0326) | Steps 688(704.58) | Grad Norm 3.5653(5.3560) | Total Time 10.00(10.00)
Iter 15320 | Time 5.9372(5.7208) | Bit/dim 1.0534(1.0386) | Steps 724(705.86) | Grad Norm 2.3304(5.1552) | Total Time 10.00(10.00)
Iter 15330 | Time 5.7729(5.7370) | Bit/dim 1.0042(1.0345) | Steps 706(707.20) | Grad Norm 2.7237(4.5477) | Total Time 10.00(10.00)
Iter 15340 | Time 5.8344(5.7864) | Bit/dim 1.0341(1.0297) | Steps 724(709.51) | Grad Norm 2.5994(3.8639) | Total Time 10.00(10.00)
Iter 15350 | Time 5.6421(5.7577) | Bit/dim 1.0541(1.0270) | Steps 700(707.31) | Grad Norm 7.8231(4.6295) | Total Time 10.00(10.00)
Iter 15360 | Time 5.5478(5.7286) | Bit/dim 1.0212(1.0304) | Steps 706(706.88) | Grad Norm 3.4296(5.4649) | Total Time 10.00(10.00)
Iter 15370 | Time 5.8257(5.7146) | Bit/dim 0.9791(1.0270) | Steps 694(706.91) | Grad Norm 8.0347(5.3326) | Total Time 10.00(10.00)
Iter 15380 | Time 5.3286(5.6865) | Bit/dim 1.0186(1.0287) | Steps 694(707.26) | Grad Norm 6.1224(5.5460) | Total Time 10.00(10.00)
Iter 15390 | Time 5.8332(5.7028) | Bit/dim 1.0066(1.0239) | Steps 718(707.84) | Grad Norm 1.7835(5.2426) | Total Time 10.00(10.00)
Iter 15400 | Time 5.4541(5.7242) | Bit/dim 0.9998(1.0209) | Steps 700(708.58) | Grad Norm 4.0089(4.8537) | Total Time 10.00(10.00)
Iter 15410 | Time 5.4219(5.7234) | Bit/dim 1.0471(1.0196) | Steps 694(707.85) | Grad Norm 3.8174(4.8861) | Total Time 10.00(10.00)
Iter 15420 | Time 5.4504(5.7043) | Bit/dim 1.0195(1.0146) | Steps 700(706.76) | Grad Norm 6.8609(4.7447) | Total Time 10.00(10.00)
Iter 15430 | Time 5.7932(5.6958) | Bit/dim 1.0398(1.0242) | Steps 718(706.29) | Grad Norm 3.7821(5.8048) | Total Time 10.00(10.00)
Iter 15440 | Time 5.6259(5.6818) | Bit/dim 1.0296(1.0281) | Steps 718(706.01) | Grad Norm 3.7255(5.6454) | Total Time 10.00(10.00)
Iter 15450 | Time 5.8657(5.7177) | Bit/dim 1.0029(1.0262) | Steps 700(708.57) | Grad Norm 6.1222(5.0664) | Total Time 10.00(10.00)
Iter 15460 | Time 5.8000(5.7565) | Bit/dim 0.9961(1.0241) | Steps 718(711.67) | Grad Norm 1.0411(4.3869) | Total Time 10.00(10.00)
Iter 15470 | Time 6.1415(5.7764) | Bit/dim 1.0115(1.0216) | Steps 724(710.84) | Grad Norm 3.7820(3.9024) | Total Time 10.00(10.00)
Iter 15480 | Time 6.0563(5.8125) | Bit/dim 0.9677(1.0169) | Steps 724(710.40) | Grad Norm 4.2939(3.4795) | Total Time 10.00(10.00)
Iter 15490 | Time 6.0891(5.8402) | Bit/dim 0.9703(1.0149) | Steps 718(711.76) | Grad Norm 8.0953(4.0445) | Total Time 10.00(10.00)
Iter 15500 | Time 5.4714(5.7891) | Bit/dim 1.1520(1.0448) | Steps 706(711.67) | Grad Norm 3.2004(5.2696) | Total Time 10.00(10.00)
Iter 15510 | Time 5.9882(5.7427) | Bit/dim 1.0602(1.0561) | Steps 700(708.14) | Grad Norm 2.2325(4.6037) | Total Time 10.00(10.00)
Iter 15520 | Time 5.5186(5.7079) | Bit/dim 1.0374(1.0533) | Steps 682(703.98) | Grad Norm 2.7060(3.9381) | Total Time 10.00(10.00)
Iter 15530 | Time 5.5897(5.6895) | Bit/dim 1.0812(1.0461) | Steps 706(702.05) | Grad Norm 9.3442(4.7844) | Total Time 10.00(10.00)
Iter 15540 | Time 5.5948(5.6812) | Bit/dim 1.0635(1.0469) | Steps 712(701.94) | Grad Norm 5.8363(6.0906) | Total Time 10.00(10.00)
Iter 15550 | Time 5.8212(5.6867) | Bit/dim 1.0152(1.0435) | Steps 712(703.95) | Grad Norm 2.9566(5.4269) | Total Time 10.00(10.00)
Iter 15560 | Time 5.6753(5.6956) | Bit/dim 0.9868(1.0388) | Steps 700(705.85) | Grad Norm 2.3652(4.5855) | Total Time 10.00(10.00)
Iter 15570 | Time 5.7586(5.7614) | Bit/dim 1.0182(1.0321) | Steps 706(707.82) | Grad Norm 1.4996(4.0768) | Total Time 10.00(10.00)
Iter 15580 | Time 5.8016(5.7859) | Bit/dim 0.9965(1.0248) | Steps 712(709.69) | Grad Norm 1.5680(3.6356) | Total Time 10.00(10.00)
Iter 15590 | Time 5.9002(5.8183) | Bit/dim 1.0377(1.0205) | Steps 700(710.51) | Grad Norm 13.6180(3.9132) | Total Time 10.00(10.00)
validating...
Epoch 0052 | Time 72.1757, Bit/dim 1.1501
===> Using batch size 200. Total 300 iterations/epoch.
Iter 15600 | Time 5.6922(5.7796) | Bit/dim 1.1397(1.0464) | Steps 718(711.38) | Grad Norm 4.0166(5.4852) | Total Time 10.00(10.00)
Iter 15610 | Time 6.0164(5.7277) | Bit/dim 1.0475(1.0552) | Steps 718(709.36) | Grad Norm 2.7059(4.9399) | Total Time 10.00(10.00)
Iter 15620 | Time 5.7692(5.6935) | Bit/dim 1.0492(1.0545) | Steps 700(705.32) | Grad Norm 1.0608(4.2988) | Total Time 10.00(10.00)
Iter 15630 | Time 5.7746(5.7111) | Bit/dim 1.0374(1.0450) | Steps 712(705.57) | Grad Norm 0.8038(3.4410) | Total Time 10.00(10.00)
Iter 15640 | Time 5.8088(5.7261) | Bit/dim 0.9992(1.0372) | Steps 712(705.92) | Grad Norm 0.9583(3.0064) | Total Time 10.00(10.00)
Iter 15650 | Time 5.3570(5.7305) | Bit/dim 1.0435(1.0320) | Steps 712(707.53) | Grad Norm 7.6177(3.5251) | Total Time 10.00(10.00)
Iter 15660 | Time 5.7460(5.7316) | Bit/dim 1.0856(1.0401) | Steps 724(708.70) | Grad Norm 6.9228(5.5033) | Total Time 10.00(10.00)
Iter 15670 | Time 5.5862(5.7353) | Bit/dim 1.0371(1.0425) | Steps 694(710.49) | Grad Norm 5.8047(5.4336) | Total Time 10.00(10.00)
Iter 15680 | Time 5.6560(5.7554) | Bit/dim 1.0273(1.0374) | Steps 712(713.00) | Grad Norm 4.3589(4.9781) | Total Time 10.00(10.00)
Iter 15690 | Time 5.5233(5.7693) | Bit/dim 1.0410(1.0324) | Steps 706(713.67) | Grad Norm 3.8585(4.7027) | Total Time 10.00(10.00)
Iter 15700 | Time 5.4441(5.7687) | Bit/dim 1.0201(1.0283) | Steps 700(711.48) | Grad Norm 4.6181(4.4729) | Total Time 10.00(10.00)
Iter 15710 | Time 5.7865(5.7572) | Bit/dim 1.0023(1.0223) | Steps 718(711.06) | Grad Norm 1.6532(4.4952) | Total Time 10.00(10.00)
Iter 15720 | Time 5.8318(5.7891) | Bit/dim 1.0031(1.0196) | Steps 718(711.83) | Grad Norm 3.0322(3.8081) | Total Time 10.00(10.00)
Iter 15730 | Time 5.7090(5.8055) | Bit/dim 1.0093(1.0167) | Steps 694(712.52) | Grad Norm 22.0578(4.2252) | Total Time 10.00(10.00)
Iter 15740 | Time 5.8410(5.7689) | Bit/dim 1.0343(1.0269) | Steps 730(710.96) | Grad Norm 5.5179(5.2843) | Total Time 10.00(10.00)
Iter 15750 | Time 5.8152(5.7646) | Bit/dim 1.0233(1.0333) | Steps 730(711.86) | Grad Norm 3.4295(5.8588) | Total Time 10.00(10.00)
Iter 15760 | Time 5.8328(5.7727) | Bit/dim 1.0400(1.0328) | Steps 724(715.06) | Grad Norm 3.0006(5.2663) | Total Time 10.00(10.00)
Iter 15770 | Time 5.8769(5.8105) | Bit/dim 1.0223(1.0303) | Steps 730(719.26) | Grad Norm 1.8594(4.4163) | Total Time 10.00(10.00)
Iter 15780 | Time 5.7742(5.7929) | Bit/dim 1.0086(1.0237) | Steps 718(717.53) | Grad Norm 2.9212(3.6536) | Total Time 10.00(10.00)
Iter 15790 | Time 6.2681(5.8194) | Bit/dim 0.9973(1.0177) | Steps 730(718.04) | Grad Norm 1.1540(3.0139) | Total Time 10.00(10.00)
Iter 15800 | Time 5.7706(5.8361) | Bit/dim 1.0083(1.0143) | Steps 718(717.95) | Grad Norm 2.9002(2.9048) | Total Time 10.00(10.00)
Iter 15810 | Time 5.5481(5.8187) | Bit/dim 1.0825(1.0148) | Steps 706(716.36) | Grad Norm 9.6757(4.0909) | Total Time 10.00(10.00)
Iter 15820 | Time 5.9772(5.7662) | Bit/dim 1.0487(1.0246) | Steps 724(714.56) | Grad Norm 6.2854(4.8572) | Total Time 10.00(10.00)
Iter 15830 | Time 5.8869(5.7821) | Bit/dim 1.0193(1.0239) | Steps 730(715.67) | Grad Norm 2.1758(4.7522) | Total Time 10.00(10.00)
Iter 15840 | Time 5.9754(5.8323) | Bit/dim 1.0393(1.0221) | Steps 724(718.13) | Grad Norm 2.0476(4.0497) | Total Time 10.00(10.00)
Iter 15850 | Time 5.8615(5.8411) | Bit/dim 0.9944(1.0208) | Steps 700(718.39) | Grad Norm 15.4708(4.1467) | Total Time 10.00(10.00)
Iter 15860 | Time 5.9199(5.7836) | Bit/dim 1.0459(1.0278) | Steps 706(715.30) | Grad Norm 19.5230(5.6870) | Total Time 10.00(10.00)
Iter 15870 | Time 5.8165(5.7511) | Bit/dim 1.0652(1.0318) | Steps 718(714.00) | Grad Norm 2.4719(5.4009) | Total Time 10.00(10.00)
Iter 15880 | Time 5.8740(5.7789) | Bit/dim 1.0086(1.0272) | Steps 736(716.95) | Grad Norm 4.3280(4.9934) | Total Time 10.00(10.00)
Iter 15890 | Time 5.6128(5.7466) | Bit/dim 1.0648(1.0305) | Steps 730(716.12) | Grad Norm 4.9962(5.8091) | Total Time 10.00(10.00)
validating...
Epoch 0053 | Time 72.6280, Bit/dim 1.0186
===> Using batch size 200. Total 300 iterations/epoch.
Iter 15900 | Time 5.7724(5.7510) | Bit/dim 1.0464(1.0316) | Steps 718(715.71) | Grad Norm 2.9160(5.4712) | Total Time 10.00(10.00)
Iter 15910 | Time 5.7248(5.7787) | Bit/dim 0.9967(1.0261) | Steps 724(719.01) | Grad Norm 2.2991(4.9831) | Total Time 10.00(10.00)
Iter 15920 | Time 5.5953(5.7476) | Bit/dim 1.0258(1.0246) | Steps 724(717.25) | Grad Norm 4.3606(5.4349) | Total Time 10.00(10.00)
Iter 15930 | Time 5.9076(5.7496) | Bit/dim 1.0165(1.0229) | Steps 736(718.68) | Grad Norm 1.4022(5.0077) | Total Time 10.00(10.00)
Iter 15940 | Time 5.5770(5.7033) | Bit/dim 1.0193(1.0281) | Steps 694(714.13) | Grad Norm 3.5218(5.7173) | Total Time 10.00(10.00)
Iter 15950 | Time 5.8605(5.7162) | Bit/dim 1.0274(1.0357) | Steps 712(714.26) | Grad Norm 2.9115(5.9188) | Total Time 10.00(10.00)
Iter 15960 | Time 5.7635(5.7358) | Bit/dim 1.0356(1.0351) | Steps 712(716.32) | Grad Norm 3.2004(5.2577) | Total Time 10.00(10.00)
Iter 15970 | Time 5.6377(5.7533) | Bit/dim 1.0236(1.0296) | Steps 718(719.83) | Grad Norm 1.3462(4.2848) | Total Time 10.00(10.00)
Iter 15980 | Time 6.1355(5.7737) | Bit/dim 0.9803(1.0252) | Steps 736(721.47) | Grad Norm 5.4067(3.6411) | Total Time 10.00(10.00)
Iter 15990 | Time 5.4428(5.7583) | Bit/dim 1.0525(1.0278) | Steps 694(720.26) | Grad Norm 6.3070(5.2414) | Total Time 10.00(10.00)
Iter 16000 | Time 5.6602(5.7611) | Bit/dim 1.0229(1.0300) | Steps 712(719.63) | Grad Norm 3.3625(5.1104) | Total Time 10.00(10.00)
Iter 16010 | Time 5.8846(5.7835) | Bit/dim 1.0229(1.0280) | Steps 730(721.80) | Grad Norm 3.2053(4.5495) | Total Time 10.00(10.00)
Iter 16020 | Time 5.9770(5.8078) | Bit/dim 1.0119(1.0233) | Steps 730(721.62) | Grad Norm 1.3217(3.8895) | Total Time 10.00(10.00)
Iter 16030 | Time 5.7973(5.8130) | Bit/dim 1.0115(1.0178) | Steps 718(720.06) | Grad Norm 1.3627(3.4385) | Total Time 10.00(10.00)
Iter 16040 | Time 5.8243(5.7911) | Bit/dim 1.0294(1.0186) | Steps 724(718.45) | Grad Norm 4.0945(4.5170) | Total Time 10.00(10.00)
Iter 16050 | Time 5.4547(5.7427) | Bit/dim 1.0205(1.0180) | Steps 706(716.90) | Grad Norm 6.0772(5.4032) | Total Time 10.00(10.00)
Iter 16060 | Time 5.8458(5.7431) | Bit/dim 1.0335(1.0249) | Steps 730(717.40) | Grad Norm 2.7435(5.9595) | Total Time 10.00(10.00)
Iter 16070 | Time 5.6416(5.7567) | Bit/dim 1.0164(1.0281) | Steps 712(718.13) | Grad Norm 2.1774(5.1977) | Total Time 10.00(10.00)
Iter 16080 | Time 5.8512(5.8043) | Bit/dim 1.0267(1.0253) | Steps 730(721.78) | Grad Norm 1.5381(4.5683) | Total Time 10.00(10.00)
Iter 16090 | Time 6.0391(5.8123) | Bit/dim 1.0258(1.0205) | Steps 736(723.36) | Grad Norm 1.3478(3.8555) | Total Time 10.00(10.00)
Iter 16100 | Time 5.7618(5.8243) | Bit/dim 1.0001(1.0174) | Steps 712(722.74) | Grad Norm 1.2872(3.4680) | Total Time 10.00(10.00)
Iter 16110 | Time 5.5208(5.7664) | Bit/dim 1.1407(1.0367) | Steps 718(719.14) | Grad Norm 4.3554(5.0800) | Total Time 10.00(10.00)
Iter 16120 | Time 6.0053(5.7364) | Bit/dim 1.0533(1.0488) | Steps 736(719.59) | Grad Norm 2.1084(4.5904) | Total Time 10.00(10.00)
Iter 16130 | Time 5.4912(5.7115) | Bit/dim 1.0318(1.0453) | Steps 700(718.85) | Grad Norm 1.0943(3.9390) | Total Time 10.00(10.00)
Iter 16140 | Time 5.6426(5.6719) | Bit/dim 0.9697(1.0360) | Steps 706(714.89) | Grad Norm 0.8537(3.2436) | Total Time 10.00(10.00)
Iter 16150 | Time 5.6153(5.6309) | Bit/dim 0.9724(1.0312) | Steps 706(711.00) | Grad Norm 4.7013(3.0917) | Total Time 10.00(10.00)
Iter 16160 | Time 6.0351(5.6308) | Bit/dim 1.0274(1.0258) | Steps 706(709.63) | Grad Norm 20.2615(3.8975) | Total Time 10.00(10.00)
Iter 16170 | Time 5.8129(5.6370) | Bit/dim 1.0385(1.0337) | Steps 718(709.86) | Grad Norm 4.1629(5.2718) | Total Time 10.00(10.00)
Iter 16180 | Time 5.7204(5.6729) | Bit/dim 1.0037(1.0388) | Steps 730(711.77) | Grad Norm 2.9641(5.6301) | Total Time 10.00(10.00)
Iter 16190 | Time 5.9452(5.7301) | Bit/dim 0.9918(1.0327) | Steps 718(715.44) | Grad Norm 3.3307(5.0529) | Total Time 10.00(10.00)
validating...
Epoch 0054 | Time 72.5115, Bit/dim 1.0061
===> Using batch size 200. Total 300 iterations/epoch.
Iter 16200 | Time 5.9511(5.7771) | Bit/dim 1.0022(1.0282) | Steps 718(719.74) | Grad Norm 1.3120(4.1870) | Total Time 10.00(10.00)
Iter 16210 | Time 5.6342(5.8107) | Bit/dim 1.0041(1.0225) | Steps 712(720.36) | Grad Norm 0.6594(3.4809) | Total Time 10.00(10.00)
Iter 16220 | Time 5.8339(5.8184) | Bit/dim 1.0296(1.0224) | Steps 718(719.80) | Grad Norm 2.6312(3.0246) | Total Time 10.00(10.00)
Iter 16230 | Time 5.8901(5.8263) | Bit/dim 1.0147(1.0189) | Steps 706(719.40) | Grad Norm 7.0969(3.6220) | Total Time 10.00(10.00)
Iter 16240 | Time 5.7083(5.7836) | Bit/dim 1.0626(1.0237) | Steps 694(716.23) | Grad Norm 26.0774(5.2470) | Total Time 10.00(10.00)
Iter 16250 | Time 5.7084(5.7559) | Bit/dim 1.0605(1.0294) | Steps 712(715.83) | Grad Norm 3.4740(5.2195) | Total Time 10.00(10.00)
Iter 16260 | Time 5.7288(5.7719) | Bit/dim 1.0192(1.0268) | Steps 724(718.85) | Grad Norm 3.1013(4.7241) | Total Time 10.00(10.00)
Iter 16270 | Time 5.7454(5.8109) | Bit/dim 1.0108(1.0224) | Steps 736(723.97) | Grad Norm 3.0367(4.3220) | Total Time 10.00(10.00)
Iter 16280 | Time 5.9084(5.8617) | Bit/dim 1.0302(1.0205) | Steps 712(728.34) | Grad Norm 11.5652(4.1931) | Total Time 10.00(10.00)
Iter 16290 | Time 5.6127(5.8011) | Bit/dim 1.0845(1.0339) | Steps 730(726.61) | Grad Norm 4.1024(5.2269) | Total Time 10.00(10.00)
Iter 16300 | Time 5.7426(5.7891) | Bit/dim 1.0159(1.0368) | Steps 712(725.29) | Grad Norm 1.5138(4.7819) | Total Time 10.00(10.00)
Iter 16310 | Time 5.9320(5.8070) | Bit/dim 0.9891(1.0326) | Steps 736(726.16) | Grad Norm 1.3577(3.9510) | Total Time 10.00(10.00)
Iter 16320 | Time 5.8648(5.8269) | Bit/dim 1.0190(1.0268) | Steps 724(727.70) | Grad Norm 1.1361(3.2532) | Total Time 10.00(10.00)
Iter 16330 | Time 6.2887(5.8585) | Bit/dim 1.0041(1.0226) | Steps 742(728.38) | Grad Norm 1.6229(2.8492) | Total Time 10.00(10.00)
Iter 16340 | Time 5.3652(5.8441) | Bit/dim 1.0722(1.0226) | Steps 700(726.95) | Grad Norm 8.7139(4.2577) | Total Time 10.00(10.00)
Iter 16350 | Time 5.7141(5.7928) | Bit/dim 1.0498(1.0343) | Steps 736(723.73) | Grad Norm 2.6158(4.6544) | Total Time 10.00(10.00)
Iter 16360 | Time 5.8342(5.8230) | Bit/dim 1.0422(1.0318) | Steps 724(725.51) | Grad Norm 3.0282(4.1478) | Total Time 10.00(10.00)
Iter 16370 | Time 5.9484(5.8513) | Bit/dim 1.0064(1.0277) | Steps 742(728.57) | Grad Norm 1.1813(3.3907) | Total Time 10.00(10.00)
Iter 16380 | Time 5.9047(5.8882) | Bit/dim 0.9934(1.0176) | Steps 736(731.45) | Grad Norm 3.4748(3.0963) | Total Time 10.00(10.00)
Iter 16390 | Time 5.8373(5.9178) | Bit/dim 1.0196(1.0167) | Steps 736(732.33) | Grad Norm 4.6025(3.3849) | Total Time 10.00(10.00)
Iter 16400 | Time 5.9378(5.9162) | Bit/dim 1.0185(1.0144) | Steps 742(731.39) | Grad Norm 7.0006(4.1055) | Total Time 10.00(10.00)
Iter 16410 | Time 5.6712(5.8403) | Bit/dim 1.1109(1.0306) | Steps 712(727.07) | Grad Norm 6.8401(5.7830) | Total Time 10.00(10.00)
Iter 16420 | Time 5.7678(5.7999) | Bit/dim 1.0817(1.0382) | Steps 748(725.54) | Grad Norm 3.8652(5.5942) | Total Time 10.00(10.00)
Iter 16430 | Time 5.8802(5.8156) | Bit/dim 1.0145(1.0337) | Steps 736(726.48) | Grad Norm 2.6967(4.9753) | Total Time 10.00(10.00)
Iter 16440 | Time 5.9143(5.8495) | Bit/dim 1.0296(1.0268) | Steps 742(728.92) | Grad Norm 2.6604(4.2844) | Total Time 10.00(10.00)
Iter 16450 | Time 5.8042(5.8877) | Bit/dim 1.0153(1.0213) | Steps 724(730.03) | Grad Norm 1.3712(3.5465) | Total Time 10.00(10.00)
Iter 16460 | Time 5.8039(5.8817) | Bit/dim 1.0053(1.0165) | Steps 718(727.84) | Grad Norm 6.3882(3.7215) | Total Time 10.00(10.00)
Iter 16470 | Time 5.6018(5.8013) | Bit/dim 1.0408(1.0296) | Steps 712(723.07) | Grad Norm 4.7140(5.0039) | Total Time 10.00(10.00)
Iter 16480 | Time 6.0476(5.7998) | Bit/dim 1.0122(1.0302) | Steps 712(722.88) | Grad Norm 5.8681(4.5970) | Total Time 10.00(10.00)
Iter 16490 | Time 5.9075(5.8184) | Bit/dim 0.9746(1.0250) | Steps 748(725.52) | Grad Norm 2.1981(4.2902) | Total Time 10.00(10.00)
validating...
Epoch 0055 | Time 73.6824, Bit/dim 1.0020
===> Using batch size 200. Total 300 iterations/epoch.
Iter 16500 | Time 5.8699(5.8490) | Bit/dim 1.0271(1.0224) | Steps 730(727.88) | Grad Norm 0.8742(4.2178) | Total Time 10.00(10.00)
Iter 16510 | Time 5.4611(5.8201) | Bit/dim 1.0366(1.0214) | Steps 718(725.65) | Grad Norm 6.4864(4.9529) | Total Time 10.00(10.00)
Iter 16520 | Time 5.8389(5.7972) | Bit/dim 1.0544(1.0376) | Steps 718(725.13) | Grad Norm 2.6818(5.5468) | Total Time 10.00(10.00)
Iter 16530 | Time 5.9157(5.8074) | Bit/dim 1.0247(1.0377) | Steps 712(724.81) | Grad Norm 6.2934(5.2611) | Total Time 10.00(10.00)
Iter 16540 | Time 5.6223(5.8021) | Bit/dim 1.0151(1.0326) | Steps 700(725.50) | Grad Norm 1.3942(4.3751) | Total Time 10.00(10.00)
Iter 16550 | Time 5.8466(5.7915) | Bit/dim 1.0385(1.0261) | Steps 730(724.05) | Grad Norm 0.8466(3.5357) | Total Time 10.00(10.00)
Iter 16560 | Time 5.8308(5.8029) | Bit/dim 1.0041(1.0205) | Steps 724(724.60) | Grad Norm 3.1369(3.3969) | Total Time 10.00(10.00)
Iter 16570 | Time 5.6735(5.7783) | Bit/dim 1.0342(1.0285) | Steps 718(722.76) | Grad Norm 5.3215(4.6839) | Total Time 10.00(10.00)
Iter 16580 | Time 6.1246(5.7907) | Bit/dim 1.0373(1.0317) | Steps 730(725.14) | Grad Norm 4.5910(4.3800) | Total Time 10.00(10.00)
