/Users/bjzhang/Dropbox (Personal)/icml2023_project/code/ffjord-master/train_discrete_toy.py
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import argparse
import os
import time

import torch
import torch.optim as optim

import lib.layers as layers
import lib.toy_data as toy_data
import lib.utils as utils
from lib.visualize_flow import visualize_transform

from train_misc import standard_normal_logprob
from train_misc import count_parameters

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'fixed_adams']
parser = argparse.ArgumentParser('Continuous Normalizing Flow')
parser.add_argument(
    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='pinwheel'
)

parser.add_argument('--depth', help='number of coupling layers', type=int, default=10)
parser.add_argument('--glow', type=eval, choices=[True, False], default=False)
parser.add_argument('--nf', type=eval, choices=[True, False], default=False)

parser.add_argument('--niters', type=int, default=100001)
parser.add_argument('--batch_size', type=int, default=100)
parser.add_argument('--test_batch_size', type=int, default=1000)
parser.add_argument('--lr', type=float, default=1e-4)
parser.add_argument('--weight_decay', type=float, default=0)

# Track quantities
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument('--save', type=str, default='experiments/cnf')
parser.add_argument('--viz_freq', type=int, default=1000)
parser.add_argument('--val_freq', type=int, default=1000)
parser.add_argument('--log_freq', type=int, default=100)
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')


def construct_model():

    if args.nf:
        chain = []
        for i in range(args.depth):
            chain.append(layers.PlanarFlow(2))
        return layers.SequentialFlow(chain)
    else:
        chain = []
        for i in range(args.depth):
            if args.glow: chain.append(layers.BruteForceLayer(2))
            chain.append(layers.CouplingLayer(2, swap=i % 2 == 0))
        return layers.SequentialFlow(chain)


def get_transforms(model):

    if args.nf:
        sample_fn = None
    else:

        def sample_fn(z, logpz=None):
            if logpz is not None:
                return model(z, logpz, reverse=True)
            else:
                return model(z, reverse=True)

    def density_fn(x, logpx=None):
        if logpx is not None:
            return model(x, logpx, reverse=False)
        else:
            return model(x, reverse=False)

    return sample_fn, density_fn


def compute_loss(args, model, batch_size=None):
    if batch_size is None: batch_size = args.batch_size

    # load data
    x = toy_data.inf_train_gen(args.data, batch_size=batch_size)
    x = torch.from_numpy(x).type(torch.float32).to(device)
    zero = torch.zeros(x.shape[0], 1).to(x)

    # transform to z
    z, delta_logp = model(x, zero)

    # compute log q(z)
    logpz = standard_normal_logprob(z).sum(1, keepdim=True)

    logpx = logpz - delta_logp
    loss = -torch.mean(logpx)
    return loss


if __name__ == '__main__':

    model = construct_model().to(device)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    optimizer = optim.Adamax(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    time_meter = utils.RunningAverageMeter(0.98)
    loss_meter = utils.RunningAverageMeter(0.98)

    end = time.time()
    best_loss = float('inf')
    model.train()
    for itr in range(1, args.niters + 1):
        optimizer.zero_grad()

        loss = compute_loss(args, model)
        loss_meter.update(loss.item())

        loss.backward()
        optimizer.step()

        time_meter.update(time.time() - end)

        if itr % args.log_freq == 0:
            log_message = (
                'Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f})'.format(
                    itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg
                )
            )
            logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                model.eval()
                test_loss = compute_loss(args, model, batch_size=args.test_batch_size)
                log_message = '[TEST] Iter {:04d} | Test Loss {:.6f}'.format(itr, test_loss)
                logger.info(log_message)

                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    utils.makedirs(args.save)
                    torch.save({
                        'args': args,
                        'state_dict': model.state_dict(),
                    }, os.path.join(args.save, 'checkpt.pth'))
                model.train()

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                model.eval()
                p_samples = toy_data.inf_train_gen(args.data, batch_size=2000)

                sample_fn, density_fn = get_transforms(model)

                plt.figure(figsize=(9, 3))
                visualize_transform(
                    p_samples, torch.randn, standard_normal_logprob, transform=sample_fn, inverse_transform=density_fn,
                    samples=True, npts=800, device=device
                )
                fig_filename = os.path.join(args.save, 'figs', '{:04d}.jpg'.format(itr))
                utils.makedirs(os.path.dirname(fig_filename))
                plt.savefig(fig_filename)
                plt.close()
                model.train()

        end = time.time()

    logger.info('Training has finished.')

Namespace(data='8gaussians', depth=10, glow=True, nf=False, niters=100001, batch_size=100, test_batch_size=1000, lr=0.0001, weight_decay=0, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='experiments/cnf', viz_freq=1000, val_freq=1000, log_freq=100, gpu=0)
SequentialFlow(
  (chain): ModuleList(
    (0): BruteForceLayer()
    (1): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (2): BruteForceLayer()
    (3): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (4): BruteForceLayer()
    (5): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (6): BruteForceLayer()
    (7): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (8): BruteForceLayer()
    (9): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (10): BruteForceLayer()
    (11): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (12): BruteForceLayer()
    (13): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (14): BruteForceLayer()
    (15): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (16): BruteForceLayer()
    (17): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (18): BruteForceLayer()
    (19): CouplingLayer(
      (net_s_t): Sequential(
        (0): Linear(in_features=1, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU(inplace=True)
        (4): Linear(in_features=64, out_features=2, bias=True)
      )
    )
  )
)
Number of trainable parameters: 44220
Iter 0100 | Time 0.0145(0.0313) | Loss 4.252824(4.248180)
Iter 0200 | Time 0.0141(0.0178) | Loss 4.181940(4.200923)
Iter 0300 | Time 0.0157(0.0147) | Loss 4.160287(4.169384)
Iter 0400 | Time 0.0145(0.0145) | Loss 4.134821(4.140947)
Iter 0500 | Time 0.0157(0.0172) | Loss 4.100995(4.117517)
Iter 0600 | Time 0.0132(0.0177) | Loss 4.086251(4.088128)
Iter 0700 | Time 0.0185(0.0168) | Loss 4.061831(4.066441)
Iter 0800 | Time 0.0132(0.0147) | Loss 4.036528(4.040595)
Iter 0900 | Time 0.0130(0.0142) | Loss 4.006174(4.008577)
Iter 1000 | Time 0.0132(0.0143) | Loss 3.958692(3.977778)
[TEST] Iter 1000 | Test Loss 3.971936
Iter 1100 | Time 0.0157(0.0253) | Loss 3.927547(3.944851)
Iter 1200 | Time 0.0156(0.0220) | Loss 3.906774(3.909724)
Iter 1300 | Time 0.0139(0.0179) | Loss 3.823327(3.872668)
Iter 1400 | Time 0.0157(0.0268) | Loss 3.857170(3.842147)
Iter 1500 | Time 0.0217(0.0250) | Loss 3.751931(3.803449)
Iter 1600 | Time 0.0266(0.0211) | Loss 3.788106(3.775714)
Iter 1700 | Time 0.0161(0.0207) | Loss 3.739270(3.738989)
Iter 1800 | Time 0.0223(0.0206) | Loss 3.655657(3.705780)
Iter 1900 | Time 0.0134(0.0204) | Loss 3.639678(3.669242)
Iter 2000 | Time 0.0155(0.0162) | Loss 3.602762(3.634780)
[TEST] Iter 2000 | Test Loss 3.602671
Iter 2100 | Time 0.0174(0.0218) | Loss 3.599586(3.604947)
Iter 2200 | Time 0.0177(0.0178) | Loss 3.524652(3.572239)
Iter 2300 | Time 0.0142(0.0172) | Loss 3.568583(3.541983)
Iter 2400 | Time 0.0147(0.0162) | Loss 3.436161(3.509734)
Iter 2500 | Time 0.0187(0.0165) | Loss 3.449093(3.482565)
Iter 2600 | Time 0.0233(0.0160) | Loss 3.441599(3.465185)
Iter 2700 | Time 0.0145(0.0154) | Loss 3.523856(3.446321)
Iter 2800 | Time 0.0151(0.0149) | Loss 3.350865(3.426391)
Iter 2900 | Time 0.0138(0.0152) | Loss 3.383886(3.404348)
Iter 3000 | Time 0.0144(0.0147) | Loss 3.322025(3.378746)
[TEST] Iter 3000 | Test Loss 3.352461
Iter 3100 | Time 0.0146(0.0144) | Loss 3.337513(3.369348)
Iter 3200 | Time 0.0136(0.0142) | Loss 3.404921(3.349895)
Iter 3300 | Time 0.0142(0.0147) | Loss 3.295879(3.336485)
Iter 3400 | Time 0.0145(0.0144) | Loss 3.182040(3.315464)
Iter 3500 | Time 0.0147(0.0144) | Loss 3.217549(3.303546)
Iter 3600 | Time 0.0123(0.0147) | Loss 3.244813(3.304482)
Iter 3700 | Time 0.0146(0.0145) | Loss 3.290303(3.289396)
Iter 3800 | Time 0.0167(0.0142) | Loss 3.340685(3.269992)
Iter 3900 | Time 0.0134(0.0160) | Loss 3.355787(3.267238)
Iter 4000 | Time 0.0145(0.0157) | Loss 3.305679(3.265400)
[TEST] Iter 4000 | Test Loss 3.223599
Iter 4100 | Time 0.0132(0.0155) | Loss 3.343773(3.250542)
Iter 4200 | Time 0.0179(0.0158) | Loss 3.229124(3.231982)
Iter 4300 | Time 0.0166(0.0169) | Loss 3.278635(3.228575)
Iter 4400 | Time 0.0139(0.0176) | Loss 3.319071(3.236722)
Iter 4500 | Time 0.0129(0.0199) | Loss 3.175461(3.221813)
Iter 4600 | Time 0.0144(0.0160) | Loss 3.384193(3.224217)
Iter 4700 | Time 0.0142(0.0145) | Loss 3.173496(3.220462)
Iter 4800 | Time 0.0133(0.0164) | Loss 3.200171(3.200194)
Iter 4900 | Time 0.0150(0.0168) | Loss 3.173581(3.180237)
Iter 5000 | Time 0.0144(0.0155) | Loss 3.301547(3.188190)
[TEST] Iter 5000 | Test Loss 3.240261
Iter 5100 | Time 0.0148(0.0148) | Loss 3.171021(3.199618)
Iter 5200 | Time 0.0175(0.0157) | Loss 3.154400(3.169672)
Iter 5300 | Time 0.0136(0.0164) | Loss 3.142489(3.171982)
Iter 5400 | Time 0.0136(0.0146) | Loss 3.177855(3.176158)
Iter 5500 | Time 0.0121(0.0148) | Loss 3.113980(3.155879)
Iter 5600 | Time 0.0134(0.0162) | Loss 3.090590(3.164282)
Iter 5700 | Time 0.0148(0.0157) | Loss 3.210772(3.169369)
Iter 5800 | Time 0.0134(0.0174) | Loss 3.185100(3.152676)
Iter 5900 | Time 0.0192(0.0234) | Loss 3.226079(3.147579)
Iter 6000 | Time 0.0150(0.0198) | Loss 3.145329(3.146238)
[TEST] Iter 6000 | Test Loss 3.134835
Iter 6100 | Time 0.0134(0.0182) | Loss 3.018651(3.153821)
Iter 6200 | Time 0.0138(0.0177) | Loss 3.108890(3.141321)
Iter 6300 | Time 0.0191(0.0212) | Loss 3.209109(3.151689)
Iter 6400 | Time 0.0270(0.0207) | Loss 2.990521(3.151083)
Iter 6500 | Time 0.0330(0.0203) | Loss 3.289956(3.148787)
Iter 6600 | Time 0.0133(0.0311) | Loss 3.118570(3.129902)
Iter 6700 | Time 0.0282(0.0360) | Loss 3.070171(3.122296)
Iter 6800 | Time 0.0292(0.0269) | Loss 3.121192(3.127358)
Iter 6900 | Time 0.0156(0.0295) | Loss 3.012558(3.123877)
Iter 7000 | Time 0.0216(0.0255) | Loss 3.144315(3.122743)
[TEST] Iter 7000 | Test Loss 3.133906
Iter 7100 | Time 0.0379(0.0225) | Loss 3.256103(3.132779)
Iter 7200 | Time 0.0186(0.0240) | Loss 3.016099(3.118241)
Iter 7300 | Time 0.0318(0.0332) | Loss 3.058696(3.117227)
Iter 7400 | Time 0.0360(0.0346) | Loss 3.069686(3.107556)
Iter 7500 | Time 0.0380(0.0357) | Loss 3.051038(3.106960)
Iter 7600 | Time 0.0281(0.0364) | Loss 2.993936(3.112415)
Iter 7700 | Time 0.0292(0.0331) | Loss 3.154880(3.094492)
Iter 7800 | Time 0.0151(0.0272) | Loss 3.025192(3.099563)
Iter 7900 | Time 0.0351(0.0288) | Loss 3.059645(3.099043)
Iter 8000 | Time 0.0342(0.0286) | Loss 3.050090(3.099146)
[TEST] Iter 8000 | Test Loss 3.038982
Iter 8100 | Time 0.0163(0.0181) | Loss 3.148585(3.094710)
Iter 8200 | Time 0.0346(0.0354) | Loss 2.955593(3.094093)
Iter 8300 | Time 0.0241(0.0349) | Loss 3.175521(3.101842)
Iter 8400 | Time 0.0451(0.0359) | Loss 3.023748(3.093583)
Iter 8500 | Time 0.0225(0.0286) | Loss 3.185827(3.096634)
Iter 8600 | Time 0.0318(0.0319) | Loss 3.150861(3.099158)
Iter 8700 | Time 0.0466(0.0304) | Loss 3.002649(3.083766)
Iter 8800 | Time 0.0192(0.0317) | Loss 3.044245(3.069967)
Iter 8900 | Time 0.0224(0.0196) | Loss 3.138337(3.079439)
Iter 9000 | Time 0.0237(0.0231) | Loss 3.014544(3.084102)
[TEST] Iter 9000 | Test Loss 3.050989
Iter 9100 | Time 0.0166(0.0275) | Loss 3.023470(3.083309)
Iter 9200 | Time 0.0201(0.0227) | Loss 2.927360(3.073935)
Iter 9300 | Time 0.0229(0.0225) | Loss 3.019462(3.089401)
Iter 9400 | Time 0.0422(0.0227) | Loss 2.992757(3.078523)
Iter 9500 | Time 0.0225(0.0249) | Loss 3.032935(3.061773)
Iter 9600 | Time 0.0210(0.0242) | Loss 3.146694(3.085779)
Iter 9700 | Time 0.0297(0.0223) | Loss 2.929006(3.077016)
Iter 9800 | Time 0.0182(0.0241) | Loss 3.074185(3.075886)
Iter 9900 | Time 0.0253(0.0234) | Loss 3.248491(3.081998)
Iter 10000 | Time 0.0248(0.0233) | Loss 3.190454(3.065886)
[TEST] Iter 10000 | Test Loss 2.980799
Iter 10100 | Time 0.0117(0.0162) | Loss 3.086148(3.075997)
Iter 10200 | Time 0.0148(0.0160) | Loss 3.234281(3.068835)
Iter 10300 | Time 0.0142(0.0150) | Loss 3.011755(3.051460)
Iter 10400 | Time 0.0125(0.0153) | Loss 3.078754(3.072336)
Iter 10500 | Time 0.0131(0.0174) | Loss 2.997368(3.061613)
Iter 10600 | Time 0.0160(0.0156) | Loss 3.040380(3.068130)
Iter 10700 | Time 0.0383(0.0161) | Loss 3.191094(3.068643)
Iter 10800 | Time 0.0165(0.0155) | Loss 3.029803(3.069458)
Iter 10900 | Time 0.0142(0.0180) | Loss 2.996211(3.073167)
Iter 11000 | Time 0.0137(0.0152) | Loss 2.887979(3.056104)
[TEST] Iter 11000 | Test Loss 3.067110
Iter 11100 | Time 0.0156(0.0333) | Loss 2.937519(3.070751)
Iter 11200 | Time 0.0148(0.0228) | Loss 3.087721(3.065723)
Iter 11300 | Time 0.0161(0.0224) | Loss 3.018423(3.050949)
Iter 11400 | Time 0.0285(0.0191) | Loss 3.089771(3.054317)
Iter 11500 | Time 0.0143(0.0196) | Loss 3.195477(3.060554)
Iter 11600 | Time 0.0222(0.0263) | Loss 3.095725(3.062954)
Iter 11700 | Time 0.0214(0.0241) | Loss 3.146262(3.044931)
Iter 11800 | Time 0.0224(0.0266) | Loss 3.169652(3.038212)
Iter 11900 | Time 0.0205(0.0269) | Loss 3.031350(3.053730)
Iter 12000 | Time 0.0149(0.0190) | Loss 2.966974(3.051205)
[TEST] Iter 12000 | Test Loss 3.084479
Iter 12100 | Time 0.0191(0.0167) | Loss 3.029343(3.045095)
Iter 12200 | Time 0.0157(0.0151) | Loss 3.027032(3.050203)
Iter 12300 | Time 0.0132(0.0175) | Loss 3.137111(3.064924)
Iter 12400 | Time 0.0128(0.0154) | Loss 3.071673(3.054421)
Iter 12500 | Time 0.0150(0.0152) | Loss 3.063286(3.049350)
Iter 12600 | Time 0.0169(0.0152) | Loss 3.043372(3.046971)
Iter 12700 | Time 0.0140(0.0148) | Loss 3.122154(3.034040)
Iter 12800 | Time 0.0139(0.0145) | Loss 3.097806(3.032759)
Iter 12900 | Time 0.0196(0.0156) | Loss 2.880623(3.046883)
Iter 13000 | Time 0.0120(0.0152) | Loss 3.200897(3.037280)
[TEST] Iter 13000 | Test Loss 3.010043
Iter 13100 | Time 0.0145(0.0150) | Loss 2.919425(3.046601)
Iter 13200 | Time 0.0146(0.0146) | Loss 3.006335(3.050208)
Iter 13300 | Time 0.0168(0.0171) | Loss 2.983888(3.057050)
Iter 13400 | Time 0.0176(0.0181) | Loss 3.077509(3.040784)
Iter 13500 | Time 0.0145(0.0191) | Loss 3.067251(3.043474)
Iter 13600 | Time 0.0126(0.0170) | Loss 2.952181(3.016826)
Iter 13700 | Time 0.0135(0.0166) | Loss 3.083375(3.043369)
Iter 13800 | Time 0.0143(0.0160) | Loss 2.982592(3.027517)
Iter 13900 | Time 0.0136(0.0162) | Loss 2.975070(3.028875)
Iter 14000 | Time 0.0157(0.0148) | Loss 3.156403(3.033834)
[TEST] Iter 14000 | Test Loss 3.050190
Iter 14100 | Time 0.0141(0.0157) | Loss 3.077931(3.042231)
Iter 14200 | Time 0.0151(0.0159) | Loss 3.071716(3.029280)
Iter 14300 | Time 0.0155(0.0164) | Loss 2.911395(3.053858)
Iter 14400 | Time 0.0149(0.0150) | Loss 3.101907(3.045168)
Iter 14500 | Time 0.0139(0.0150) | Loss 3.045330(3.038665)
Iter 14600 | Time 0.0156(0.0147) | Loss 3.010392(3.036676)
Iter 14700 | Time 0.0142(0.0150) | Loss 3.166364(3.018471)
Iter 14800 | Time 0.0141(0.0160) | Loss 2.965108(3.020785)
Iter 14900 | Time 0.0164(0.0190) | Loss 2.835969(3.022149)
Iter 15000 | Time 0.0209(0.0245) | Loss 3.111636(3.027309)
[TEST] Iter 15000 | Test Loss 2.977141
Iter 15100 | Time 0.0125(0.0232) | Loss 3.228494(3.037596)
Iter 15200 | Time 0.0151(0.0240) | Loss 3.100567(3.026837)
Iter 15300 | Time 0.0150(0.0183) | Loss 3.103103(3.026801)
Iter 15400 | Time 0.0156(0.0160) | Loss 3.062709(3.017789)
Iter 15500 | Time 0.0124(0.0150) | Loss 2.972526(3.016658)
Iter 15600 | Time 0.0172(0.0155) | Loss 3.303849(3.029050)
Iter 15700 | Time 0.0134(0.0201) | Loss 2.890948(3.020236)
Iter 15800 | Time 0.0254(0.0180) | Loss 3.167366(3.031853)
Iter 15900 | Time 0.0135(0.0164) | Loss 3.082167(3.021501)
Iter 16000 | Time 0.0144(0.0154) | Loss 3.083770(3.026210)
[TEST] Iter 16000 | Test Loss 3.002810
Iter 16100 | Time 0.0141(0.0162) | Loss 3.182891(3.013542)
Iter 16200 | Time 0.0149(0.0166) | Loss 2.918850(3.022805)
Iter 16300 | Time 0.0167(0.0153) | Loss 3.183358(3.012553)
Iter 16400 | Time 0.0137(0.0155) | Loss 3.151467(3.029394)
Iter 16500 | Time 0.0126(0.0160) | Loss 3.060701(3.021407)
Iter 16600 | Time 0.0128(0.0163) | Loss 2.937614(3.003249)
Iter 16700 | Time 0.0130(0.0158) | Loss 3.281940(3.014006)
Iter 16800 | Time 0.0145(0.0158) | Loss 3.178781(3.017794)
Iter 16900 | Time 0.0144(0.0164) | Loss 3.065340(3.031203)
Iter 17000 | Time 0.0212(0.0158) | Loss 2.980558(3.005198)
[TEST] Iter 17000 | Test Loss 3.049087
Iter 17100 | Time 0.0133(0.0155) | Loss 2.984550(3.029447)
Iter 17200 | Time 0.0145(0.0155) | Loss 2.939546(3.020211)
Iter 17300 | Time 0.0141(0.0156) | Loss 2.895311(3.030302)
Iter 17400 | Time 0.0151(0.0170) | Loss 3.076373(3.017406)
Iter 17500 | Time 0.0145(0.0178) | Loss 2.943325(3.015448)
Iter 17600 | Time 0.0133(0.0169) | Loss 2.999692(3.010108)
Iter 17700 | Time 0.0171(0.0159) | Loss 2.886475(3.017132)
Iter 17800 | Time 0.0191(0.0163) | Loss 2.953449(3.005866)
Iter 17900 | Time 0.0139(0.0157) | Loss 2.751276(3.004420)
Iter 18000 | Time 0.0153(0.0161) | Loss 3.170529(3.014095)
[TEST] Iter 18000 | Test Loss 3.025492
Iter 18100 | Time 0.0144(0.0220) | Loss 3.070004(3.013727)
Iter 18200 | Time 0.0166(0.0227) | Loss 2.857958(3.016026)
Iter 18300 | Time 0.0224(0.0198) | Loss 3.148110(3.018616)
Iter 18400 | Time 0.0130(0.0200) | Loss 3.064963(3.023156)
Iter 18500 | Time 0.0255(0.0219) | Loss 2.932773(3.026422)
Iter 18600 | Time 0.0149(0.0196) | Loss 3.019441(3.017467)
Iter 18700 | Time 0.0133(0.0193) | Loss 3.051023(3.019434)
Iter 18800 | Time 0.0131(0.0172) | Loss 2.962893(2.997002)
Iter 18900 | Time 0.0168(0.0199) | Loss 3.012375(3.019911)
Iter 19000 | Time 0.0135(0.0167) | Loss 2.939473(3.025327)
[TEST] Iter 19000 | Test Loss 2.971290
Iter 19100 | Time 0.0233(0.0204) | Loss 2.969689(3.011311)
Iter 19200 | Time 0.0212(0.0323) | Loss 3.063931(3.010891)
Iter 19300 | Time 0.0265(0.0285) | Loss 2.961311(3.003384)
Iter 19400 | Time 0.0139(0.0207) | Loss 3.065257(2.996310)
Iter 19500 | Time 0.0160(0.0164) | Loss 2.944291(3.011577)
Iter 19600 | Time 0.0435(0.0164) | Loss 3.011726(3.025143)
Iter 19700 | Time 0.0181(0.0187) | Loss 3.094168(3.026042)
Iter 19800 | Time 0.0182(0.0208) | Loss 2.989354(3.018364)
Iter 19900 | Time 0.0185(0.0181) | Loss 3.019460(2.997668)
Iter 20000 | Time 0.0154(0.0180) | Loss 3.205812(3.013200)
[TEST] Iter 20000 | Test Loss 3.030168
Iter 20100 | Time 0.0154(0.0168) | Loss 2.938896(3.000798)
Iter 20200 | Time 0.0149(0.0168) | Loss 2.949604(3.013524)
Iter 20300 | Time 0.0164(0.0158) | Loss 2.972281(2.984567)
Iter 20400 | Time 0.0229(0.0172) | Loss 2.922031(2.990095)
Iter 20500 | Time 0.0133(0.0169) | Loss 3.012571(2.987072)
Iter 20600 | Time 0.0139(0.0150) | Loss 3.008801(3.012886)
Iter 20700 | Time 0.0174(0.0154) | Loss 2.964390(2.995211)
Iter 20800 | Time 0.0140(0.0148) | Loss 3.096584(3.005169)
Iter 20900 | Time 0.0145(0.0155) | Loss 3.103539(2.975679)
Iter 21000 | Time 0.0149(0.0152) | Loss 3.244659(3.016886)
[TEST] Iter 21000 | Test Loss 2.992754
Iter 21100 | Time 0.0216(0.0219) | Loss 2.892262(3.013370)
Iter 21200 | Time 0.0154(0.0187) | Loss 3.174297(3.004782)
Iter 21300 | Time 0.0126(0.0174) | Loss 3.085639(3.023607)
Iter 21400 | Time 0.0145(0.0216) | Loss 2.949283(2.999671)
Iter 21500 | Time 0.0194(0.0193) | Loss 2.975126(2.996870)
Iter 21600 | Time 0.0150(0.0165) | Loss 3.068375(3.008077)
Iter 21700 | Time 0.0190(0.0161) | Loss 3.032440(2.997348)
Iter 21800 | Time 0.0724(0.0391) | Loss 2.934987(2.991057)
Iter 21900 | Time 0.0160(0.0241) | Loss 2.940115(2.970004)
Iter 22000 | Time 0.0153(0.0174) | Loss 2.817603(3.015303)
[TEST] Iter 22000 | Test Loss 2.944659
Iter 22100 | Time 0.0164(0.0175) | Loss 2.851501(2.994405)
Iter 22200 | Time 0.0158(0.0172) | Loss 2.997030(2.986713)
Iter 22300 | Time 0.0176(0.0166) | Loss 2.982459(2.990686)
Iter 22400 | Time 0.0184(0.0177) | Loss 3.040259(2.999292)
Iter 22500 | Time 0.0196(0.0175) | Loss 2.831097(2.998579)
Iter 22600 | Time 0.0139(0.0163) | Loss 2.810616(2.996073)
Iter 22700 | Time 0.0142(0.0225) | Loss 3.003687(2.991012)
Iter 22800 | Time 0.0146(0.0189) | Loss 2.803472(2.988738)
Iter 22900 | Time 0.0190(0.0163) | Loss 3.093928(2.991458)
Iter 23000 | Time 0.0148(0.0185) | Loss 3.105254(2.991967)
[TEST] Iter 23000 | Test Loss 2.988146
Iter 23100 | Time 0.2162(0.1731) | Loss 2.855831(2.997575)
Iter 23200 | Time 0.0157(0.0398) | Loss 2.754916(2.993830)
Iter 23300 | Time 0.0156(0.0189) | Loss 2.950391(2.979642)
Iter 23400 | Time 0.0143(0.0162) | Loss 2.810451(2.982712)
Iter 23500 | Time 0.0138(0.0176) | Loss 3.050579(2.988834)
Iter 23600 | Time 0.0127(0.0163) | Loss 3.080288(2.994486)
Iter 23700 | Time 0.0153(0.0161) | Loss 3.099504(2.981763)
Iter 23800 | Time 0.0131(0.0157) | Loss 3.110916(2.995168)
Iter 23900 | Time 0.0148(0.0158) | Loss 3.117587(2.990278)
Iter 24000 | Time 0.0208(0.0157) | Loss 3.022139(2.983854)
[TEST] Iter 24000 | Test Loss 2.921281
Iter 24100 | Time 0.0136(0.0153) | Loss 3.202885(2.987726)
Iter 24200 | Time 0.0138(0.0152) | Loss 3.047517(2.974395)
Iter 24300 | Time 0.0163(0.0157) | Loss 3.137309(2.984841)
Iter 24400 | Time 0.0133(0.0158) | Loss 2.967711(3.000152)
Iter 24500 | Time 0.0158(0.0174) | Loss 2.931812(2.992504)
Iter 24600 | Time 0.0154(0.0158) | Loss 2.930771(2.987229)
Iter 24700 | Time 0.0143(0.0151) | Loss 3.090019(2.990775)
Iter 24800 | Time 0.0154(0.0154) | Loss 2.844444(3.004268)
Iter 24900 | Time 0.0140(0.0153) | Loss 2.865800(2.981094)
Iter 25000 | Time 0.0131(0.0161) | Loss 3.301193(2.999190)
[TEST] Iter 25000 | Test Loss 2.978580
Iter 25100 | Time 0.0131(0.0158) | Loss 3.114531(2.976180)
Iter 25200 | Time 0.0126(0.0148) | Loss 2.968436(2.986101)
Iter 25300 | Time 0.0138(0.0148) | Loss 3.003857(2.983900)
Iter 25400 | Time 0.0179(0.0146) | Loss 3.058456(2.974208)
Iter 25500 | Time 0.0121(0.0152) | Loss 2.792845(2.993252)
Iter 25600 | Time 0.0138(0.0148) | Loss 3.046121(2.989123)
Iter 25700 | Time 0.0144(0.0150) | Loss 2.913690(2.991601)
Iter 25800 | Time 0.0150(0.0150) | Loss 2.817183(2.987377)
Iter 25900 | Time 0.0154(0.0159) | Loss 3.110929(2.985886)
Iter 26000 | Time 0.0144(0.0148) | Loss 3.243409(2.979864)
[TEST] Iter 26000 | Test Loss 2.993246
Iter 26100 | Time 0.0137(0.0148) | Loss 2.859144(2.964842)
Iter 26200 | Time 0.0138(0.0153) | Loss 3.005196(2.990202)
Iter 26300 | Time 0.0138(0.0148) | Loss 2.908125(2.966278)
Iter 26400 | Time 0.0135(0.0147) | Loss 2.844337(2.962880)
Iter 26500 | Time 0.0130(0.0150) | Loss 2.991264(2.994967)
Iter 26600 | Time 0.0136(0.0149) | Loss 3.074037(2.982580)
Iter 26700 | Time 0.0143(0.0148) | Loss 2.945035(2.992853)
Iter 26800 | Time 0.0149(0.0146) | Loss 2.965544(2.983194)
Iter 26900 | Time 0.0139(0.0153) | Loss 2.879037(2.985627)
Iter 27000 | Time 0.0153(0.0158) | Loss 3.042991(2.981816)
[TEST] Iter 27000 | Test Loss 2.992406
Iter 27100 | Time 0.0177(0.0179) | Loss 2.947565(2.969456)
Iter 27200 | Time 0.0144(0.0157) | Loss 2.879345(2.975192)
Iter 27300 | Time 0.0135(0.0152) | Loss 2.840840(2.974006)
Iter 27400 | Time 0.0180(0.0152) | Loss 2.971287(2.994228)
Iter 27500 | Time 0.0136(0.0150) | Loss 3.010891(2.978938)
Iter 27600 | Time 0.0147(0.0162) | Loss 3.051583(2.994693)
Iter 27700 | Time 0.0159(0.0178) | Loss 3.202351(2.976581)
Iter 27800 | Time 0.0204(0.0288) | Loss 2.986169(2.993412)
Iter 27900 | Time 0.0143(0.0197) | Loss 2.915705(2.990753)
Iter 28000 | Time 0.0132(0.0156) | Loss 2.996659(2.979248)
[TEST] Iter 28000 | Test Loss 2.995595
Iter 28100 | Time 0.0169(0.0165) | Loss 2.836632(2.965569)
Iter 28200 | Time 0.0176(0.0159) | Loss 3.034538(2.974160)
Iter 28300 | Time 0.0135(0.0148) | Loss 3.231358(2.981871)
Iter 28400 | Time 0.0148(0.0155) | Loss 2.983022(2.982221)
Iter 28500 | Time 0.0133(0.0163) | Loss 2.984394(2.968480)
Iter 28600 | Time 0.0148(0.0161) | Loss 2.956425(2.964149)
Iter 28700 | Time 0.0138(0.0231) | Loss 3.088873(2.978252)
Iter 28800 | Time 0.0645(0.0263) | Loss 3.027201(2.971979)
Iter 28900 | Time 0.0174(0.0175) | Loss 2.778724(2.971961)
Iter 29000 | Time 0.0139(0.0184) | Loss 2.818577(2.974685)
[TEST] Iter 29000 | Test Loss 2.947844
Iter 29100 | Time 0.0159(0.0203) | Loss 3.054912(2.977066)
Iter 29200 | Time 0.0129(0.0177) | Loss 2.938426(2.971653)
Iter 29300 | Time 0.0851(0.0231) | Loss 2.856724(2.970916)
Iter 29400 | Time 0.0159(0.0227) | Loss 2.930077(2.962324)
Iter 29500 | Time 0.0122(0.0169) | Loss 2.997731(2.971920)
Iter 29600 | Time 0.0162(0.0166) | Loss 2.849316(2.950682)
Iter 29700 | Time 0.0133(0.0173) | Loss 3.048265(2.977205)
Iter 29800 | Time 0.0165(0.0219) | Loss 3.166140(2.970960)
Iter 29900 | Time 0.0170(0.0178) | Loss 2.919262(2.974395)
Iter 30000 | Time 0.0191(0.0188) | Loss 3.086837(2.987921)
[TEST] Iter 30000 | Test Loss 2.983350
Iter 30100 | Time 0.0149(0.0150) | Loss 3.071171(2.970121)
Iter 30200 | Time 0.0147(0.0156) | Loss 3.022977(2.971901)
Iter 30300 | Time 0.0119(0.0149) | Loss 3.163999(2.997880)
Iter 30400 | Time 0.0157(0.0158) | Loss 2.925488(2.975972)
Iter 30500 | Time 0.0191(0.0190) | Loss 2.817060(2.964614)
Iter 30600 | Time 0.0136(0.0189) | Loss 2.901065(2.953798)
Iter 30700 | Time 0.0161(0.0163) | Loss 3.322501(2.969619)
Iter 30800 | Time 0.0167(0.0192) | Loss 2.970054(2.984493)
Iter 30900 | Time 0.0155(0.0165) | Loss 2.932135(2.966462)
Iter 31000 | Time 0.0168(0.0155) | Loss 2.908481(2.975763)
[TEST] Iter 31000 | Test Loss 2.932933
Iter 31100 | Time 0.0185(0.0149) | Loss 3.037554(2.962421)
Iter 31200 | Time 0.0171(0.0149) | Loss 2.829136(2.953119)
Iter 31300 | Time 0.0168(0.0148) | Loss 3.153898(2.950445)
Iter 31400 | Time 0.0172(0.0144) | Loss 2.951971(2.966300)
Iter 31500 | Time 0.0151(0.0149) | Loss 2.853526(2.958115)
Iter 31600 | Time 0.0184(0.0162) | Loss 3.052024(2.949975)
Iter 31700 | Time 0.0192(0.0163) | Loss 2.841404(2.975806)
Iter 31800 | Time 0.0144(0.0179) | Loss 3.038304(2.978393)
Iter 31900 | Time 0.0171(0.0167) | Loss 2.995322(2.964794)
Iter 32000 | Time 0.0159(0.0176) | Loss 2.922723(2.963918)
[TEST] Iter 32000 | Test Loss 3.011179
Iter 32100 | Time 0.0133(0.0153) | Loss 3.181924(2.967804)
Iter 32200 | Time 0.0153(0.0150) | Loss 3.060255(2.964457)
Iter 32300 | Time 0.0231(0.0185) | Loss 2.946080(2.963569)
Iter 32400 | Time 0.0153(0.0169) | Loss 2.929088(2.961201)
Iter 32500 | Time 0.0142(0.0162) | Loss 3.016744(2.977611)
Iter 32600 | Time 0.0142(0.0151) | Loss 2.970628(2.966644)
Iter 32700 | Time 0.0177(0.0150) | Loss 2.994205(2.976671)
Iter 32800 | Time 0.0145(0.0147) | Loss 3.049563(2.969368)
Iter 32900 | Time 0.0161(0.0164) | Loss 2.993012(2.953767)
Iter 33000 | Time 0.0211(0.0206) | Loss 2.840312(2.963600)
[TEST] Iter 33000 | Test Loss 2.949138
Iter 33100 | Time 0.0141(0.0174) | Loss 3.039211(2.957699)
Iter 33200 | Time 0.0136(0.0159) | Loss 2.934984(2.954917)
Iter 33300 | Time 0.0155(0.0153) | Loss 2.929399(2.947832)
Iter 33400 | Time 0.0175(0.0157) | Loss 3.011973(2.978971)
Iter 33500 | Time 0.0154(0.0160) | Loss 2.946981(2.965284)
Iter 33600 | Time 0.0129(0.0153) | Loss 3.002886(2.965009)
Iter 33700 | Time 0.0157(0.0151) | Loss 2.970559(2.979006)
Iter 33800 | Time 0.0142(0.0152) | Loss 3.092759(2.961955)
Iter 33900 | Time 0.0163(0.0152) | Loss 2.966529(2.956588)
Iter 34000 | Time 0.0168(0.0169) | Loss 3.078546(2.968447)
[TEST] Iter 34000 | Test Loss 2.914819
Iter 34100 | Time 0.0137(0.0151) | Loss 2.879646(2.955647)
Iter 34200 | Time 0.0146(0.0162) | Loss 2.941873(2.954168)
Iter 34300 | Time 0.0155(0.0168) | Loss 3.004993(2.953865)
Iter 34400 | Time 0.0157(0.0165) | Loss 2.806366(2.949666)
Iter 34500 | Time 0.0184(0.0173) | Loss 2.987504(2.965268)
Iter 34600 | Time 0.0144(0.0165) | Loss 2.980582(2.981372)
Iter 34700 | Time 0.0134(0.0166) | Loss 2.911336(2.961485)
Iter 34800 | Time 0.0141(0.0151) | Loss 3.016886(2.963982)
Iter 34900 | Time 0.0133(0.0146) | Loss 3.016733(2.964490)
Iter 35000 | Time 0.0135(0.0146) | Loss 3.108047(2.963766)
[TEST] Iter 35000 | Test Loss 3.006241
Iter 35100 | Time 0.0134(0.0147) | Loss 3.085430(2.963464)
Iter 35200 | Time 0.0142(0.0150) | Loss 2.942213(2.963113)
Iter 35300 | Time 0.0171(0.0147) | Loss 3.017555(2.955156)
Iter 35400 | Time 0.0167(0.0148) | Loss 3.256039(2.957726)
Iter 35500 | Time 0.0145(0.0148) | Loss 2.742277(2.963483)
Iter 35600 | Time 0.0130(0.0147) | Loss 2.807851(2.946005)
Iter 35700 | Time 0.0184(0.0147) | Loss 3.010755(2.977071)
Iter 35800 | Time 0.0141(0.0148) | Loss 2.973197(2.960992)
Iter 35900 | Time 0.0138(0.0150) | Loss 3.140829(2.955439)
Iter 36000 | Time 0.0174(0.0152) | Loss 2.819413(2.951277)
[TEST] Iter 36000 | Test Loss 2.976232
Iter 36100 | Time 0.0156(0.0175) | Loss 2.794600(2.966375)
Iter 36200 | Time 0.0123(0.0156) | Loss 2.964889(2.954034)
Iter 36300 | Time 0.0149(0.0147) | Loss 2.969257(2.962574)
Iter 36400 | Time 0.0138(0.0147) | Loss 3.069121(2.955920)
Iter 36500 | Time 0.0155(0.0147) | Loss 2.777278(2.960286)
Iter 36600 | Time 0.0152(0.0146) | Loss 2.968361(2.960231)
Iter 36700 | Time 0.0164(0.0148) | Loss 2.926075(2.958949)
Iter 36800 | Time 0.0154(0.0151) | Loss 2.827856(2.953381)
Iter 36900 | Time 0.0144(0.0149) | Loss 2.894925(2.965090)
Iter 37000 | Time 0.0145(0.0148) | Loss 3.006721(2.931453)
[TEST] Iter 37000 | Test Loss 2.944618
Iter 37100 | Time 0.0150(0.0148) | Loss 2.780583(2.945053)
Iter 37200 | Time 0.0145(0.0147) | Loss 2.952223(2.954348)
Iter 37300 | Time 0.0135(0.0148) | Loss 2.914239(2.949672)
Iter 37400 | Time 0.0136(0.0145) | Loss 2.949367(2.962508)
Iter 37500 | Time 0.0131(0.0143) | Loss 2.844594(2.952077)
Iter 37600 | Time 0.0308(0.0168) | Loss 3.009916(2.955038)
Iter 37700 | Time 0.0149(0.0150) | Loss 3.007015(2.954657)
Iter 37800 | Time 0.0144(0.0165) | Loss 2.930283(2.945659)
Iter 37900 | Time 0.0133(0.0154) | Loss 3.164908(2.967716)
Iter 38000 | Time 0.0140(0.0157) | Loss 2.775154(2.941834)
[TEST] Iter 38000 | Test Loss 2.918588
Iter 38100 | Time 0.0127(0.0153) | Loss 3.175288(2.971675)
Iter 38200 | Time 0.0142(0.0144) | Loss 3.037066(2.944497)
Iter 38300 | Time 0.0129(0.0145) | Loss 2.926373(2.958934)
Iter 38400 | Time 0.0160(0.0146) | Loss 2.835755(2.941464)
Iter 38500 | Time 0.0163(0.0148) | Loss 2.717949(2.955223)
Iter 38600 | Time 0.0156(0.0150) | Loss 2.953140(2.955801)
Iter 38700 | Time 0.0150(0.0151) | Loss 3.056616(2.954445)
Iter 38800 | Time 0.0151(0.0148) | Loss 2.809052(2.951910)
Iter 38900 | Time 0.0185(0.0154) | Loss 2.865890(2.963004)
Iter 39000 | Time 0.0133(0.0160) | Loss 3.130060(2.956960)
[TEST] Iter 39000 | Test Loss 2.932786
Iter 39100 | Time 0.0153(0.0152) | Loss 3.018112(2.948176)
Iter 39200 | Time 0.0158(0.0155) | Loss 3.135998(2.962449)
Iter 39300 | Time 0.0157(0.0167) | Loss 2.886135(2.956767)
Iter 39400 | Time 0.0146(0.0153) | Loss 2.994013(2.960388)
Iter 39500 | Time 0.0152(0.0147) | Loss 3.171500(2.954165)
Iter 39600 | Time 0.0154(0.0179) | Loss 2.800597(2.948264)
Iter 39700 | Time 0.0144(0.0173) | Loss 2.824405(2.937462)
Iter 39800 | Time 0.0133(0.0153) | Loss 2.974518(2.944596)
Iter 39900 | Time 0.0137(0.0145) | Loss 3.046442(2.956136)
Iter 40000 | Time 0.0136(0.0144) | Loss 2.916132(2.947757)
[TEST] Iter 40000 | Test Loss 3.023343
Iter 40100 | Time 0.0144(0.0151) | Loss 3.127371(2.942492)
Iter 40200 | Time 0.0129(0.0148) | Loss 3.017912(2.954839)
Iter 40300 | Time 0.0141(0.0143) | Loss 2.755865(2.940442)
Iter 40400 | Time 0.0150(0.0155) | Loss 2.945850(2.927409)
Iter 40500 | Time 0.0138(0.0188) | Loss 3.160945(2.939415)
Iter 40600 | Time 0.0141(0.0156) | Loss 2.840073(2.942525)
Iter 40700 | Time 0.0160(0.0150) | Loss 2.889812(2.941861)
Iter 40800 | Time 0.0144(0.0146) | Loss 2.950609(2.948894)
Iter 40900 | Time 0.0176(0.0151) | Loss 3.108976(2.954256)
Iter 41000 | Time 0.0173(0.0164) | Loss 2.963844(2.943060)
[TEST] Iter 41000 | Test Loss 2.972131
Iter 41100 | Time 0.0134(0.0166) | Loss 3.010485(2.948858)
Iter 41200 | Time 0.0148(0.0150) | Loss 2.927299(2.960666)
Iter 41300 | Time 0.0141(0.0147) | Loss 3.005358(2.933896)
Iter 41400 | Time 0.0143(0.0145) | Loss 3.108320(2.952597)
Iter 41500 | Time 0.0139(0.0143) | Loss 2.853628(2.944773)
Iter 41600 | Time 0.0149(0.0148) | Loss 3.068743(2.958086)
Iter 41700 | Time 0.0209(0.0148) | Loss 2.992236(2.950432)
Iter 41800 | Time 0.0152(0.0145) | Loss 3.030724(2.939712)
Iter 41900 | Time 0.0130(0.0170) | Loss 2.964693(2.937296)
Iter 42000 | Time 0.0139(0.0145) | Loss 2.786798(2.947105)
[TEST] Iter 42000 | Test Loss 2.972551
Iter 42100 | Time 0.0135(0.0145) | Loss 3.177478(2.954099)
Iter 42200 | Time 0.0148(0.0146) | Loss 2.868276(2.940888)
Iter 42300 | Time 0.0139(0.0148) | Loss 3.027882(2.940054)
Iter 42400 | Time 0.0150(0.0146) | Loss 2.938444(2.930152)
Iter 42500 | Time 0.0144(0.0151) | Loss 2.935119(2.964630)
Iter 42600 | Time 0.0153(0.0150) | Loss 2.961508(2.932449)
Iter 42700 | Time 0.0135(0.0144) | Loss 2.930384(2.935515)
Iter 42800 | Time 0.0140(0.0145) | Loss 2.826812(2.954342)
Iter 42900 | Time 0.0147(0.0160) | Loss 2.966689(2.925732)
Iter 43000 | Time 0.0165(0.0155) | Loss 2.877027(2.929734)
[TEST] Iter 43000 | Test Loss 2.949056
Iter 43100 | Time 0.0137(0.0198) | Loss 2.740434(2.923326)
Iter 43200 | Time 0.0136(0.0166) | Loss 2.998992(2.946001)
Iter 43300 | Time 0.0138(0.0193) | Loss 2.973463(2.940587)
Iter 43400 | Time 0.0130(0.0162) | Loss 3.048754(2.945614)
Iter 43500 | Time 0.0144(0.0155) | Loss 2.871336(2.951443)
Iter 43600 | Time 0.0184(0.0161) | Loss 2.976432(2.928413)
Iter 43700 | Time 0.0154(0.0212) | Loss 2.894653(2.928280)
Iter 43800 | Time 0.0155(0.0194) | Loss 2.960405(2.948862)
Iter 43900 | Time 0.0147(0.0173) | Loss 2.772324(2.941315)
Iter 44000 | Time 0.0164(0.0178) | Loss 3.013520(2.953572)
[TEST] Iter 44000 | Test Loss 2.928558
Iter 44100 | Time 0.0151(0.0167) | Loss 2.832465(2.929095)
Iter 44200 | Time 0.0153(0.0157) | Loss 2.931733(2.933639)
Iter 44300 | Time 0.0158(0.0171) | Loss 2.831510(2.926206)
Iter 44400 | Time 0.0152(0.0164) | Loss 2.885571(2.934285)
Iter 44500 | Time 0.0141(0.0174) | Loss 2.846555(2.937179)
Iter 44600 | Time 0.0151(0.0167) | Loss 3.123225(2.953505)
Iter 44700 | Time 0.0143(0.0158) | Loss 2.895756(2.928913)
Iter 44800 | Time 0.0144(0.0152) | Loss 2.925833(2.945464)
Iter 44900 | Time 0.0159(0.0156) | Loss 3.147625(2.943676)
Iter 45000 | Time 0.0153(0.0152) | Loss 3.044195(2.924850)
[TEST] Iter 45000 | Test Loss 2.940468
Iter 45100 | Time 0.0148(0.0161) | Loss 2.891413(2.944512)
Iter 45200 | Time 0.0130(0.0148) | Loss 2.913762(2.938704)
Iter 45300 | Time 0.0160(0.0152) | Loss 2.968045(2.944994)
Iter 45400 | Time 0.0148(0.0150) | Loss 3.031611(2.946253)
Iter 45500 | Time 0.0165(0.0152) | Loss 2.974562(2.944952)
Iter 45600 | Time 0.0150(0.0166) | Loss 3.012579(2.929205)
Iter 45700 | Time 0.0155(0.0149) | Loss 3.100744(2.944527)
Iter 45800 | Time 0.0198(0.0149) | Loss 3.092727(2.939040)
Iter 45900 | Time 0.0148(0.0147) | Loss 2.932572(2.949216)
Iter 46000 | Time 0.0118(0.0149) | Loss 2.793709(2.920299)
[TEST] Iter 46000 | Test Loss 2.930758
Iter 46100 | Time 0.0131(0.0145) | Loss 2.887046(2.940991)
Iter 46200 | Time 0.0144(0.0149) | Loss 2.985084(2.937848)
Iter 46300 | Time 0.0137(0.0148) | Loss 2.830137(2.927711)
Iter 46400 | Time 0.0147(0.0146) | Loss 2.957059(2.928540)
Iter 46500 | Time 0.0164(0.0143) | Loss 2.746141(2.934084)
Iter 46600 | Time 0.0181(0.0162) | Loss 3.105214(2.946445)
Iter 46700 | Time 0.0322(0.0203) | Loss 2.896882(2.940196)
Iter 46800 | Time 0.0141(0.0179) | Loss 2.870522(2.949349)
Iter 46900 | Time 0.0132(0.0158) | Loss 2.822644(2.921172)
Iter 47000 | Time 0.0141(0.0149) | Loss 2.829177(2.926564)
[TEST] Iter 47000 | Test Loss 2.930463
Iter 47100 | Time 0.0138(0.0155) | Loss 2.979454(2.946937)
Iter 47200 | Time 0.0147(0.0149) | Loss 2.923935(2.938310)
Iter 47300 | Time 0.0141(0.0147) | Loss 2.985071(2.948401)
Iter 47400 | Time 0.0163(0.0148) | Loss 3.191200(2.942716)
Iter 47500 | Time 0.0160(0.0149) | Loss 2.986520(2.931641)
Iter 47600 | Time 0.0144(0.0145) | Loss 2.799091(2.928762)
Iter 47700 | Time 0.0151(0.0152) | Loss 3.082195(2.960845)
Iter 47800 | Time 0.0135(0.0146) | Loss 2.974169(2.931874)
Iter 47900 | Time 0.0130(0.0149) | Loss 2.883725(2.937529)
Iter 48000 | Time 0.0128(0.0146) | Loss 2.800667(2.918236)
[TEST] Iter 48000 | Test Loss 2.953170
Iter 48100 | Time 0.0162(0.0184) | Loss 2.895074(2.951057)
Iter 48200 | Time 0.0146(0.0151) | Loss 3.009291(2.932286)
Iter 48300 | Time 0.0165(0.0180) | Loss 2.828325(2.932198)
Iter 48400 | Time 0.0139(0.0159) | Loss 2.997409(2.939976)
Iter 48500 | Time 0.0227(0.0179) | Loss 2.867254(2.937552)
Iter 48600 | Time 0.0149(0.0167) | Loss 2.841722(2.964691)
Iter 48700 | Time 0.0166(0.0193) | Loss 2.859610(2.933605)
Iter 48800 | Time 0.0161(0.0179) | Loss 2.842642(2.936968)
Iter 48900 | Time 0.0150(0.0161) | Loss 2.836827(2.948204)
Iter 49000 | Time 0.0156(0.0154) | Loss 2.965227(2.941770)
[TEST] Iter 49000 | Test Loss 2.980233
Iter 49100 | Time 0.0154(0.0183) | Loss 2.848367(2.939590)
Iter 49200 | Time 0.0138(0.0149) | Loss 2.873281(2.920772)
Iter 49300 | Time 0.0143(0.0149) | Loss 3.183768(2.939314)
Iter 49400 | Time 0.0144(0.0150) | Loss 2.986969(2.940656)
Iter 49500 | Time 0.0142(0.0160) | Loss 2.847548(2.938368)
Iter 49600 | Time 0.0117(0.0147) | Loss 2.943469(2.945756)
Iter 49700 | Time 0.0138(0.0152) | Loss 2.884881(2.938001)
Iter 49800 | Time 0.0130(0.0153) | Loss 2.852571(2.937791)
Iter 49900 | Time 0.0132(0.0154) | Loss 2.862139(2.938739)
Iter 50000 | Time 0.0127(0.0144) | Loss 3.009507(2.923474)
[TEST] Iter 50000 | Test Loss 2.939368
Iter 50100 | Time 0.0146(0.0143) | Loss 2.928511(2.917270)
Iter 50200 | Time 0.0141(0.0145) | Loss 2.934239(2.940753)
Iter 50300 | Time 0.0130(0.0145) | Loss 2.871707(2.923821)
Iter 50400 | Time 0.0139(0.0143) | Loss 2.840752(2.925480)
Iter 50500 | Time 0.0134(0.0146) | Loss 2.850062(2.936945)
Iter 50600 | Time 0.0145(0.0142) | Loss 3.057171(2.947772)
Iter 50700 | Time 0.0142(0.0144) | Loss 2.970253(2.931013)
Iter 50800 | Time 0.0156(0.0145) | Loss 2.817924(2.941001)
Iter 50900 | Time 0.0165(0.0145) | Loss 2.889031(2.926961)
Iter 51000 | Time 0.0139(0.0145) | Loss 2.856915(2.929989)
[TEST] Iter 51000 | Test Loss 2.914503
Iter 51100 | Time 0.0191(0.0182) | Loss 2.689864(2.937421)
Iter 51200 | Time 0.0168(0.0168) | Loss 3.060061(2.951708)
Iter 51300 | Time 0.0272(0.0184) | Loss 2.891778(2.943813)
Iter 51400 | Time 0.0335(0.0171) | Loss 3.015425(2.929410)
Iter 51500 | Time 0.0310(0.0198) | Loss 2.857954(2.935128)
Iter 51600 | Time 0.0147(0.0153) | Loss 2.811180(2.926636)
Iter 51700 | Time 0.0180(0.0153) | Loss 3.144861(2.934380)
Iter 51800 | Time 0.0130(0.0179) | Loss 3.058869(2.929133)
Iter 51900 | Time 0.0158(0.0163) | Loss 2.931004(2.951337)
Iter 52000 | Time 0.0203(0.0170) | Loss 2.872547(2.924419)
[TEST] Iter 52000 | Test Loss 2.897407
Iter 52100 | Time 0.0175(0.0172) | Loss 2.904662(2.951986)
Iter 52200 | Time 0.0156(0.0169) | Loss 2.918530(2.947933)
Iter 52300 | Time 0.0142(0.0150) | Loss 2.869839(2.932669)
Iter 52400 | Time 0.0130(0.0150) | Loss 2.790450(2.924565)
Iter 52500 | Time 0.0134(0.0153) | Loss 2.837848(2.940620)
Iter 52600 | Time 0.0192(0.0148) | Loss 3.002702(2.934447)
Iter 52700 | Time 0.0159(0.0145) | Loss 3.024871(2.933011)
Iter 52800 | Time 0.0275(0.0167) | Loss 2.967046(2.915334)
Iter 52900 | Time 0.0133(0.0180) | Loss 2.839105(2.931383)
Iter 53000 | Time 0.0142(0.0177) | Loss 2.830733(2.934147)
[TEST] Iter 53000 | Test Loss 2.880249
Iter 53100 | Time 0.0192(0.0186) | Loss 2.976458(2.950793)
Iter 53200 | Time 0.0159(0.0175) | Loss 2.981871(2.934825)
Iter 53300 | Time 0.0189(0.0160) | Loss 2.893090(2.936103)
Iter 53400 | Time 0.0163(0.0174) | Loss 2.870937(2.925439)
Iter 53500 | Time 0.0149(0.0156) | Loss 3.011791(2.932204)
Iter 53600 | Time 0.0145(0.0151) | Loss 3.091045(2.940903)
Iter 53700 | Time 0.0143(0.0162) | Loss 2.928525(2.943194)
Iter 53800 | Time 0.0177(0.0197) | Loss 2.944329(2.931062)
Iter 53900 | Time 0.0158(0.0156) | Loss 2.897370(2.935928)
Iter 54000 | Time 0.0148(0.0159) | Loss 2.931649(2.934810)
[TEST] Iter 54000 | Test Loss 2.972565
Iter 54100 | Time 0.0143(0.0162) | Loss 3.003374(2.938887)
Iter 54200 | Time 0.0166(0.0150) | Loss 2.825021(2.936551)
Iter 54300 | Time 0.0157(0.0150) | Loss 2.953022(2.942933)
Iter 54400 | Time 0.0141(0.0152) | Loss 2.961135(2.942777)
Iter 54500 | Time 0.0161(0.0150) | Loss 3.014194(2.934936)
Iter 54600 | Time 0.0156(0.0149) | Loss 2.933686(2.918477)
Iter 54700 | Time 0.0136(0.0148) | Loss 2.976355(2.947970)
Iter 54800 | Time 0.0160(0.0152) | Loss 2.751872(2.928719)
Iter 54900 | Time 0.0158(0.0149) | Loss 2.978123(2.925027)
Iter 55000 | Time 0.0163(0.0148) | Loss 3.026286(2.933681)
[TEST] Iter 55000 | Test Loss 2.908039
Iter 55100 | Time 0.0143(0.0145) | Loss 3.042075(2.926653)
Iter 55200 | Time 0.0137(0.0148) | Loss 2.928821(2.935890)
Iter 55300 | Time 0.0168(0.0154) | Loss 2.916273(2.932729)
Iter 55400 | Time 0.0130(0.0177) | Loss 2.728686(2.933779)
Iter 55500 | Time 0.0133(0.0168) | Loss 2.964178(2.930572)
Iter 55600 | Time 0.0143(0.0154) | Loss 3.131157(2.938019)
Iter 55700 | Time 0.0156(0.0226) | Loss 2.835696(2.916715)
Iter 55800 | Time 0.0183(0.0182) | Loss 2.936584(2.921534)
Iter 55900 | Time 0.0219(0.0175) | Loss 2.843735(2.903429)
Iter 56000 | Time 0.0222(0.0187) | Loss 3.100272(2.927761)
[TEST] Iter 56000 | Test Loss 2.928885
Iter 56100 | Time 0.0167(0.0172) | Loss 2.808761(2.929585)
Iter 56200 | Time 0.0145(0.0189) | Loss 2.928749(2.946943)
Iter 56300 | Time 0.0212(0.0388) | Loss 3.112528(2.944533)
Iter 56400 | Time 0.0312(0.0229) | Loss 2.865226(2.918275)
Iter 56500 | Time 0.0129(0.0172) | Loss 2.864512(2.906195)
Iter 56600 | Time 0.0203(0.0187) | Loss 3.047554(2.931143)
Iter 56700 | Time 0.0165(0.0175) | Loss 3.108119(2.932950)
Iter 56800 | Time 0.0153(0.0186) | Loss 2.897511(2.928538)
Iter 56900 | Time 0.0142(0.0160) | Loss 2.900393(2.926291)
Iter 57000 | Time 0.0150(0.0152) | Loss 2.914634(2.908322)
[TEST] Iter 57000 | Test Loss 2.980951
Iter 57100 | Time 0.0134(0.0188) | Loss 2.868598(2.921869)
Iter 57200 | Time 0.0181(0.0184) | Loss 2.713274(2.939573)
Iter 57300 | Time 0.0163(0.0281) | Loss 2.872905(2.915007)
Iter 57400 | Time 0.0126(0.0185) | Loss 2.940438(2.934899)
Iter 57500 | Time 0.0159(0.0207) | Loss 2.975995(2.934364)
Iter 57600 | Time 0.0135(0.0211) | Loss 3.054463(2.929112)
Iter 57700 | Time 0.0127(0.0220) | Loss 2.903522(2.940060)
Iter 57800 | Time 0.0151(0.0193) | Loss 2.956913(2.929381)
Iter 57900 | Time 0.0387(0.0200) | Loss 2.860288(2.937574)
Iter 58000 | Time 0.0136(0.0206) | Loss 2.854084(2.927630)
[TEST] Iter 58000 | Test Loss 2.953950
Iter 58100 | Time 0.0147(0.0201) | Loss 2.897331(2.941017)
Iter 58200 | Time 0.0162(0.0164) | Loss 2.874939(2.950359)
Iter 58300 | Time 0.0145(0.0209) | Loss 2.895566(2.933161)
Iter 58400 | Time 0.0197(0.0172) | Loss 2.739359(2.923972)
Iter 58500 | Time 0.0129(0.0153) | Loss 3.009310(2.930345)
Iter 58600 | Time 0.0136(0.0149) | Loss 2.886191(2.940963)
Iter 58700 | Time 0.0136(0.0165) | Loss 2.853997(2.944850)
Iter 58800 | Time 0.0160(0.0153) | Loss 2.999741(2.948835)
Iter 58900 | Time 0.0153(0.0150) | Loss 2.845130(2.931985)
Iter 59000 | Time 0.0140(0.0154) | Loss 3.089531(2.926661)
[TEST] Iter 59000 | Test Loss 2.952277
Iter 59100 | Time 0.0206(0.0183) | Loss 2.884755(2.945179)
Iter 59200 | Time 0.0248(0.0217) | Loss 3.060017(2.927578)
Iter 59300 | Time 0.0172(0.0177) | Loss 3.013228(2.928036)
Iter 59400 | Time 0.0147(0.0155) | Loss 2.972222(2.952208)
Iter 59500 | Time 0.0129(0.0169) | Loss 2.993661(2.937659)
Iter 59600 | Time 0.0182(0.0173) | Loss 3.068574(2.920397)
Iter 59700 | Time 0.0154(0.0155) | Loss 2.985248(2.936306)
Iter 59800 | Time 0.0159(0.0150) | Loss 2.906441(2.928293)
Iter 59900 | Time 0.0143(0.0151) | Loss 2.902573(2.936346)
Iter 60000 | Time 0.0134(0.0179) | Loss 2.987641(2.930051)
[TEST] Iter 60000 | Test Loss 2.893775
Iter 60100 | Time 0.0142(0.0167) | Loss 2.982642(2.943210)
Iter 60200 | Time 0.0153(0.0152) | Loss 3.065368(2.936251)
Iter 60300 | Time 0.0143(0.0148) | Loss 2.855345(2.931889)
Iter 60400 | Time 0.0164(0.0148) | Loss 3.129298(2.943441)
Iter 60500 | Time 0.0161(0.0151) | Loss 2.832327(2.929121)
Iter 60600 | Time 0.0183(0.0152) | Loss 2.802075(2.920148)
Iter 60700 | Time 0.0165(0.0150) | Loss 2.836766(2.924265)
Iter 60800 | Time 0.0131(0.0150) | Loss 2.969907(2.916008)
Iter 60900 | Time 0.0135(0.0151) | Loss 2.909399(2.914281)
Iter 61000 | Time 0.0148(0.0156) | Loss 2.994172(2.919685)
[TEST] Iter 61000 | Test Loss 2.918630
Iter 61100 | Time 0.0234(0.0172) | Loss 2.970422(2.930500)
Iter 61200 | Time 0.0171(0.0207) | Loss 2.964048(2.930517)
Iter 61300 | Time 0.0140(0.0173) | Loss 3.085847(2.949357)
Iter 61400 | Time 0.0190(0.0180) | Loss 3.057340(2.925450)
Iter 61500 | Time 0.0157(0.0188) | Loss 2.892439(2.948254)
Iter 61600 | Time 0.0134(0.0184) | Loss 2.928803(2.946087)
Iter 61700 | Time 0.0151(0.0158) | Loss 2.887892(2.923118)
Iter 61800 | Time 0.0158(0.0154) | Loss 2.773731(2.937516)
Iter 61900 | Time 0.0132(0.0154) | Loss 2.765205(2.937334)
Iter 62000 | Time 0.0141(0.0153) | Loss 2.844830(2.935869)
[TEST] Iter 62000 | Test Loss 2.964612
Iter 62100 | Time 0.0159(0.0189) | Loss 2.988764(2.939256)
Iter 62200 | Time 0.0130(0.0173) | Loss 3.119365(2.953866)
Iter 62300 | Time 0.0176(0.0168) | Loss 2.742675(2.917067)
Iter 62400 | Time 0.0218(0.0165) | Loss 2.922451(2.912977)
Iter 62500 | Time 0.0210(0.0193) | Loss 2.924000(2.922449)
Iter 62600 | Time 0.0160(0.0165) | Loss 2.832671(2.915561)
Iter 62700 | Time 0.0133(0.0152) | Loss 2.912004(2.920783)
Iter 62800 | Time 0.0153(0.0151) | Loss 2.986518(2.912534)
Iter 62900 | Time 0.0244(0.0159) | Loss 3.065783(2.907624)
Iter 63000 | Time 0.0158(0.0146) | Loss 2.885567(2.944204)
[TEST] Iter 63000 | Test Loss 2.923358
Iter 63100 | Time 0.0152(0.0151) | Loss 2.841887(2.940159)
Iter 63200 | Time 0.0142(0.0154) | Loss 2.999166(2.927998)
Iter 63300 | Time 0.0173(0.0175) | Loss 3.001091(2.931714)
Iter 63400 | Time 0.0126(0.0177) | Loss 2.890146(2.932875)
Iter 63500 | Time 0.0138(0.0166) | Loss 2.911870(2.914510)
Iter 63600 | Time 0.0176(0.0213) | Loss 2.754775(2.928083)
Iter 63700 | Time 0.0189(0.0189) | Loss 2.893579(2.925087)
Iter 63800 | Time 0.0142(0.0160) | Loss 2.915467(2.922325)
Iter 63900 | Time 0.0175(0.0161) | Loss 2.832733(2.919758)
Iter 64000 | Time 0.0157(0.0161) | Loss 3.065796(2.917788)
[TEST] Iter 64000 | Test Loss 2.916105
Iter 64100 | Time 0.0173(0.0159) | Loss 3.052112(2.922216)
Iter 64200 | Time 0.0153(0.0203) | Loss 3.181445(2.929916)
Iter 64300 | Time 0.0153(0.0155) | Loss 2.795218(2.938082)
Iter 64400 | Time 0.0158(0.0149) | Loss 3.020809(2.939260)
Iter 64500 | Time 0.0139(0.0145) | Loss 3.150809(2.932384)
Iter 64600 | Time 0.0133(0.0150) | Loss 3.085095(2.924556)
Iter 64700 | Time 0.0129(0.0154) | Loss 2.802599(2.927805)
Iter 64800 | Time 0.0150(0.0146) | Loss 2.850902(2.909540)
Iter 64900 | Time 0.0146(0.0147) | Loss 2.941358(2.904973)
Iter 65000 | Time 0.0134(0.0157) | Loss 3.015490(2.934325)
[TEST] Iter 65000 | Test Loss 2.912509
Iter 65100 | Time 0.0136(0.0150) | Loss 2.955228(2.915392)
Iter 65200 | Time 0.0133(0.0148) | Loss 3.047339(2.927115)
Iter 65300 | Time 0.0147(0.0146) | Loss 2.993999(2.940186)
Iter 65400 | Time 0.0164(0.0154) | Loss 2.969219(2.932964)
Iter 65500 | Time 0.0146(0.0156) | Loss 3.021851(2.925578)
Iter 65600 | Time 0.0130(0.0150) | Loss 2.753345(2.917503)
Iter 65700 | Time 0.0146(0.0146) | Loss 2.828647(2.918322)
Iter 65800 | Time 0.0143(0.0174) | Loss 3.004310(2.932880)
Iter 65900 | Time 0.0144(0.0154) | Loss 3.015573(2.944710)
Iter 66000 | Time 0.0136(0.0159) | Loss 2.889657(2.932457)
[TEST] Iter 66000 | Test Loss 2.856009
Iter 66100 | Time 0.0194(0.0244) | Loss 2.864070(2.923132)
Iter 66200 | Time 0.0156(0.0216) | Loss 2.871716(2.920526)
Iter 66300 | Time 0.0139(0.0192) | Loss 2.902099(2.901881)
Iter 66400 | Time 0.0145(0.0173) | Loss 2.991287(2.919287)
Iter 66500 | Time 0.0182(0.0189) | Loss 2.867638(2.935432)
Iter 66600 | Time 0.0152(0.0183) | Loss 2.889914(2.923069)
Iter 66700 | Time 0.0179(0.0160) | Loss 2.775823(2.924588)
Iter 66800 | Time 0.0131(0.0162) | Loss 3.002662(2.931513)
Iter 66900 | Time 0.0166(0.0158) | Loss 3.016321(2.931835)
Iter 67000 | Time 0.0185(0.0157) | Loss 2.911751(2.934465)
[TEST] Iter 67000 | Test Loss 2.876973
Iter 67100 | Time 0.0158(0.0153) | Loss 2.948930(2.935377)
Iter 67200 | Time 0.0143(0.0144) | Loss 2.999441(2.912455)
Iter 67300 | Time 0.0143(0.0145) | Loss 2.829113(2.915117)
Iter 67400 | Time 0.0132(0.0144) | Loss 3.055820(2.928759)
Iter 67500 | Time 0.0175(0.0146) | Loss 2.792031(2.922945)
Iter 67600 | Time 0.0135(0.0148) | Loss 3.053084(2.924608)
Iter 67700 | Time 0.0143(0.0145) | Loss 2.939165(2.940289)
Iter 67800 | Time 0.0137(0.0144) | Loss 3.006143(2.919585)
Iter 67900 | Time 0.0174(0.0161) | Loss 2.975879(2.917957)
Iter 68000 | Time 0.0140(0.0160) | Loss 3.034203(2.917243)
[TEST] Iter 68000 | Test Loss 2.902893
Iter 68100 | Time 0.0144(0.0146) | Loss 2.899160(2.917262)
Iter 68200 | Time 0.0135(0.0146) | Loss 2.956019(2.930175)
Iter 68300 | Time 0.0141(0.0155) | Loss 2.958633(2.909894)
Iter 68400 | Time 0.0135(0.0149) | Loss 2.818124(2.925157)
Iter 68500 | Time 0.0142(0.0146) | Loss 2.969823(2.926930)
Iter 68600 | Time 0.0137(0.0148) | Loss 2.725823(2.932723)
Iter 68700 | Time 0.0149(0.0145) | Loss 2.960266(2.910726)
Iter 68800 | Time 0.0163(0.0153) | Loss 2.977782(2.913388)
Iter 68900 | Time 0.0127(0.0154) | Loss 2.984208(2.924301)
Iter 69000 | Time 0.0143(0.0154) | Loss 2.839527(2.946073)
[TEST] Iter 69000 | Test Loss 2.855761
Iter 69100 | Time 0.0134(0.0170) | Loss 2.830238(2.922978)
Iter 69200 | Time 0.0149(0.0176) | Loss 3.000432(2.901313)
Iter 69300 | Time 0.0185(0.0185) | Loss 2.891581(2.926247)
Iter 69400 | Time 0.0171(0.0173) | Loss 2.915909(2.914600)
Iter 69500 | Time 0.0222(0.0175) | Loss 2.911933(2.907656)
Iter 69600 | Time 0.0148(0.0170) | Loss 3.029359(2.934301)
Iter 69700 | Time 0.0167(0.0200) | Loss 2.958504(2.901777)
Iter 69800 | Time 0.0141(0.0178) | Loss 3.091964(2.918834)
Iter 69900 | Time 0.0142(0.0161) | Loss 2.792029(2.915345)
Iter 70000 | Time 0.0167(0.0154) | Loss 2.874450(2.923575)
[TEST] Iter 70000 | Test Loss 2.906207
Iter 70100 | Time 0.0159(0.0157) | Loss 2.904020(2.911972)
Iter 70200 | Time 0.0184(0.0154) | Loss 2.908777(2.907753)
Iter 70300 | Time 0.0502(0.0172) | Loss 2.868613(2.914010)
Iter 70400 | Time 0.0147(0.0158) | Loss 2.926947(2.918056)
Iter 70500 | Time 0.0136(0.0149) | Loss 3.021588(2.922399)
Iter 70600 | Time 0.0138(0.0151) | Loss 2.829997(2.922024)
Iter 70700 | Time 0.0135(0.0145) | Loss 2.729768(2.936535)
Iter 70800 | Time 0.0152(0.0149) | Loss 2.964911(2.918353)
Iter 70900 | Time 0.0154(0.0150) | Loss 2.774805(2.911635)
Iter 71000 | Time 0.0153(0.0161) | Loss 2.933319(2.925622)
[TEST] Iter 71000 | Test Loss 2.944936
Iter 71100 | Time 0.0205(0.0169) | Loss 2.947934(2.923396)
Iter 71200 | Time 0.0154(0.0203) | Loss 2.663064(2.902831)
Iter 71300 | Time 0.0199(0.0191) | Loss 2.712458(2.915102)
Iter 71400 | Time 0.0177(0.0232) | Loss 2.936929(2.916513)
Iter 71500 | Time 0.0237(0.0250) | Loss 3.230819(2.905270)
Iter 71600 | Time 0.0420(0.0287) | Loss 2.780757(2.922504)
Iter 71700 | Time 0.0185(0.0198) | Loss 2.945584(2.918058)
Iter 71800 | Time 0.0293(0.0237) | Loss 2.800760(2.928293)
Iter 71900 | Time 0.0133(0.0354) | Loss 2.874689(2.914686)
Iter 72000 | Time 0.0155(0.0197) | Loss 3.052781(2.925830)
[TEST] Iter 72000 | Test Loss 2.986562
Iter 72100 | Time 0.0170(0.0178) | Loss 2.934372(2.906119)
Iter 72200 | Time 0.0161(0.0170) | Loss 2.925738(2.925962)
Iter 72300 | Time 0.0136(0.0171) | Loss 2.942404(2.921730)
Iter 72400 | Time 0.0267(0.0199) | Loss 2.876938(2.898997)
Iter 72500 | Time 0.0146(0.0233) | Loss 2.893905(2.938433)
Iter 72600 | Time 0.0273(0.0208) | Loss 2.949377(2.928446)
Iter 72700 | Time 0.0149(0.0170) | Loss 2.873851(2.918058)
Iter 72800 | Time 0.0250(0.0200) | Loss 2.883810(2.906828)
Iter 72900 | Time 0.0239(0.0201) | Loss 3.072649(2.917793)
Iter 73000 | Time 0.0193(0.0200) | Loss 2.806722(2.922498)
[TEST] Iter 73000 | Test Loss 2.941532
Iter 73100 | Time 0.0158(0.0176) | Loss 2.965576(2.917576)
Iter 73200 | Time 0.0178(0.0167) | Loss 2.977194(2.925884)
Iter 73300 | Time 0.0155(0.0197) | Loss 3.002093(2.914053)
Iter 73400 | Time 0.0166(0.0184) | Loss 2.803095(2.897522)
Iter 73500 | Time 0.0190(0.0179) | Loss 2.918964(2.893544)
Iter 73600 | Time 0.0163(0.0179) | Loss 2.782771(2.917541)
Iter 73700 | Time 0.0160(0.0200) | Loss 3.090190(2.936348)
Iter 73800 | Time 0.0285(0.0214) | Loss 2.815369(2.912551)
Iter 73900 | Time 0.0162(0.0177) | Loss 2.889513(2.923651)
Iter 74000 | Time 0.0171(0.0161) | Loss 2.996166(2.934254)
[TEST] Iter 74000 | Test Loss 2.963227
Iter 74100 | Time 0.0140(0.0160) | Loss 3.230075(2.940559)
Iter 74200 | Time 0.0182(0.0165) | Loss 3.040630(2.934331)
Iter 74300 | Time 0.0148(0.0169) | Loss 2.956745(2.908039)
Iter 74400 | Time 0.0162(0.0165) | Loss 2.962811(2.921646)
Iter 74500 | Time 0.0139(0.0165) | Loss 2.912107(2.913912)
Iter 74600 | Time 0.0166(0.0171) | Loss 3.001147(2.921925)
Iter 74700 | Time 0.0169(0.0175) | Loss 2.829470(2.907562)
Iter 74800 | Time 0.0177(0.0171) | Loss 2.780058(2.903268)
Iter 74900 | Time 0.0178(0.0167) | Loss 3.025400(2.928531)
Iter 75000 | Time 0.0134(0.0165) | Loss 2.933892(2.928119)
[TEST] Iter 75000 | Test Loss 2.916182
Iter 75100 | Time 0.0163(0.0159) | Loss 2.945852(2.920982)
Iter 75200 | Time 0.0128(0.0161) | Loss 2.784976(2.923162)
Iter 75300 | Time 0.0128(0.0163) | Loss 2.919895(2.895608)
Iter 75400 | Time 0.0138(0.0163) | Loss 2.820899(2.922110)
Iter 75500 | Time 0.0158(0.0200) | Loss 2.874977(2.905443)
Iter 75600 | Time 0.0163(0.0190) | Loss 2.956917(2.913697)
Iter 75700 | Time 0.0142(0.0164) | Loss 2.876032(2.912752)
Iter 75800 | Time 0.0168(0.0181) | Loss 2.955087(2.907869)
Iter 75900 | Time 0.0145(0.0180) | Loss 2.876438(2.915860)
Iter 76000 | Time 0.0157(0.0179) | Loss 2.927047(2.912091)
[TEST] Iter 76000 | Test Loss 2.899318
Iter 76100 | Time 0.0149(0.0172) | Loss 2.842428(2.909690)
Iter 76200 | Time 0.0147(0.0165) | Loss 3.120443(2.915349)
Iter 76300 | Time 0.0160(0.0298) | Loss 2.928616(2.925848)
Iter 76400 | Time 0.0148(0.0247) | Loss 2.918417(2.919284)
Iter 76500 | Time 0.0165(0.0174) | Loss 2.992592(2.925355)
Iter 76600 | Time 0.0187(0.0160) | Loss 2.937375(2.910870)
Iter 76700 | Time 0.0158(0.0171) | Loss 2.926035(2.916258)
Iter 76800 | Time 0.0138(0.0165) | Loss 2.873522(2.927687)
Iter 76900 | Time 0.0157(0.0162) | Loss 2.912249(2.916972)
Iter 77000 | Time 0.0148(0.0165) | Loss 2.987012(2.929735)
[TEST] Iter 77000 | Test Loss 2.940589
Iter 77100 | Time 0.1116(0.0318) | Loss 2.933146(2.936858)
Iter 77200 | Time 0.0163(0.0192) | Loss 2.866714(2.936203)
Iter 77300 | Time 0.0139(0.0599) | Loss 2.973822(2.909203)
Iter 77400 | Time 0.0156(0.0228) | Loss 2.859160(2.919657)
Iter 77500 | Time 0.0141(0.0169) | Loss 2.890430(2.926022)
Iter 77600 | Time 0.0178(0.0160) | Loss 2.838359(2.918797)
Iter 77700 | Time 0.0157(0.0311) | Loss 2.853550(2.904382)
Iter 77800 | Time 0.0166(0.0250) | Loss 2.823829(2.921426)
Iter 77900 | Time 0.0176(0.0312) | Loss 2.745690(2.896513)
Iter 78000 | Time 0.0151(0.0343) | Loss 3.027732(2.920498)
[TEST] Iter 78000 | Test Loss 2.937007
Iter 78100 | Time 0.0300(0.0446) | Loss 3.028492(2.923924)
Iter 78200 | Time 0.0482(0.0498) | Loss 3.003082(2.931168)
Iter 78300 | Time 0.0171(0.0217) | Loss 3.108570(2.913121)
Iter 78400 | Time 0.0153(0.0297) | Loss 2.807280(2.914172)
Iter 78500 | Time 0.0143(0.0179) | Loss 2.940100(2.912885)
Iter 78600 | Time 0.0143(0.0157) | Loss 3.006972(2.938227)
Iter 78700 | Time 0.0161(0.0152) | Loss 3.011649(2.933229)
Iter 78800 | Time 0.0145(0.0156) | Loss 2.847679(2.928728)
Iter 78900 | Time 0.0148(0.0156) | Loss 3.240952(2.936115)
Iter 79000 | Time 0.0178(0.0154) | Loss 2.951771(2.915560)
[TEST] Iter 79000 | Test Loss 2.943205
Iter 79100 | Time 0.0160(0.0283) | Loss 2.887122(2.917510)
Iter 79200 | Time 0.0165(0.0177) | Loss 2.789750(2.913814)
Iter 79300 | Time 0.0150(0.0153) | Loss 2.940923(2.938505)
Iter 79400 | Time 0.0147(0.0190) | Loss 2.886755(2.916888)
Iter 79500 | Time 0.0133(0.0166) | Loss 2.883203(2.916703)
Iter 79600 | Time 0.0138(0.0155) | Loss 3.012780(2.931822)
Iter 79700 | Time 0.0159(0.0160) | Loss 2.977277(2.917477)
Iter 79800 | Time 0.0177(0.0164) | Loss 2.959879(2.928154)
Iter 79900 | Time 0.0154(0.0155) | Loss 2.845205(2.924313)
Iter 80000 | Time 0.0157(0.0153) | Loss 2.866931(2.935923)
[TEST] Iter 80000 | Test Loss 2.864532
Iter 80100 | Time 0.0148(0.0152) | Loss 3.030342(2.942921)
Iter 80200 | Time 0.0166(0.0161) | Loss 2.902936(2.906268)
Iter 80300 | Time 0.0177(0.0185) | Loss 2.871603(2.908596)
Iter 80400 | Time 0.0171(0.0204) | Loss 2.825145(2.913419)
Iter 80500 | Time 0.0175(0.0194) | Loss 2.919425(2.940210)
Iter 80600 | Time 0.0169(0.0396) | Loss 3.083671(2.942988)
Iter 80700 | Time 0.0221(0.0411) | Loss 2.977740(2.921172)
Iter 80800 | Time 0.0463(0.0280) | Loss 2.954319(2.928243)
Iter 80900 | Time 0.0351(0.0536) | Loss 3.106051(2.938823)
Iter 81000 | Time 0.0171(0.0422) | Loss 2.939676(2.924189)
[TEST] Iter 81000 | Test Loss 2.936283
Iter 81100 | Time 0.0169(0.0219) | Loss 2.841749(2.922298)
Iter 81200 | Time 0.0121(0.0184) | Loss 2.845583(2.911747)
Iter 81300 | Time 0.0142(0.0221) | Loss 3.007311(2.907274)
Iter 81400 | Time 0.0197(0.0175) | Loss 2.846014(2.908903)
Iter 81500 | Time 0.0152(0.0158) | Loss 2.826504(2.913602)
Iter 81600 | Time 0.0152(0.0151) | Loss 2.955889(2.928779)
Iter 81700 | Time 0.0135(0.0161) | Loss 2.950465(2.906624)
Iter 81800 | Time 0.0187(0.0211) | Loss 2.926742(2.925579)
Iter 81900 | Time 0.0151(0.0183) | Loss 2.939678(2.936251)
Iter 82000 | Time 0.0157(0.0170) | Loss 2.814181(2.910718)
[TEST] Iter 82000 | Test Loss 2.926678
Iter 82100 | Time 0.0166(0.0166) | Loss 2.740597(2.896926)
Iter 82200 | Time 0.0156(0.0170) | Loss 2.907227(2.919814)
Iter 82300 | Time 0.0149(0.0165) | Loss 2.891562(2.909617)
Iter 82400 | Time 0.0185(0.0172) | Loss 2.903904(2.924878)
Iter 82500 | Time 0.0161(0.0168) | Loss 3.023771(2.913567)
Iter 82600 | Time 0.0183(0.0167) | Loss 2.888515(2.907864)
Iter 82700 | Time 0.0176(0.0165) | Loss 2.957003(2.916405)
Iter 82800 | Time 0.0153(0.0167) | Loss 2.884010(2.912595)
Iter 82900 | Time 0.0164(0.0157) | Loss 3.031006(2.923791)
Iter 83000 | Time 0.0169(0.0164) | Loss 2.860026(2.923792)
[TEST] Iter 83000 | Test Loss 2.942400
Iter 83100 | Time 0.0145(0.0161) | Loss 2.937152(2.914820)
Iter 83200 | Time 0.0165(0.0161) | Loss 2.736650(2.904328)
Iter 83300 | Time 0.0151(0.0161) | Loss 2.917742(2.907636)
Iter 83400 | Time 0.0153(0.0173) | Loss 3.082184(2.920717)
Iter 83500 | Time 0.0151(0.0165) | Loss 3.014443(2.922840)
Iter 83600 | Time 0.0142(0.0161) | Loss 3.039666(2.929694)
Iter 83700 | Time 0.0145(0.0157) | Loss 2.915185(2.918096)
Iter 83800 | Time 0.0267(0.0225) | Loss 3.052191(2.923065)
Iter 83900 | Time 0.0141(0.0196) | Loss 2.892502(2.935155)
Iter 84000 | Time 0.0164(0.0166) | Loss 2.833873(2.921133)
[TEST] Iter 84000 | Test Loss 2.945951
Iter 84100 | Time 0.0137(0.0167) | Loss 2.785114(2.883462)
Iter 84200 | Time 0.0168(0.0182) | Loss 3.049580(2.921786)
Iter 84300 | Time 0.0178(0.0183) | Loss 2.874673(2.895426)
Iter 84400 | Time 0.0151(0.0169) | Loss 2.856803(2.915784)
Iter 84500 | Time 0.0157(0.0166) | Loss 2.983779(2.929265)
Iter 84600 | Time 0.0154(0.0162) | Loss 2.861379(2.921511)
Iter 84700 | Time 0.0165(0.0165) | Loss 2.851390(2.911456)
Iter 84800 | Time 0.0185(0.0204) | Loss 3.118716(2.927463)
Iter 84900 | Time 0.0161(0.0179) | Loss 2.905697(2.922125)
Iter 85000 | Time 0.0161(0.0162) | Loss 2.878187(2.914252)
[TEST] Iter 85000 | Test Loss 2.993499
Iter 85100 | Time 0.0149(0.0171) | Loss 2.823094(2.900396)
Iter 85200 | Time 0.0179(0.0164) | Loss 2.890029(2.914032)
Iter 85300 | Time 0.0159(0.0159) | Loss 2.988102(2.904006)
Iter 85400 | Time 0.0149(0.0159) | Loss 2.822530(2.906129)
Iter 85500 | Time 0.0134(0.0154) | Loss 3.079940(2.922769)
Iter 85600 | Time 0.0161(0.0154) | Loss 2.920178(2.907613)
Iter 85700 | Time 0.0216(0.0184) | Loss 2.869559(2.907270)
Iter 85800 | Time 0.0237(0.0203) | Loss 2.797529(2.902956)
Iter 85900 | Time 0.0170(0.0182) | Loss 2.996420(2.901677)
Iter 86000 | Time 0.0175(0.0177) | Loss 2.946012(2.911204)
[TEST] Iter 86000 | Test Loss 2.906565
Iter 86100 | Time 0.0140(0.0165) | Loss 2.889564(2.904888)
Iter 86200 | Time 0.0157(0.0171) | Loss 2.678817(2.902446)
Iter 86300 | Time 0.0158(0.0172) | Loss 2.778487(2.896204)
Iter 86400 | Time 0.0215(0.0159) | Loss 2.795439(2.912487)
Iter 86500 | Time 0.0159(0.0160) | Loss 2.861533(2.921437)
Iter 86600 | Time 0.0272(0.0171) | Loss 3.103478(2.894255)
Iter 86700 | Time 0.0147(0.0168) | Loss 2.944788(2.928072)
Iter 86800 | Time 0.0269(0.0174) | Loss 2.877371(2.918691)
Iter 86900 | Time 0.0148(0.0180) | Loss 2.898930(2.899492)
Iter 87000 | Time 0.0168(0.0166) | Loss 2.995154(2.901787)
[TEST] Iter 87000 | Test Loss 2.951138
Iter 87100 | Time 0.0157(0.0164) | Loss 2.802971(2.918230)
Iter 87200 | Time 0.0126(0.0161) | Loss 3.029130(2.892356)
Iter 87300 | Time 0.0176(0.0153) | Loss 2.873618(2.915765)
Iter 87400 | Time 0.0327(0.0171) | Loss 2.793648(2.916571)
Iter 87500 | Time 0.0154(0.0172) | Loss 2.932228(2.901642)
Iter 87600 | Time 0.0158(0.0153) | Loss 2.857396(2.910754)
Iter 87700 | Time 0.0152(0.0164) | Loss 2.925653(2.917738)
Iter 87800 | Time 0.0169(0.0190) | Loss 2.907892(2.920122)
Iter 87900 | Time 0.0169(0.0181) | Loss 2.732769(2.893411)
Iter 88000 | Time 0.0153(0.0166) | Loss 2.871066(2.910297)
[TEST] Iter 88000 | Test Loss 2.905987
Iter 88100 | Time 0.0130(0.0153) | Loss 2.859005(2.908830)
Iter 88200 | Time 0.0139(0.0153) | Loss 2.819992(2.914857)
Iter 88300 | Time 0.0131(0.0154) | Loss 2.781318(2.905604)
Iter 88400 | Time 0.0147(0.0151) | Loss 2.896705(2.917230)
Iter 88500 | Time 0.0141(0.0160) | Loss 2.978276(2.922010)
Iter 88600 | Time 0.0167(0.0157) | Loss 2.884246(2.903491)
Iter 88700 | Time 0.0215(0.0157) | Loss 2.951365(2.917529)
Iter 88800 | Time 0.0147(0.0190) | Loss 2.719749(2.912199)
Iter 88900 | Time 0.0635(0.0240) | Loss 2.849782(2.922057)
Iter 89000 | Time 0.0203(0.0278) | Loss 2.739552(2.910815)
[TEST] Iter 89000 | Test Loss 2.932953
Iter 89100 | Time 0.0199(0.0209) | Loss 2.879094(2.911076)
Iter 89200 | Time 0.0548(0.0211) | Loss 2.927607(2.918413)
Iter 89300 | Time 0.0180(0.0187) | Loss 2.940282(2.922122)
Iter 89400 | Time 0.0246(0.0197) | Loss 2.900905(2.908527)
Iter 89500 | Time 0.0162(0.0215) | Loss 2.976398(2.901989)
Iter 89600 | Time 0.0155(0.0176) | Loss 2.802871(2.904952)
Iter 89700 | Time 0.0170(0.0167) | Loss 2.918895(2.895392)
Iter 89800 | Time 0.0162(0.0173) | Loss 3.042464(2.922615)
Iter 89900 | Time 0.0189(0.0169) | Loss 2.854317(2.929200)
Iter 90000 | Time 0.0147(0.0194) | Loss 2.759114(2.900290)
[TEST] Iter 90000 | Test Loss 2.878444
Iter 90100 | Time 0.0158(0.0218) | Loss 3.147927(2.907429)
Iter 90200 | Time 0.0163(0.0228) | Loss 2.980080(2.898660)
Iter 90300 | Time 0.0170(0.0183) | Loss 2.836770(2.909564)
Iter 90400 | Time 0.0168(0.0171) | Loss 3.108915(2.891597)
Iter 90500 | Time 0.0171(0.0196) | Loss 2.954470(2.892064)
Iter 90600 | Time 0.0162(0.0189) | Loss 2.880189(2.902306)
Iter 90700 | Time 0.0147(0.0273) | Loss 2.831774(2.898916)
Iter 90800 | Time 0.0149(0.0199) | Loss 2.922297(2.922951)
Iter 90900 | Time 0.0183(0.0199) | Loss 2.792344(2.924045)
Iter 91000 | Time 0.0141(0.0274) | Loss 2.924176(2.917832)
[TEST] Iter 91000 | Test Loss 2.901001
Iter 91100 | Time 0.2587(0.0620) | Loss 2.858432(2.911972)
Iter 91200 | Time 0.0157(0.0244) | Loss 2.794180(2.918625)
Iter 91300 | Time 0.0165(0.0191) | Loss 2.799037(2.924339)
Iter 91400 | Time 0.0193(0.0185) | Loss 2.896436(2.900245)
Iter 91500 | Time 0.0136(0.0205) | Loss 2.949016(2.908014)
Iter 91600 | Time 0.0180(0.0239) | Loss 2.871345(2.903677)
Iter 91700 | Time 0.0178(0.0225) | Loss 2.843290(2.895105)
Iter 91800 | Time 0.0141(0.0168) | Loss 3.231555(2.927133)
Iter 91900 | Time 0.0159(0.0193) | Loss 2.909275(2.912315)
Iter 92000 | Time 0.0149(0.0161) | Loss 3.004338(2.897499)
[TEST] Iter 92000 | Test Loss 2.910671
Iter 92100 | Time 0.0141(0.0166) | Loss 2.966557(2.916328)
Iter 92200 | Time 0.0145(0.0155) | Loss 2.979285(2.895196)
Iter 92300 | Time 0.0174(0.0173) | Loss 2.870567(2.907021)
Iter 92400 | Time 0.0168(0.0168) | Loss 2.759730(2.908421)
Iter 92500 | Time 0.0190(0.0194) | Loss 2.902594(2.903029)
Iter 92600 | Time 0.0156(0.0175) | Loss 2.937454(2.913713)
Iter 92700 | Time 0.0139(0.0165) | Loss 2.779962(2.909097)
Iter 92800 | Time 0.0138(0.0170) | Loss 2.941810(2.921476)
Iter 92900 | Time 0.0279(0.0185) | Loss 3.021482(2.905310)
Iter 93000 | Time 0.0155(0.0199) | Loss 2.789207(2.917036)
[TEST] Iter 93000 | Test Loss 2.932678
Iter 93100 | Time 0.0228(0.0169) | Loss 2.748545(2.931857)
Iter 93200 | Time 0.0179(0.0325) | Loss 3.101425(2.937113)
Iter 93300 | Time 0.0177(0.0263) | Loss 2.909191(2.918037)
Iter 93400 | Time 0.0624(0.0275) | Loss 3.003264(2.910533)
Iter 93500 | Time 0.0457(0.0445) | Loss 2.829675(2.904614)
Iter 93600 | Time 0.0195(0.0240) | Loss 2.994483(2.910722)
Iter 93700 | Time 0.0184(0.0179) | Loss 2.893119(2.911810)
Iter 93800 | Time 0.0145(0.0168) | Loss 2.776293(2.904783)
Iter 93900 | Time 0.0162(0.0166) | Loss 2.930634(2.923584)
Iter 94000 | Time 0.0167(0.0182) | Loss 2.723250(2.901381)
[TEST] Iter 94000 | Test Loss 2.850298
Iter 94100 | Time 0.0237(0.0182) | Loss 2.906482(2.917921)
Iter 94200 | Time 0.0309(0.0201) | Loss 2.903712(2.920900)
Iter 94300 | Time 0.0241(0.0208) | Loss 3.045289(2.918163)
Iter 94400 | Time 0.0171(0.0178) | Loss 2.868275(2.904012)
Iter 94500 | Time 0.0193(0.0388) | Loss 2.783457(2.901280)
Iter 94600 | Time 0.0209(0.0243) | Loss 2.886975(2.905332)
Iter 94700 | Time 0.0425(0.0238) | Loss 2.902761(2.914321)
Iter 94800 | Time 0.0236(0.0228) | Loss 3.065284(2.910903)
Iter 94900 | Time 0.0223(0.0287) | Loss 2.885782(2.919167)
Iter 95000 | Time 0.0192(0.0251) | Loss 3.012891(2.895942)
[TEST] Iter 95000 | Test Loss 2.934198
Iter 95100 | Time 0.0162(0.0269) | Loss 2.874158(2.900248)
Iter 95200 | Time 0.0216(0.0229) | Loss 2.914275(2.911409)
Iter 95300 | Time 0.0231(0.0242) | Loss 2.937788(2.918837)
Iter 95400 | Time 0.0278(0.0221) | Loss 3.118431(2.909620)
Iter 95500 | Time 0.1581(0.0278) | Loss 2.749468(2.912912)
Iter 95600 | Time 0.0268(0.0229) | Loss 3.138416(2.909706)
Iter 95700 | Time 0.0239(0.0234) | Loss 2.981179(2.904552)
Iter 95800 | Time 0.0331(0.0209) | Loss 2.842242(2.895482)
Iter 95900 | Time 0.0163(0.0194) | Loss 2.807240(2.906391)
Iter 96000 | Time 0.0205(0.0193) | Loss 2.954872(2.918581)
[TEST] Iter 96000 | Test Loss 2.883315
Iter 96100 | Time 0.0193(0.0192) | Loss 2.881815(2.920173)
Iter 96200 | Time 0.0176(0.0218) | Loss 2.785218(2.915913)
Iter 96300 | Time 0.0242(0.0205) | Loss 2.938491(2.906697)
Iter 96400 | Time 0.0292(0.0207) | Loss 3.047060(2.896018)
Iter 96500 | Time 0.0153(0.0247) | Loss 2.905367(2.905198)
Iter 96600 | Time 0.0624(0.0284) | Loss 2.994861(2.916903)
Iter 96700 | Time 0.0146(0.0203) | Loss 2.782115(2.913582)
Iter 96800 | Time 0.0266(0.0232) | Loss 2.797126(2.920075)
Iter 96900 | Time 0.0201(0.0391) | Loss 3.092749(2.905145)
Iter 97000 | Time 0.0249(0.0272) | Loss 2.991039(2.912130)
[TEST] Iter 97000 | Test Loss 2.874851
Iter 97100 | Time 0.0163(0.0224) | Loss 3.020915(2.913936)
Iter 97200 | Time 0.0194(0.0214) | Loss 2.810932(2.913958)
Iter 97300 | Time 0.0206(0.0201) | Loss 2.928570(2.912622)
Iter 97400 | Time 0.0166(0.0187) | Loss 2.792212(2.892510)
Iter 97500 | Time 0.0227(0.0196) | Loss 2.891812(2.890130)
Iter 97600 | Time 0.0299(0.0251) | Loss 2.842736(2.899215)
Iter 97700 | Time 0.0218(0.0230) | Loss 2.905419(2.906060)
Iter 97800 | Time 0.0200(0.0239) | Loss 2.936914(2.900765)
Iter 97900 | Time 0.0162(0.0255) | Loss 2.949534(2.911798)
Iter 98000 | Time 0.0206(0.0203) | Loss 2.900555(2.905992)
[TEST] Iter 98000 | Test Loss 2.929425
Iter 98100 | Time 0.0166(0.0181) | Loss 2.916493(2.916838)
Iter 98200 | Time 0.0154(0.0179) | Loss 3.001266(2.931992)
Iter 98300 | Time 0.0184(0.0221) | Loss 2.875798(2.917755)
Iter 98400 | Time 0.0498(0.0196) | Loss 2.849095(2.914977)
Iter 98500 | Time 0.0175(0.0209) | Loss 2.930821(2.916104)
Iter 98600 | Time 0.0279(0.0396) | Loss 2.895338(2.922569)
Iter 98700 | Time 0.0285(0.0300) | Loss 2.897153(2.900451)
Iter 98800 | Time 0.0145(0.0203) | Loss 2.888999(2.892675)
Iter 98900 | Time 0.0168(0.0187) | Loss 2.935079(2.910797)
Iter 99000 | Time 0.0172(0.0174) | Loss 2.950912(2.893978)
[TEST] Iter 99000 | Test Loss 2.897493
Iter 99100 | Time 0.0136(0.0205) | Loss 2.856095(2.909241)
Iter 99200 | Time 0.0173(0.0178) | Loss 3.025550(2.894291)
Iter 99300 | Time 0.0155(0.0197) | Loss 3.026011(2.926110)
Iter 99400 | Time 0.0145(0.0182) | Loss 2.691635(2.896196)
Iter 99500 | Time 0.0610(0.0262) | Loss 2.716596(2.883234)
Iter 99600 | Time 0.0227(0.0213) | Loss 2.817922(2.903699)
Iter 99700 | Time 0.0150(0.0214) | Loss 2.875108(2.925813)
Iter 99800 | Time 0.0202(0.0223) | Loss 2.999594(2.915119)
Iter 99900 | Time 0.0190(0.0228) | Loss 2.792691(2.913895)
Iter 100000 | Time 0.0443(0.0212) | Loss 2.975493(2.918000)
[TEST] Iter 100000 | Test Loss 2.903561
[TEST] Iter 100001 | Test Loss 2.844191
Training has finished.
/Users/bjzhang/gitrepos/tap-sgm/ffjord-master/train_toy.py
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import argparse
import os
import time

import torch
import torch.optim as optim

import lib.toy_data as toy_data
import lib.utils as utils
from lib.visualize_flow import visualize_transform
import lib.layers.odefunc as odefunc

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import build_model_tabular

from diagnostics.viz_toy import save_trajectory, trajectory_to_video

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'fixed_adams']
parser = argparse.ArgumentParser('Continuous Normalizing Flow')
parser.add_argument(
    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='pinwheel'
)
parser.add_argument(
    "--layer_type", type=str, default="concatsquash",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument('--dims', type=str, default='64-64-64')
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')
parser.add_argument('--time_length', type=float, default=0.5)
parser.add_argument('--train_T', type=eval, default=True)
parser.add_argument("--divergence_fn", type=str, default="brute_force", choices=["brute_force", "approximate"])
parser.add_argument("--nonlinearity", type=str, default="tanh", choices=odefunc.NONLINEARITIES)

parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=False, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--batch_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--bn_lag', type=float, default=0)

parser.add_argument('--niters', type=int, default=10000)
parser.add_argument('--batch_size', type=int, default=100)
parser.add_argument('--test_batch_size', type=int, default=1000)
parser.add_argument('--lr', type=float, default=1e-3)
parser.add_argument('--weight_decay', type=float, default=1e-5)

# Track quantities
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument('--save', type=str, default='experiments/cnf')
parser.add_argument('--viz_freq', type=int, default=100)
parser.add_argument('--val_freq', type=int, default=100)
parser.add_argument('--log_freq', type=int, default=10)
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')


def get_transforms(model):

    def sample_fn(z, logpz=None):
        if logpz is not None:
            return model(z, logpz, reverse=True)
        else:
            return model(z, reverse=True)

    def density_fn(x, logpx=None):
        if logpx is not None:
            return model(x, logpx, reverse=False)
        else:
            return model(x, reverse=False)

    return sample_fn, density_fn


def compute_loss(args, model, batch_size=None):
    if batch_size is None: batch_size = args.batch_size

    # load data
    x = toy_data.inf_train_gen(args.data, batch_size=batch_size)
    x = torch.from_numpy(x).type(torch.float32).to(device)
    zero = torch.zeros(x.shape[0], 1).to(x)

    # transform to z
    z, delta_logp = model(x, zero)

    # compute log q(z)
    logpz = standard_normal_logprob(z).sum(1, keepdim=True)

    logpx = logpz - delta_logp
    loss = -torch.mean(logpx)
    return loss


if __name__ == '__main__':

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, 2, regularization_fns).to(device)
    if args.spectral_norm: add_spectral_norm(model)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    end = time.time()
    best_loss = float('inf')
    model.train()
    for itr in range(1, args.niters + 1):
        optimizer.zero_grad()
        if args.spectral_norm: spectral_norm_power_iteration(model, 1)

        loss = compute_loss(args, model)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
            )
            loss = loss + reg_loss

        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)

        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            'Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) | NFE Forward {:.0f}({:.1f})'
            ' | NFE Backward {:.0f}({:.1f}) | CNF Time {:.4f}({:.4f})'.format(
                itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, nfef_meter.val, nfef_meter.avg,
                nfeb_meter.val, nfeb_meter.avg, tt_meter.val, tt_meter.avg
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)

        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                model.eval()
                test_loss = compute_loss(args, model, batch_size=args.test_batch_size)
                test_nfe = count_nfe(model)
                log_message = '[TEST] Iter {:04d} | Test Loss {:.6f} | NFE {:.0f}'.format(itr, test_loss, test_nfe)
                logger.info(log_message)

                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    utils.makedirs(args.save)
                    torch.save({
                        'args': args,
                        'state_dict': model.state_dict(),
                    }, os.path.join(args.save, 'checkpt.pth'))
                model.train()

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                model.eval()
                p_samples = toy_data.inf_train_gen(args.data, batch_size=2000)

                sample_fn, density_fn = get_transforms(model)

                plt.figure(figsize=(9, 3))
                visualize_transform(
                    p_samples, torch.randn, standard_normal_logprob, transform=sample_fn, inverse_transform=density_fn,
                    samples=True, npts=800, device=device
                )
                fig_filename = os.path.join(args.save, 'figs', '{:04d}.jpg'.format(itr))
                utils.makedirs(os.path.dirname(fig_filename))
                plt.savefig(fig_filename)
                plt.close()
                model.train()

        end = time.time()

    logger.info('Training has finished.')

    save_traj_dir = os.path.join(args.save, 'trajectory')
    logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = toy_data.inf_train_gen(args.data, batch_size=2000)
    save_trajectory(model, data_samples, save_traj_dir, device=device)
    trajectory_to_video(save_traj_dir)

Namespace(data='pinwheel', layer_type='concatsquash', dims='64-64-64', num_blocks=1, time_length=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=10000, batch_size=100, test_batch_size=1000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='experiments/cnf', viz_freq=100, val_freq=100, log_freq=10, gpu=0)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=2, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=2, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=2, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=2, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9225
Iter 0001 | Time 0.2389(0.2389) | Loss 4.175737(4.175737) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0002 | Time 0.1258(0.2310) | Loss 4.015407(4.164514) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0003 | Time 0.1419(0.2248) | Loss 4.084425(4.158908) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0004 | Time 0.1392(0.2188) | Loss 4.116217(4.155920) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0005 | Time 0.1591(0.2146) | Loss 3.843551(4.134054) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0006 | Time 0.1654(0.2112) | Loss 3.924026(4.119352) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0007 | Time 0.1482(0.2068) | Loss 3.850503(4.100532) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0008 | Time 0.1356(0.2018) | Loss 4.103516(4.100741) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0009 | Time 0.4938(0.2222) | Loss 4.094011(4.100270) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0010 | Time 0.1392(0.2164) | Loss 3.967548(4.090980) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0011 | Time 0.1480(0.2116) | Loss 3.953470(4.081354) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0012 | Time 0.1226(0.2054) | Loss 4.167248(4.087366) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0013 | Time 0.1654(0.2026) | Loss 3.821631(4.068765) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0014 | Time 0.1500(0.1989) | Loss 3.867145(4.054652) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0015 | Time 0.1436(0.1950) | Loss 4.167717(4.062566) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0016 | Time 0.1363(0.1909) | Loss 3.890300(4.050508) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0017 | Time 0.1407(0.1874) | Loss 3.827575(4.034902) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0018 | Time 0.1194(0.1826) | Loss 3.833036(4.020772) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0019 | Time 0.1620(0.1812) | Loss 3.862908(4.009721) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0020 | Time 0.1434(0.1786) | Loss 3.937043(4.004634) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0021 | Time 0.1632(0.1775) | Loss 3.919178(3.998652) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0022 | Time 0.1370(0.1747) | Loss 3.838925(3.987471) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0023 | Time 0.1557(0.1733) | Loss 3.644921(3.963492) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0024 | Time 0.1266(0.1701) | Loss 3.732529(3.947325) | NFE Forward 14(14.0) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0025 | Time 0.1581(0.1692) | Loss 3.909115(3.944650) | NFE Forward 20(14.4) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0026 | Time 0.1347(0.1668) | Loss 3.834271(3.936924) | NFE Forward 20(14.8) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0027 | Time 0.1821(0.1679) | Loss 3.864195(3.931833) | NFE Forward 20(15.2) | NFE Backward 14(14.0) | CNF Time 0.5000(0.5000)
Iter 0028 | Time 0.1739(0.1683) | Loss 3.831734(3.924826) | NFE Forward 20(15.5) | NFE Backward 20(14.4) | CNF Time 0.5000(0.5000)
Iter 0029 | Time 0.2201(0.1719) | Loss 3.747014(3.912379) | NFE Forward 20(15.8) | NFE Backward 20(14.8) | CNF Time 0.5000(0.5000)
Iter 0030 | Time 0.1994(0.1738) | Loss 3.680512(3.896148) | NFE Forward 20(16.1) | NFE Backward 20(15.2) | CNF Time 0.5000(0.5000)
Iter 0031 | Time 0.1791(0.1742) | Loss 3.901613(3.896531) | NFE Forward 20(16.4) | NFE Backward 20(15.5) | CNF Time 0.5000(0.5000)
Iter 0032 | Time 0.2375(0.1786) | Loss 3.683931(3.881649) | NFE Forward 20(16.6) | NFE Backward 20(15.8) | CNF Time 0.5000(0.5000)
Iter 0033 | Time 0.1982(0.1800) | Loss 3.451755(3.851556) | NFE Forward 20(16.9) | NFE Backward 20(16.1) | CNF Time 0.5000(0.5000)
Iter 0034 | Time 0.2602(0.1856) | Loss 3.586614(3.833010) | NFE Forward 20(17.1) | NFE Backward 20(16.4) | CNF Time 0.5000(0.5000)
Iter 0035 | Time 0.1988(0.1865) | Loss 3.787968(3.829857) | NFE Forward 20(17.3) | NFE Backward 20(16.6) | CNF Time 0.5000(0.5000)
Iter 0036 | Time 0.2107(0.1882) | Loss 3.807451(3.828289) | NFE Forward 20(17.5) | NFE Backward 20(16.9) | CNF Time 0.5000(0.5000)
Iter 0037 | Time 0.2010(0.1891) | Loss 3.772305(3.824370) | NFE Forward 20(17.7) | NFE Backward 20(17.1) | CNF Time 0.5000(0.5000)
Iter 0038 | Time 0.1886(0.1891) | Loss 3.726130(3.817493) | NFE Forward 20(17.8) | NFE Backward 20(17.3) | CNF Time 0.5000(0.5000)
Iter 0039 | Time 0.2013(0.1899) | Loss 3.674942(3.807515) | NFE Forward 20(18.0) | NFE Backward 20(17.5) | CNF Time 0.5000(0.5000)
Iter 0040 | Time 0.1998(0.1906) | Loss 3.542269(3.788947) | NFE Forward 20(18.1) | NFE Backward 20(17.7) | CNF Time 0.5000(0.5000)
Iter 0041 | Time 0.2081(0.1919) | Loss 3.559999(3.772921) | NFE Forward 20(18.3) | NFE Backward 20(17.8) | CNF Time 0.5000(0.5000)
Iter 0042 | Time 0.1970(0.1922) | Loss 3.667212(3.765521) | NFE Forward 20(18.4) | NFE Backward 20(18.0) | CNF Time 0.5000(0.5000)
Iter 0043 | Time 0.2027(0.1929) | Loss 3.548285(3.750315) | NFE Forward 20(18.5) | NFE Backward 20(18.1) | CNF Time 0.5000(0.5000)
Iter 0044 | Time 0.1911(0.1928) | Loss 3.620743(3.741245) | NFE Forward 20(18.6) | NFE Backward 20(18.3) | CNF Time 0.5000(0.5000)
Iter 0045 | Time 0.1942(0.1929) | Loss 3.583318(3.730190) | NFE Forward 20(18.7) | NFE Backward 20(18.4) | CNF Time 0.5000(0.5000)
Iter 0046 | Time 0.2038(0.1937) | Loss 3.519541(3.715445) | NFE Forward 20(18.8) | NFE Backward 20(18.5) | CNF Time 0.5000(0.5000)
Iter 0047 | Time 0.1819(0.1928) | Loss 3.563237(3.704790) | NFE Forward 20(18.9) | NFE Backward 20(18.6) | CNF Time 0.5000(0.5000)
Iter 0048 | Time 0.2143(0.1944) | Loss 3.527167(3.692356) | NFE Forward 20(18.9) | NFE Backward 20(18.7) | CNF Time 0.5000(0.5000)
Iter 0049 | Time 0.1982(0.1946) | Loss 3.456237(3.675828) | NFE Forward 20(19.0) | NFE Backward 20(18.8) | CNF Time 0.5000(0.5000)
Iter 0050 | Time 0.1779(0.1935) | Loss 3.540310(3.666342) | NFE Forward 20(19.1) | NFE Backward 20(18.9) | CNF Time 0.5000(0.5000)
Iter 0051 | Time 0.2182(0.1952) | Loss 3.537065(3.657292) | NFE Forward 20(19.2) | NFE Backward 20(18.9) | CNF Time 0.5000(0.5000)
Iter 0052 | Time 0.2099(0.1962) | Loss 3.502392(3.646449) | NFE Forward 20(19.2) | NFE Backward 20(19.0) | CNF Time 0.5000(0.5000)
Iter 0053 | Time 0.1975(0.1963) | Loss 3.516059(3.637322) | NFE Forward 20(19.3) | NFE Backward 20(19.1) | CNF Time 0.5000(0.5000)
Iter 0054 | Time 0.2607(0.2008) | Loss 3.472272(3.625769) | NFE Forward 20(19.3) | NFE Backward 20(19.2) | CNF Time 0.5000(0.5000)
Iter 0055 | Time 0.2132(0.2017) | Loss 3.483462(3.615807) | NFE Forward 20(19.4) | NFE Backward 20(19.2) | CNF Time 0.5000(0.5000)
Iter 0056 | Time 0.2241(0.2032) | Loss 3.500817(3.607758) | NFE Forward 20(19.4) | NFE Backward 20(19.3) | CNF Time 0.5000(0.5000)
Iter 0057 | Time 0.1916(0.2024) | Loss 3.539594(3.602986) | NFE Forward 20(19.5) | NFE Backward 20(19.3) | CNF Time 0.5000(0.5000)
Iter 0058 | Time 0.2128(0.2032) | Loss 3.517603(3.597009) | NFE Forward 20(19.5) | NFE Backward 20(19.4) | CNF Time 0.5000(0.5000)
Iter 0059 | Time 0.2097(0.2036) | Loss 3.545796(3.593424) | NFE Forward 20(19.5) | NFE Backward 20(19.4) | CNF Time 0.5000(0.5000)
Iter 0060 | Time 0.1909(0.2027) | Loss 3.639335(3.596638) | NFE Forward 20(19.6) | NFE Backward 20(19.5) | CNF Time 0.5000(0.5000)
Iter 0061 | Time 0.2234(0.2042) | Loss 3.528627(3.591877) | NFE Forward 20(19.6) | NFE Backward 20(19.5) | CNF Time 0.5000(0.5000)
Iter 0062 | Time 0.2567(0.2078) | Loss 3.500369(3.585472) | NFE Forward 20(19.6) | NFE Backward 20(19.5) | CNF Time 0.5000(0.5000)
Iter 0063 | Time 0.2335(0.2096) | Loss 3.486563(3.578548) | NFE Forward 20(19.6) | NFE Backward 20(19.6) | CNF Time 0.5000(0.5000)
Iter 0064 | Time 0.1823(0.2077) | Loss 3.518244(3.574327) | NFE Forward 20(19.7) | NFE Backward 20(19.6) | CNF Time 0.5000(0.5000)
Iter 0065 | Time 0.2549(0.2110) | Loss 3.579278(3.574673) | NFE Forward 20(19.7) | NFE Backward 20(19.6) | CNF Time 0.5000(0.5000)
Iter 0066 | Time 0.2023(0.2104) | Loss 3.519546(3.570815) | NFE Forward 20(19.7) | NFE Backward 20(19.6) | CNF Time 0.5000(0.5000)
Iter 0067 | Time 0.1900(0.2090) | Loss 3.562949(3.570264) | NFE Forward 20(19.7) | NFE Backward 20(19.7) | CNF Time 0.5000(0.5000)
Iter 0068 | Time 0.2018(0.2085) | Loss 3.470211(3.563260) | NFE Forward 20(19.8) | NFE Backward 20(19.7) | CNF Time 0.5000(0.5000)
Iter 0069 | Time 0.1927(0.2074) | Loss 3.448247(3.555209) | NFE Forward 20(19.8) | NFE Backward 20(19.7) | CNF Time 0.5000(0.5000)
Iter 0070 | Time 0.1930(0.2064) | Loss 3.457960(3.548402) | NFE Forward 20(19.8) | NFE Backward 20(19.7) | CNF Time 0.5000(0.5000)
Iter 0071 | Time 0.2104(0.2067) | Loss 3.525666(3.546810) | NFE Forward 20(19.8) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0072 | Time 0.2109(0.2070) | Loss 3.497503(3.543359) | NFE Forward 20(19.8) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0073 | Time 0.1966(0.2062) | Loss 3.443167(3.536345) | NFE Forward 20(19.8) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0074 | Time 0.2003(0.2058) | Loss 3.540097(3.536608) | NFE Forward 20(19.8) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0075 | Time 0.1848(0.2043) | Loss 3.503524(3.534292) | NFE Forward 20(19.9) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0076 | Time 0.1806(0.2027) | Loss 3.465221(3.529457) | NFE Forward 20(19.9) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0077 | Time 0.2026(0.2027) | Loss 3.500501(3.527430) | NFE Forward 20(19.9) | NFE Backward 20(19.8) | CNF Time 0.5000(0.5000)
Iter 0078 | Time 0.1926(0.2020) | Loss 3.463894(3.522983) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0079 | Time 0.1851(0.2008) | Loss 3.584189(3.527267) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0080 | Time 0.1928(0.2002) | Loss 3.475141(3.523618) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0081 | Time 0.2321(0.2024) | Loss 3.485722(3.520966) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0082 | Time 0.1721(0.2003) | Loss 3.586827(3.525576) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0083 | Time 0.2110(0.2011) | Loss 3.497185(3.523588) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0084 | Time 0.1933(0.2005) | Loss 3.434608(3.517360) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0085 | Time 0.1815(0.1992) | Loss 3.478798(3.514661) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0086 | Time 0.2156(0.2003) | Loss 3.501339(3.513728) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0087 | Time 0.2005(0.2004) | Loss 3.507254(3.513275) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0088 | Time 0.1932(0.1998) | Loss 3.573087(3.517462) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0089 | Time 0.1899(0.1991) | Loss 3.485320(3.515212) | NFE Forward 20(19.9) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0090 | Time 0.1778(0.1977) | Loss 3.577333(3.519560) | NFE Forward 20(20.0) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0091 | Time 0.2386(0.2005) | Loss 3.443672(3.514248) | NFE Forward 20(20.0) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0092 | Time 0.1986(0.2004) | Loss 3.431632(3.508465) | NFE Forward 20(20.0) | NFE Backward 20(19.9) | CNF Time 0.5000(0.5000)
Iter 0093 | Time 0.2093(0.2010) | Loss 3.510691(3.508621) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0094 | Time 0.1661(0.1986) | Loss 3.503780(3.508282) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0095 | Time 0.1951(0.1983) | Loss 3.487221(3.506808) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0096 | Time 0.1753(0.1967) | Loss 3.463694(3.503790) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0097 | Time 0.1873(0.1960) | Loss 3.402258(3.496682) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0098 | Time 0.1985(0.1962) | Loss 3.471441(3.494916) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0099 | Time 0.1730(0.1946) | Loss 3.530133(3.497381) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0100 | Time 0.1967(0.1947) | Loss 3.416353(3.491709) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
[TEST] Iter 0100 | Test Loss 3.477859 | NFE 20
Iter 0101 | Time 0.2140(0.1961) | Loss 3.516887(3.493471) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0102 | Time 0.2418(0.1993) | Loss 3.486501(3.492983) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0103 | Time 0.2267(0.2012) | Loss 3.415286(3.487545) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0104 | Time 0.2590(0.2052) | Loss 3.462195(3.485770) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0105 | Time 0.2531(0.2086) | Loss 3.497474(3.486589) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0106 | Time 0.2615(0.2123) | Loss 3.436027(3.483050) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0107 | Time 0.2505(0.2150) | Loss 3.408206(3.477811) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0108 | Time 0.2088(0.2145) | Loss 3.485833(3.478372) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0109 | Time 0.2232(0.2151) | Loss 3.471497(3.477891) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0110 | Time 0.2553(0.2179) | Loss 3.415708(3.473538) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0111 | Time 0.2166(0.2179) | Loss 3.489724(3.474671) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0112 | Time 0.1987(0.2165) | Loss 3.478570(3.474944) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0113 | Time 0.2275(0.2173) | Loss 3.417668(3.470935) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0114 | Time 0.2261(0.2179) | Loss 3.402850(3.466169) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0115 | Time 0.2081(0.2172) | Loss 3.452883(3.465239) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0116 | Time 0.2167(0.2172) | Loss 3.410737(3.461424) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0117 | Time 0.2212(0.2175) | Loss 3.463673(3.461581) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0118 | Time 0.2427(0.2192) | Loss 3.514245(3.465268) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0119 | Time 0.1979(0.2177) | Loss 3.414717(3.461729) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0120 | Time 0.3931(0.2300) | Loss 3.493913(3.463982) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0121 | Time 0.2379(0.2306) | Loss 3.478543(3.465001) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0122 | Time 0.1884(0.2276) | Loss 3.358094(3.457518) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0123 | Time 0.2018(0.2258) | Loss 3.495787(3.460197) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0124 | Time 0.2224(0.2256) | Loss 3.469420(3.460842) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0125 | Time 0.2062(0.2242) | Loss 3.414941(3.457629) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0126 | Time 0.3173(0.2307) | Loss 3.506305(3.461037) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0127 | Time 0.2063(0.2290) | Loss 3.481536(3.462471) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0128 | Time 0.2579(0.2310) | Loss 3.485261(3.464067) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0129 | Time 0.2038(0.2291) | Loss 3.494399(3.466190) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0130 | Time 0.2265(0.2290) | Loss 3.423461(3.463199) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0131 | Time 0.2141(0.2279) | Loss 3.448657(3.462181) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0132 | Time 0.2250(0.2277) | Loss 3.342959(3.453835) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0133 | Time 0.1992(0.2257) | Loss 3.411215(3.450852) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0134 | Time 0.2192(0.2253) | Loss 3.441890(3.450225) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0135 | Time 0.2280(0.2255) | Loss 3.397954(3.446566) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0136 | Time 0.2355(0.2262) | Loss 3.460976(3.447574) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0137 | Time 0.2181(0.2256) | Loss 3.476183(3.449577) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0138 | Time 0.2245(0.2255) | Loss 3.340427(3.441937) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0139 | Time 0.2308(0.2259) | Loss 3.402126(3.439150) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0140 | Time 0.2218(0.2256) | Loss 3.407924(3.436964) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0141 | Time 0.2018(0.2239) | Loss 3.329247(3.429424) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0142 | Time 0.2264(0.2241) | Loss 3.335816(3.422871) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0143 | Time 0.2502(0.2259) | Loss 3.464075(3.425756) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0144 | Time 0.3730(0.2362) | Loss 3.410889(3.424715) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0145 | Time 0.2682(0.2385) | Loss 3.408948(3.423611) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0146 | Time 0.2743(0.2410) | Loss 3.413012(3.422869) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0147 | Time 0.2832(0.2439) | Loss 3.558583(3.432369) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0148 | Time 0.1833(0.2397) | Loss 3.438066(3.432768) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0149 | Time 0.2230(0.2385) | Loss 3.399160(3.430415) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0150 | Time 0.2239(0.2375) | Loss 3.367847(3.426036) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0151 | Time 0.2355(0.2374) | Loss 3.391573(3.423623) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0152 | Time 0.1980(0.2346) | Loss 3.424937(3.423715) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0153 | Time 0.2369(0.2348) | Loss 3.356177(3.418988) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0154 | Time 0.2778(0.2378) | Loss 3.312794(3.411554) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0155 | Time 0.2580(0.2392) | Loss 3.422394(3.412313) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0156 | Time 0.2421(0.2394) | Loss 3.363030(3.408863) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0157 | Time 0.2044(0.2369) | Loss 3.336647(3.403808) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0158 | Time 0.2160(0.2355) | Loss 3.409923(3.404236) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0159 | Time 0.2942(0.2396) | Loss 3.473405(3.409078) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0160 | Time 0.2777(0.2422) | Loss 3.333758(3.403805) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0161 | Time 0.2845(0.2452) | Loss 3.261350(3.393834) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0162 | Time 0.2440(0.2451) | Loss 3.439443(3.397026) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0163 | Time 0.1879(0.2411) | Loss 3.404807(3.397571) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0164 | Time 0.2266(0.2401) | Loss 3.385386(3.396718) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0165 | Time 0.2165(0.2384) | Loss 3.417053(3.398141) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0166 | Time 0.2335(0.2381) | Loss 3.418299(3.399552) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0167 | Time 0.1869(0.2345) | Loss 3.278907(3.391107) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0168 | Time 0.2239(0.2338) | Loss 3.360840(3.388989) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0169 | Time 0.2594(0.2356) | Loss 3.335742(3.385261) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0170 | Time 0.2259(0.2349) | Loss 3.222257(3.373851) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0171 | Time 0.2068(0.2329) | Loss 3.271115(3.366659) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0172 | Time 0.2246(0.2323) | Loss 3.349460(3.365456) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0173 | Time 0.2207(0.2315) | Loss 3.245214(3.357039) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0174 | Time 0.2033(0.2296) | Loss 3.287326(3.352159) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0175 | Time 0.2067(0.2280) | Loss 3.324172(3.350200) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0176 | Time 0.2088(0.2266) | Loss 3.347359(3.350001) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0177 | Time 0.2108(0.2255) | Loss 3.357779(3.350545) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0178 | Time 0.2070(0.2242) | Loss 3.296392(3.346755) | NFE Forward 20(20.0) | NFE Backward 20(20.0) | CNF Time 0.5000(0.5000)
Iter 0179 | Time 0.2453(0.2257) | Loss 3.310524(3.344218) | NFE Forward 20(20.0) | NFE Backward 26(20.4) | CNF Time 0.5000(0.5000)
Iter 0180 | Time 0.2361(0.2264) | Loss 3.345576(3.344313) | NFE Forward 20(20.0) | NFE Backward 20(20.4) | CNF Time 0.5000(0.5000)
Iter 0181 | Time 0.2410(0.2274) | Loss 3.311259(3.342000) | NFE Forward 20(20.0) | NFE Backward 20(20.4) | CNF Time 0.5000(0.5000)
Iter 0182 | Time 0.2658(0.2301) | Loss 3.346353(3.342304) | NFE Forward 20(20.0) | NFE Backward 26(20.8) | CNF Time 0.5000(0.5000)
Iter 0183 | Time 0.2302(0.2301) | Loss 3.227737(3.334285) | NFE Forward 20(20.0) | NFE Backward 26(21.1) | CNF Time 0.5000(0.5000)
Iter 0184 | Time 0.2406(0.2309) | Loss 3.371522(3.336891) | NFE Forward 20(20.0) | NFE Backward 20(21.0) | CNF Time 0.5000(0.5000)
Iter 0185 | Time 0.2555(0.2326) | Loss 3.533442(3.350650) | NFE Forward 20(20.0) | NFE Backward 26(21.4) | CNF Time 0.5000(0.5000)
Iter 0186 | Time 0.1953(0.2300) | Loss 3.230359(3.342229) | NFE Forward 20(20.0) | NFE Backward 20(21.3) | CNF Time 0.5000(0.5000)
Iter 0187 | Time 0.2161(0.2290) | Loss 3.323942(3.340949) | NFE Forward 26(20.4) | NFE Backward 20(21.2) | CNF Time 0.5000(0.5000)
Iter 0188 | Time 0.2525(0.2307) | Loss 3.175907(3.329396) | NFE Forward 26(20.8) | NFE Backward 20(21.1) | CNF Time 0.5000(0.5000)
Iter 0189 | Time 0.1971(0.2283) | Loss 3.321307(3.328830) | NFE Forward 20(20.8) | NFE Backward 20(21.0) | CNF Time 0.5000(0.5000)
Iter 0190 | Time 0.3249(0.2351) | Loss 3.276740(3.325184) | NFE Forward 26(21.1) | NFE Backward 26(21.4) | CNF Time 0.5000(0.5000)
Iter 0191 | Time 0.2544(0.2364) | Loss 3.176451(3.314773) | NFE Forward 20(21.0) | NFE Backward 26(21.7) | CNF Time 0.5000(0.5000)
Iter 0192 | Time 0.3380(0.2435) | Loss 3.258499(3.310833) | NFE Forward 26(21.4) | NFE Backward 26(22.0) | CNF Time 0.5000(0.5000)
Iter 0193 | Time 0.2675(0.2452) | Loss 3.245712(3.306275) | NFE Forward 20(21.3) | NFE Backward 26(22.3) | CNF Time 0.5000(0.5000)
Iter 0194 | Time 0.2585(0.2461) | Loss 3.187325(3.297948) | NFE Forward 20(21.2) | NFE Backward 26(22.6) | CNF Time 0.5000(0.5000)
Iter 0195 | Time 0.2541(0.2467) | Loss 3.238024(3.293754) | NFE Forward 26(21.5) | NFE Backward 20(22.4) | CNF Time 0.5000(0.5000)
Iter 0196 | Time 0.2614(0.2477) | Loss 3.214013(3.288172) | NFE Forward 26(21.9) | NFE Backward 26(22.6) | CNF Time 0.5000(0.5000)
Iter 0197 | Time 0.2866(0.2504) | Loss 3.253559(3.285749) | NFE Forward 26(22.1) | NFE Backward 26(22.9) | CNF Time 0.5000(0.5000)
Iter 0198 | Time 0.2631(0.2513) | Loss 3.228414(3.281735) | NFE Forward 26(22.4) | NFE Backward 26(23.1) | CNF Time 0.5000(0.5000)
Iter 0199 | Time 0.2823(0.2535) | Loss 3.323366(3.284650) | NFE Forward 26(22.7) | NFE Backward 26(23.3) | CNF Time 0.5000(0.5000)
Iter 0200 | Time 0.2835(0.2556) | Loss 3.204545(3.279042) | NFE Forward 26(22.9) | NFE Backward 26(23.5) | CNF Time 0.5000(0.5000)
[TEST] Iter 0200 | Test Loss 3.294578 | NFE 20
Iter 0201 | Time 0.3178(0.2599) | Loss 3.320126(3.281918) | NFE Forward 32(23.5) | NFE Backward 26(23.7) | CNF Time 0.5000(0.5000)
Iter 0202 | Time 0.3789(0.2683) | Loss 3.213924(3.277159) | NFE Forward 32(24.1) | NFE Backward 26(23.8) | CNF Time 0.5000(0.5000)
Iter 0203 | Time 0.3836(0.2763) | Loss 3.259859(3.275948) | NFE Forward 32(24.7) | NFE Backward 26(24.0) | CNF Time 0.5000(0.5000)
Iter 0204 | Time 0.3386(0.2807) | Loss 3.239931(3.273427) | NFE Forward 32(25.2) | NFE Backward 26(24.1) | CNF Time 0.5000(0.5000)
Iter 0205 | Time 0.3023(0.2822) | Loss 3.237503(3.270912) | NFE Forward 32(25.7) | NFE Backward 26(24.2) | CNF Time 0.5000(0.5000)
Iter 0206 | Time 0.4678(0.2952) | Loss 3.357677(3.276985) | NFE Forward 38(26.5) | NFE Backward 26(24.4) | CNF Time 0.5000(0.5000)
Iter 0207 | Time 0.2685(0.2933) | Loss 3.212217(3.272452) | NFE Forward 26(26.5) | NFE Backward 26(24.5) | CNF Time 0.5000(0.5000)
Iter 0208 | Time 0.3797(0.2994) | Loss 3.161199(3.264664) | NFE Forward 32(26.9) | NFE Backward 32(25.0) | CNF Time 0.5000(0.5000)
Iter 0209 | Time 0.3665(0.3041) | Loss 3.224376(3.261844) | NFE Forward 38(27.7) | NFE Backward 26(25.1) | CNF Time 0.5000(0.5000)
Iter 0210 | Time 0.2749(0.3020) | Loss 3.309770(3.265199) | NFE Forward 32(28.0) | NFE Backward 26(25.1) | CNF Time 0.5000(0.5000)
Iter 0211 | Time 0.3679(0.3066) | Loss 3.180097(3.259241) | NFE Forward 32(28.2) | NFE Backward 26(25.2) | CNF Time 0.5000(0.5000)
Iter 0212 | Time 0.3579(0.3102) | Loss 3.167770(3.252839) | NFE Forward 32(28.5) | NFE Backward 26(25.3) | CNF Time 0.5000(0.5000)
Iter 0213 | Time 0.3235(0.3112) | Loss 3.240489(3.251974) | NFE Forward 32(28.8) | NFE Backward 26(25.3) | CNF Time 0.5000(0.5000)
Iter 0214 | Time 0.2981(0.3102) | Loss 3.136347(3.243880) | NFE Forward 32(29.0) | NFE Backward 26(25.4) | CNF Time 0.5000(0.5000)
Iter 0215 | Time 0.5560(0.3275) | Loss 3.220428(3.242238) | NFE Forward 38(29.6) | NFE Backward 26(25.4) | CNF Time 0.5000(0.5000)
Iter 0216 | Time 0.6872(0.3526) | Loss 3.253985(3.243061) | NFE Forward 32(29.8) | NFE Backward 26(25.4) | CNF Time 0.5000(0.5000)
Iter 0217 | Time 1.8174(0.4552) | Loss 3.396479(3.253800) | NFE Forward 32(29.9) | NFE Backward 26(25.5) | CNF Time 0.5000(0.5000)
Iter 0218 | Time 0.9477(0.4896) | Loss 3.159868(3.247225) | NFE Forward 32(30.1) | NFE Backward 32(25.9) | CNF Time 0.5000(0.5000)
Iter 0219 | Time 0.3561(0.4803) | Loss 3.234252(3.246317) | NFE Forward 32(30.2) | NFE Backward 26(25.9) | CNF Time 0.5000(0.5000)
Iter 0220 | Time 0.4689(0.4795) | Loss 3.252120(3.246723) | NFE Forward 32(30.3) | NFE Backward 26(25.9) | CNF Time 0.5000(0.5000)
Iter 0221 | Time 0.3556(0.4708) | Loss 3.184294(3.242353) | NFE Forward 32(30.5) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0222 | Time 0.4515(0.4695) | Loss 3.322705(3.247977) | NFE Forward 32(30.6) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0223 | Time 0.4717(0.4696) | Loss 3.213758(3.245582) | NFE Forward 38(31.1) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0224 | Time 0.5038(0.4720) | Loss 3.111138(3.236171) | NFE Forward 38(31.6) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0225 | Time 0.5321(0.4762) | Loss 3.238674(3.236346) | NFE Forward 32(31.6) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0226 | Time 0.5350(0.4803) | Loss 3.175792(3.232107) | NFE Forward 38(32.0) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0227 | Time 0.3048(0.4680) | Loss 3.239650(3.232635) | NFE Forward 32(32.0) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0228 | Time 0.3997(0.4633) | Loss 3.286924(3.236436) | NFE Forward 38(32.5) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0229 | Time 0.3812(0.4575) | Loss 3.280758(3.239538) | NFE Forward 32(32.4) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0230 | Time 0.3581(0.4506) | Loss 3.087531(3.228898) | NFE Forward 38(32.8) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0231 | Time 0.3199(0.4414) | Loss 3.127555(3.221804) | NFE Forward 32(32.8) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0232 | Time 0.2755(0.4298) | Loss 3.151803(3.216904) | NFE Forward 32(32.7) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0233 | Time 0.3030(0.4209) | Loss 3.358048(3.226784) | NFE Forward 38(33.1) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0234 | Time 0.3040(0.4127) | Loss 3.057106(3.214906) | NFE Forward 38(33.4) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0235 | Time 0.3523(0.4085) | Loss 3.033033(3.202175) | NFE Forward 38(33.7) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0236 | Time 0.5929(0.4214) | Loss 3.138038(3.197686) | NFE Forward 32(33.6) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0237 | Time 0.3085(0.4135) | Loss 3.148366(3.194233) | NFE Forward 32(33.5) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0238 | Time 0.3289(0.4076) | Loss 3.115823(3.188745) | NFE Forward 38(33.8) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0239 | Time 0.2800(0.3986) | Loss 3.112914(3.183436) | NFE Forward 32(33.7) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0240 | Time 0.3002(0.3918) | Loss 3.113594(3.178547) | NFE Forward 32(33.6) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0241 | Time 0.3127(0.3862) | Loss 3.108152(3.173620) | NFE Forward 38(33.9) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0242 | Time 0.3060(0.3806) | Loss 3.061156(3.165747) | NFE Forward 38(34.2) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0243 | Time 0.3359(0.3775) | Loss 3.267993(3.172904) | NFE Forward 38(34.4) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0244 | Time 0.2895(0.3713) | Loss 3.059226(3.164947) | NFE Forward 38(34.7) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0245 | Time 0.3227(0.3679) | Loss 2.943312(3.149433) | NFE Forward 32(34.5) | NFE Backward 26(26.0) | CNF Time 0.5000(0.5000)
Iter 0246 | Time 0.3284(0.3652) | Loss 3.186715(3.152042) | NFE Forward 32(34.3) | NFE Backward 32(26.4) | CNF Time 0.5000(0.5000)
Iter 0247 | Time 0.3290(0.3626) | Loss 3.193326(3.154932) | NFE Forward 32(34.2) | NFE Backward 26(26.4) | CNF Time 0.5000(0.5000)
Iter 0248 | Time 0.3693(0.3631) | Loss 3.066538(3.148745) | NFE Forward 32(34.0) | NFE Backward 32(26.8) | CNF Time 0.5000(0.5000)
Iter 0249 | Time 0.2874(0.3578) | Loss 3.123601(3.146985) | NFE Forward 32(33.9) | NFE Backward 26(26.7) | CNF Time 0.5000(0.5000)
Iter 0250 | Time 0.3718(0.3588) | Loss 3.283460(3.156538) | NFE Forward 32(33.7) | NFE Backward 26(26.7) | CNF Time 0.5000(0.5000)
Iter 0251 | Time 0.6218(0.3772) | Loss 3.074957(3.150827) | NFE Forward 38(34.0) | NFE Backward 32(27.0) | CNF Time 0.5000(0.5000)
Iter 0252 | Time 0.4279(0.3807) | Loss 3.078518(3.145766) | NFE Forward 38(34.3) | NFE Backward 26(27.0) | CNF Time 0.5000(0.5000)
Iter 0253 | Time 1.4570(0.4561) | Loss 2.987548(3.134690) | NFE Forward 38(34.6) | NFE Backward 26(26.9) | CNF Time 0.5000(0.5000)
Iter 0254 | Time 1.4121(0.5230) | Loss 3.128944(3.134288) | NFE Forward 32(34.4) | NFE Backward 26(26.8) | CNF Time 0.5000(0.5000)
Iter 0255 | Time 0.3302(0.5095) | Loss 3.028537(3.126885) | NFE Forward 38(34.6) | NFE Backward 26(26.8) | CNF Time 0.5000(0.5000)
Iter 0256 | Time 0.4114(0.5026) | Loss 3.142614(3.127986) | NFE Forward 38(34.9) | NFE Backward 26(26.7) | CNF Time 0.5000(0.5000)
Iter 0257 | Time 0.4994(0.5024) | Loss 3.159080(3.130163) | NFE Forward 38(35.1) | NFE Backward 26(26.7) | CNF Time 0.5000(0.5000)
Iter 0258 | Time 0.4570(0.4992) | Loss 3.168078(3.132817) | NFE Forward 38(35.3) | NFE Backward 26(26.6) | CNF Time 0.5000(0.5000)
Iter 0259 | Time 0.4542(0.4961) | Loss 2.960459(3.120752) | NFE Forward 38(35.5) | NFE Backward 32(27.0) | CNF Time 0.5000(0.5000)
Iter 0260 | Time 0.3064(0.4828) | Loss 3.049869(3.115790) | NFE Forward 44(36.1) | NFE Backward 26(26.9) | CNF Time 0.5000(0.5000)
Iter 0261 | Time 0.4516(0.4806) | Loss 3.065855(3.112295) | NFE Forward 44(36.6) | NFE Backward 38(27.7) | CNF Time 0.5000(0.5000)
Iter 0262 | Time 0.6192(0.4903) | Loss 3.348483(3.128828) | NFE Forward 32(36.3) | NFE Backward 26(27.6) | CNF Time 0.5000(0.5000)
Iter 0263 | Time 0.3387(0.4797) | Loss 3.016657(3.120976) | NFE Forward 32(36.0) | NFE Backward 32(27.9) | CNF Time 0.5000(0.5000)
Iter 0264 | Time 0.4657(0.4787) | Loss 2.949180(3.108950) | NFE Forward 32(35.7) | NFE Backward 26(27.8) | CNF Time 0.5000(0.5000)
Iter 0265 | Time 0.4198(0.4746) | Loss 2.846035(3.090546) | NFE Forward 38(35.9) | NFE Backward 32(28.1) | CNF Time 0.5000(0.5000)
Iter 0266 | Time 0.3507(0.4659) | Loss 3.028754(3.086221) | NFE Forward 38(36.0) | NFE Backward 32(28.3) | CNF Time 0.5000(0.5000)
Iter 0267 | Time 0.4560(0.4652) | Loss 3.024353(3.081890) | NFE Forward 38(36.2) | NFE Backward 38(29.0) | CNF Time 0.5000(0.5000)
Iter 0268 | Time 0.3432(0.4567) | Loss 3.221767(3.091681) | NFE Forward 38(36.3) | NFE Backward 32(29.2) | CNF Time 0.5000(0.5000)
Iter 0269 | Time 0.4116(0.4535) | Loss 3.104503(3.092579) | NFE Forward 44(36.8) | NFE Backward 32(29.4) | CNF Time 0.5000(0.5000)
Iter 0270 | Time 0.3927(0.4493) | Loss 3.044426(3.089208) | NFE Forward 38(36.9) | NFE Backward 32(29.6) | CNF Time 0.5000(0.5000)
Iter 0271 | Time 0.3590(0.4430) | Loss 2.995866(3.082674) | NFE Forward 50(37.8) | NFE Backward 32(29.8) | CNF Time 0.5000(0.5000)
Iter 0272 | Time 0.4100(0.4406) | Loss 3.098134(3.083756) | NFE Forward 50(38.7) | NFE Backward 32(29.9) | CNF Time 0.5000(0.5000)
Iter 0273 | Time 0.6121(0.4526) | Loss 3.265804(3.096500) | NFE Forward 32(38.2) | NFE Backward 38(30.5) | CNF Time 0.5000(0.5000)
Iter 0274 | Time 0.5526(0.4596) | Loss 3.011762(3.090568) | NFE Forward 44(38.6) | NFE Backward 32(30.6) | CNF Time 0.5000(0.5000)
Iter 0275 | Time 0.4312(0.4577) | Loss 3.057836(3.088277) | NFE Forward 44(39.0) | NFE Backward 38(31.1) | CNF Time 0.5000(0.5000)
Iter 0276 | Time 0.7047(0.4750) | Loss 3.095618(3.088791) | NFE Forward 38(38.9) | NFE Backward 38(31.6) | CNF Time 0.5000(0.5000)
Iter 0277 | Time 0.4169(0.4709) | Loss 3.037020(3.085167) | NFE Forward 38(38.9) | NFE Backward 32(31.6) | CNF Time 0.5000(0.5000)
Iter 0278 | Time 0.5361(0.4754) | Loss 2.939586(3.074976) | NFE Forward 38(38.8) | NFE Backward 32(31.6) | CNF Time 0.5000(0.5000)
Iter 0279 | Time 0.6338(0.4865) | Loss 3.110901(3.077491) | NFE Forward 50(39.6) | NFE Backward 32(31.7) | CNF Time 0.5000(0.5000)
Iter 0280 | Time 0.7864(0.5075) | Loss 2.830476(3.060200) | NFE Forward 44(39.9) | NFE Backward 32(31.7) | CNF Time 0.5000(0.5000)
Iter 0281 | Time 0.4374(0.5026) | Loss 3.213459(3.070928) | NFE Forward 50(40.6) | NFE Backward 32(31.7) | CNF Time 0.5000(0.5000)
Iter 0282 | Time 0.5304(0.5046) | Loss 3.033328(3.068296) | NFE Forward 44(40.8) | NFE Backward 32(31.7) | CNF Time 0.5000(0.5000)
Iter 0283 | Time 0.4246(0.4990) | Loss 3.013351(3.064450) | NFE Forward 44(41.1) | NFE Backward 32(31.8) | CNF Time 0.5000(0.5000)
Iter 0284 | Time 0.3977(0.4919) | Loss 3.028820(3.061956) | NFE Forward 44(41.3) | NFE Backward 32(31.8) | CNF Time 0.5000(0.5000)
Iter 0285 | Time 0.4669(0.4901) | Loss 3.012020(3.058460) | NFE Forward 44(41.5) | NFE Backward 32(31.8) | CNF Time 0.5000(0.5000)
Iter 0286 | Time 0.3480(0.4802) | Loss 2.994902(3.054011) | NFE Forward 38(41.2) | NFE Backward 32(31.8) | CNF Time 0.5000(0.5000)
Iter 0287 | Time 0.8206(0.5040) | Loss 3.127625(3.059164) | NFE Forward 44(41.4) | NFE Backward 32(31.8) | CNF Time 0.5000(0.5000)
Iter 0288 | Time 0.5005(0.5038) | Loss 3.363303(3.080454) | NFE Forward 38(41.2) | NFE Backward 38(32.3) | CNF Time 0.5000(0.5000)
Iter 0289 | Time 0.4808(0.5022) | Loss 3.146558(3.085081) | NFE Forward 44(41.4) | NFE Backward 38(32.7) | CNF Time 0.5000(0.5000)
Iter 0290 | Time 0.4815(0.5007) | Loss 3.186059(3.092150) | NFE Forward 38(41.1) | NFE Backward 32(32.6) | CNF Time 0.5000(0.5000)
Iter 0291 | Time 0.5799(0.5063) | Loss 3.122025(3.094241) | NFE Forward 38(40.9) | NFE Backward 38(33.0) | CNF Time 0.5000(0.5000)
Iter 0292 | Time 0.5069(0.5063) | Loss 3.153439(3.098385) | NFE Forward 50(41.6) | NFE Backward 32(32.9) | CNF Time 0.5000(0.5000)
Iter 0293 | Time 0.3664(0.4965) | Loss 3.152926(3.102203) | NFE Forward 38(41.3) | NFE Backward 32(32.9) | CNF Time 0.5000(0.5000)
Iter 0294 | Time 0.4689(0.4946) | Loss 3.282609(3.114831) | NFE Forward 38(41.1) | NFE Backward 38(33.2) | CNF Time 0.5000(0.5000)
Iter 0295 | Time 0.3991(0.4879) | Loss 2.974107(3.104980) | NFE Forward 50(41.7) | NFE Backward 32(33.1) | CNF Time 0.5000(0.5000)
Iter 0296 | Time 0.3410(0.4776) | Loss 3.076125(3.102961) | NFE Forward 38(41.4) | NFE Backward 32(33.0) | CNF Time 0.5000(0.5000)
Iter 0297 | Time 0.4114(0.4730) | Loss 2.797441(3.081574) | NFE Forward 38(41.2) | NFE Backward 32(33.0) | CNF Time 0.5000(0.5000)
Iter 0298 | Time 0.4114(0.4687) | Loss 3.085316(3.081836) | NFE Forward 44(41.4) | NFE Backward 32(32.9) | CNF Time 0.5000(0.5000)
Iter 0299 | Time 0.4552(0.4677) | Loss 2.977237(3.074514) | NFE Forward 44(41.6) | NFE Backward 32(32.8) | CNF Time 0.5000(0.5000)
Iter 0300 | Time 0.4937(0.4695) | Loss 2.822181(3.056851) | NFE Forward 44(41.7) | NFE Backward 32(32.8) | CNF Time 0.5000(0.5000)
[TEST] Iter 0300 | Test Loss 2.919248 | NFE 38
Iter 0301 | Time 0.5035(0.4719) | Loss 2.940650(3.048717) | NFE Forward 44(41.9) | NFE Backward 32(32.7) | CNF Time 0.5000(0.5000)
Iter 0302 | Time 0.4737(0.4720) | Loss 2.991664(3.044723) | NFE Forward 44(42.1) | NFE Backward 32(32.7) | CNF Time 0.5000(0.5000)
Iter 0303 | Time 0.3750(0.4652) | Loss 2.920872(3.036054) | NFE Forward 44(42.2) | NFE Backward 32(32.6) | CNF Time 0.5000(0.5000)
Iter 0304 | Time 0.4618(0.4650) | Loss 2.809242(3.020177) | NFE Forward 44(42.3) | NFE Backward 32(32.6) | CNF Time 0.5000(0.5000)
Iter 0305 | Time 0.3904(0.4598) | Loss 3.151831(3.029393) | NFE Forward 44(42.4) | NFE Backward 38(33.0) | CNF Time 0.5000(0.5000)
Iter 0306 | Time 0.3867(0.4547) | Loss 2.904185(3.020628) | NFE Forward 50(43.0) | NFE Backward 32(32.9) | CNF Time 0.5000(0.5000)
Iter 0307 | Time 0.3903(0.4502) | Loss 3.062905(3.023587) | NFE Forward 44(43.0) | NFE Backward 38(33.3) | CNF Time 0.5000(0.5000)
Iter 0308 | Time 0.3938(0.4462) | Loss 2.985234(3.020903) | NFE Forward 38(42.7) | NFE Backward 32(33.2) | CNF Time 0.5000(0.5000)
Iter 0309 | Time 0.3475(0.4393) | Loss 2.874837(3.010678) | NFE Forward 38(42.4) | NFE Backward 32(33.1) | CNF Time 0.5000(0.5000)
Iter 0310 | Time 0.3559(0.4335) | Loss 2.807353(2.996445) | NFE Forward 44(42.5) | NFE Backward 32(33.0) | CNF Time 0.5000(0.5000)
Iter 0311 | Time 0.3931(0.4306) | Loss 2.886122(2.988723) | NFE Forward 44(42.6) | NFE Backward 32(32.9) | CNF Time 0.5000(0.5000)
Iter 0312 | Time 0.3551(0.4254) | Loss 2.818822(2.976830) | NFE Forward 44(42.7) | NFE Backward 32(32.9) | CNF Time 0.5000(0.5000)
Iter 0313 | Time 0.6832(0.4434) | Loss 3.023978(2.980130) | NFE Forward 50(43.2) | NFE Backward 44(33.7) | CNF Time 0.5000(0.5000)
Iter 0314 | Time 0.3392(0.4361) | Loss 3.140879(2.991383) | NFE Forward 38(42.8) | NFE Backward 32(33.5) | CNF Time 0.5000(0.5000)
Iter 0315 | Time 0.4281(0.4356) | Loss 2.863600(2.982438) | NFE Forward 44(42.9) | NFE Backward 32(33.4) | CNF Time 0.5000(0.5000)
Iter 0316 | Time 0.3880(0.4322) | Loss 2.982076(2.982412) | NFE Forward 38(42.6) | NFE Backward 32(33.3) | CNF Time 0.5000(0.5000)
Iter 0317 | Time 0.3810(0.4286) | Loss 3.001722(2.983764) | NFE Forward 44(42.7) | NFE Backward 32(33.2) | CNF Time 0.5000(0.5000)
Iter 0318 | Time 0.4501(0.4301) | Loss 2.994447(2.984512) | NFE Forward 44(42.8) | NFE Backward 38(33.6) | CNF Time 0.5000(0.5000)
Iter 0319 | Time 0.3664(0.4257) | Loss 2.911503(2.979401) | NFE Forward 38(42.4) | NFE Backward 38(33.9) | CNF Time 0.5000(0.5000)
Iter 0320 | Time 0.4214(0.4254) | Loss 2.950794(2.977399) | NFE Forward 50(43.0) | NFE Backward 38(34.2) | CNF Time 0.5000(0.5000)
Iter 0321 | Time 0.4560(0.4275) | Loss 2.999332(2.978934) | NFE Forward 50(43.4) | NFE Backward 32(34.0) | CNF Time 0.5000(0.5000)
Iter 0322 | Time 0.4075(0.4261) | Loss 2.982517(2.979185) | NFE Forward 50(43.9) | NFE Backward 38(34.3) | CNF Time 0.5000(0.5000)
Iter 0323 | Time 0.4354(0.4268) | Loss 2.764101(2.964129) | NFE Forward 44(43.9) | NFE Backward 38(34.6) | CNF Time 0.5000(0.5000)
Iter 0324 | Time 0.3783(0.4234) | Loss 3.051284(2.970230) | NFE Forward 44(43.9) | NFE Backward 32(34.4) | CNF Time 0.5000(0.5000)
Iter 0325 | Time 0.3997(0.4217) | Loss 3.001770(2.972438) | NFE Forward 44(43.9) | NFE Backward 32(34.2) | CNF Time 0.5000(0.5000)
Iter 0326 | Time 0.4201(0.4216) | Loss 3.097375(2.981183) | NFE Forward 56(44.8) | NFE Backward 32(34.1) | CNF Time 0.5000(0.5000)
Iter 0327 | Time 0.5441(0.4302) | Loss 3.152189(2.993154) | NFE Forward 44(44.7) | NFE Backward 38(34.3) | CNF Time 0.5000(0.5000)
Iter 0328 | Time 0.4728(0.4332) | Loss 2.906904(2.987116) | NFE Forward 50(45.1) | NFE Backward 38(34.6) | CNF Time 0.5000(0.5000)
Iter 0329 | Time 0.4771(0.4362) | Loss 2.967066(2.985713) | NFE Forward 50(45.4) | NFE Backward 44(35.2) | CNF Time 0.5000(0.5000)
Iter 0330 | Time 0.4609(0.4380) | Loss 2.788177(2.971885) | NFE Forward 50(45.8) | NFE Backward 44(35.9) | CNF Time 0.5000(0.5000)
Iter 0331 | Time 0.3861(0.4343) | Loss 2.864381(2.964360) | NFE Forward 44(45.6) | NFE Backward 32(35.6) | CNF Time 0.5000(0.5000)
Iter 0332 | Time 0.5234(0.4406) | Loss 3.028373(2.968841) | NFE Forward 44(45.5) | NFE Backward 32(35.3) | CNF Time 0.5000(0.5000)
Iter 0333 | Time 0.5137(0.4457) | Loss 2.814188(2.958015) | NFE Forward 44(45.4) | NFE Backward 38(35.5) | CNF Time 0.5000(0.5000)
Iter 0334 | Time 0.3676(0.4402) | Loss 2.857726(2.950995) | NFE Forward 50(45.7) | NFE Backward 32(35.3) | CNF Time 0.5000(0.5000)
Iter 0335 | Time 0.3810(0.4361) | Loss 3.016971(2.955613) | NFE Forward 38(45.2) | NFE Backward 32(35.0) | CNF Time 0.5000(0.5000)
Iter 0336 | Time 0.4251(0.4353) | Loss 3.022256(2.960278) | NFE Forward 44(45.1) | NFE Backward 32(34.8) | CNF Time 0.5000(0.5000)
Iter 0337 | Time 1.0306(0.4770) | Loss 2.819177(2.950401) | NFE Forward 44(45.0) | NFE Backward 32(34.6) | CNF Time 0.5000(0.5000)
Iter 0338 | Time 0.4859(0.4776) | Loss 2.718377(2.934160) | NFE Forward 56(45.8) | NFE Backward 32(34.5) | CNF Time 0.5000(0.5000)
Iter 0339 | Time 0.5429(0.4822) | Loss 3.010448(2.939500) | NFE Forward 56(46.5) | NFE Backward 38(34.7) | CNF Time 0.5000(0.5000)
Iter 0340 | Time 0.5972(0.4902) | Loss 2.887828(2.935883) | NFE Forward 50(46.8) | NFE Backward 44(35.4) | CNF Time 0.5000(0.5000)
Iter 0341 | Time 0.4400(0.4867) | Loss 3.087336(2.946484) | NFE Forward 38(46.1) | NFE Backward 44(36.0) | CNF Time 0.5000(0.5000)
Iter 0342 | Time 0.5223(0.4892) | Loss 2.837238(2.938837) | NFE Forward 44(46.0) | NFE Backward 38(36.1) | CNF Time 0.5000(0.5000)
Iter 0343 | Time 0.4713(0.4880) | Loss 2.967279(2.940828) | NFE Forward 44(45.9) | NFE Backward 44(36.7) | CNF Time 0.5000(0.5000)
Iter 0344 | Time 0.5792(0.4943) | Loss 2.952105(2.941617) | NFE Forward 50(46.1) | NFE Backward 38(36.7) | CNF Time 0.5000(0.5000)
Iter 0345 | Time 0.5624(0.4991) | Loss 3.087942(2.951860) | NFE Forward 62(47.3) | NFE Backward 44(37.3) | CNF Time 0.5000(0.5000)
Iter 0346 | Time 0.7440(0.5163) | Loss 3.034044(2.957613) | NFE Forward 50(47.4) | NFE Backward 44(37.7) | CNF Time 0.5000(0.5000)
Iter 0347 | Time 0.4158(0.5092) | Loss 2.962233(2.957936) | NFE Forward 44(47.2) | NFE Backward 38(37.7) | CNF Time 0.5000(0.5000)
Iter 0348 | Time 0.4732(0.5067) | Loss 2.800765(2.946934) | NFE Forward 50(47.4) | NFE Backward 44(38.2) | CNF Time 0.5000(0.5000)
Iter 0349 | Time 0.6195(0.5146) | Loss 3.032055(2.952893) | NFE Forward 50(47.6) | NFE Backward 44(38.6) | CNF Time 0.5000(0.5000)
Iter 0350 | Time 0.6219(0.5221) | Loss 2.848901(2.945613) | NFE Forward 62(48.6) | NFE Backward 38(38.5) | CNF Time 0.5000(0.5000)
Iter 0351 | Time 0.6495(0.5310) | Loss 3.176021(2.961742) | NFE Forward 50(48.7) | NFE Backward 44(38.9) | CNF Time 0.5000(0.5000)
Iter 0352 | Time 0.6356(0.5384) | Loss 2.957957(2.961477) | NFE Forward 56(49.2) | NFE Backward 38(38.9) | CNF Time 0.5000(0.5000)
Iter 0353 | Time 0.4540(0.5324) | Loss 2.997511(2.963999) | NFE Forward 44(48.8) | NFE Backward 38(38.8) | CNF Time 0.5000(0.5000)
Iter 0354 | Time 0.7363(0.5467) | Loss 2.879313(2.958071) | NFE Forward 50(48.9) | NFE Backward 50(39.6) | CNF Time 0.5000(0.5000)
Iter 0355 | Time 0.6488(0.5539) | Loss 2.776997(2.945396) | NFE Forward 50(49.0) | NFE Backward 44(39.9) | CNF Time 0.5000(0.5000)
Iter 0356 | Time 0.4090(0.5437) | Loss 2.929705(2.944298) | NFE Forward 44(48.6) | NFE Backward 38(39.8) | CNF Time 0.5000(0.5000)
Iter 0357 | Time 0.3364(0.5292) | Loss 2.857500(2.938222) | NFE Forward 44(48.3) | NFE Backward 32(39.2) | CNF Time 0.5000(0.5000)
Iter 0358 | Time 0.4649(0.5247) | Loss 2.932161(2.937798) | NFE Forward 56(48.9) | NFE Backward 44(39.6) | CNF Time 0.5000(0.5000)
Iter 0359 | Time 0.4578(0.5200) | Loss 2.801183(2.928235) | NFE Forward 50(48.9) | NFE Backward 38(39.4) | CNF Time 0.5000(0.5000)
Iter 0360 | Time 0.4269(0.5135) | Loss 2.830724(2.921409) | NFE Forward 44(48.6) | NFE Backward 38(39.3) | CNF Time 0.5000(0.5000)
Iter 0361 | Time 0.4479(0.5089) | Loss 2.892883(2.919412) | NFE Forward 56(49.1) | NFE Backward 38(39.3) | CNF Time 0.5000(0.5000)
Iter 0362 | Time 0.4174(0.5025) | Loss 2.949485(2.921517) | NFE Forward 44(48.8) | NFE Backward 38(39.2) | CNF Time 0.5000(0.5000)
Iter 0363 | Time 0.4551(0.4992) | Loss 2.855430(2.916891) | NFE Forward 50(48.8) | NFE Backward 44(39.5) | CNF Time 0.5000(0.5000)
Iter 0364 | Time 0.4026(0.4924) | Loss 2.899692(2.915687) | NFE Forward 56(49.3) | NFE Backward 38(39.4) | CNF Time 0.5000(0.5000)
Iter 0365 | Time 0.5114(0.4938) | Loss 2.807372(2.908105) | NFE Forward 50(49.4) | NFE Backward 32(38.9) | CNF Time 0.5000(0.5000)
Iter 0366 | Time 0.4715(0.4922) | Loss 2.925235(2.909304) | NFE Forward 50(49.4) | NFE Backward 44(39.2) | CNF Time 0.5000(0.5000)
Iter 0367 | Time 0.4703(0.4907) | Loss 2.943973(2.911731) | NFE Forward 50(49.5) | NFE Backward 38(39.2) | CNF Time 0.5000(0.5000)
Iter 0368 | Time 0.4541(0.4881) | Loss 2.634648(2.892335) | NFE Forward 44(49.1) | NFE Backward 44(39.5) | CNF Time 0.5000(0.5000)
Iter 0369 | Time 0.5354(0.4914) | Loss 2.841966(2.888809) | NFE Forward 50(49.2) | NFE Backward 38(39.4) | CNF Time 0.5000(0.5000)
Iter 0370 | Time 0.3947(0.4846) | Loss 2.897549(2.889421) | NFE Forward 50(49.2) | NFE Backward 38(39.3) | CNF Time 0.5000(0.5000)
Iter 0371 | Time 0.4561(0.4826) | Loss 2.800871(2.883223) | NFE Forward 50(49.3) | NFE Backward 38(39.2) | CNF Time 0.5000(0.5000)
Iter 0372 | Time 0.4124(0.4777) | Loss 2.934225(2.886793) | NFE Forward 50(49.3) | NFE Backward 38(39.1) | CNF Time 0.5000(0.5000)
Iter 0373 | Time 0.4329(0.4746) | Loss 2.985854(2.893727) | NFE Forward 56(49.8) | NFE Backward 32(38.6) | CNF Time 0.5000(0.5000)
Iter 0374 | Time 0.4734(0.4745) | Loss 3.064893(2.905709) | NFE Forward 44(49.4) | NFE Backward 50(39.4) | CNF Time 0.5000(0.5000)
Iter 0375 | Time 0.6829(0.4891) | Loss 2.823709(2.899969) | NFE Forward 50(49.4) | NFE Backward 44(39.7) | CNF Time 0.5000(0.5000)
Iter 0376 | Time 0.5085(0.4905) | Loss 2.857194(2.896974) | NFE Forward 50(49.5) | NFE Backward 44(40.0) | CNF Time 0.5000(0.5000)
Iter 0377 | Time 0.4411(0.4870) | Loss 2.905309(2.897558) | NFE Forward 50(49.5) | NFE Backward 38(39.9) | CNF Time 0.5000(0.5000)
Iter 0378 | Time 0.4159(0.4820) | Loss 2.968128(2.902498) | NFE Forward 56(50.0) | NFE Backward 38(39.8) | CNF Time 0.5000(0.5000)
Iter 0379 | Time 0.7572(0.5013) | Loss 2.973777(2.907487) | NFE Forward 44(49.5) | NFE Backward 50(40.5) | CNF Time 0.5000(0.5000)
Iter 0380 | Time 0.4194(0.4956) | Loss 2.809894(2.900656) | NFE Forward 56(50.0) | NFE Backward 38(40.3) | CNF Time 0.5000(0.5000)
Iter 0381 | Time 0.5005(0.4959) | Loss 2.988165(2.906781) | NFE Forward 50(50.0) | NFE Backward 38(40.1) | CNF Time 0.5000(0.5000)
Iter 0382 | Time 0.4283(0.4912) | Loss 2.794873(2.898948) | NFE Forward 50(50.0) | NFE Backward 32(39.6) | CNF Time 0.5000(0.5000)
Iter 0383 | Time 0.5491(0.4952) | Loss 2.861940(2.896357) | NFE Forward 44(49.6) | NFE Backward 44(39.9) | CNF Time 0.5000(0.5000)
Iter 0384 | Time 0.6348(0.5050) | Loss 3.099295(2.910563) | NFE Forward 44(49.2) | NFE Backward 56(41.0) | CNF Time 0.5000(0.5000)
Iter 0385 | Time 0.4841(0.5035) | Loss 2.841438(2.905724) | NFE Forward 56(49.7) | NFE Backward 38(40.8) | CNF Time 0.5000(0.5000)
Iter 0386 | Time 0.6379(0.5129) | Loss 2.865856(2.902933) | NFE Forward 44(49.3) | NFE Backward 44(41.0) | CNF Time 0.5000(0.5000)
Iter 0387 | Time 0.4718(0.5101) | Loss 2.909683(2.903406) | NFE Forward 50(49.3) | NFE Backward 44(41.2) | CNF Time 0.5000(0.5000)
Iter 0388 | Time 0.4536(0.5061) | Loss 2.869387(2.901024) | NFE Forward 50(49.4) | NFE Backward 44(41.4) | CNF Time 0.5000(0.5000)
Iter 0389 | Time 0.6794(0.5182) | Loss 2.769845(2.891842) | NFE Forward 62(50.2) | NFE Backward 50(42.0) | CNF Time 0.5000(0.5000)
Iter 0390 | Time 0.6124(0.5248) | Loss 2.829738(2.887495) | NFE Forward 50(50.2) | NFE Backward 38(41.7) | CNF Time 0.5000(0.5000)
Iter 0391 | Time 0.6450(0.5332) | Loss 2.958787(2.892485) | NFE Forward 56(50.6) | NFE Backward 44(41.9) | CNF Time 0.5000(0.5000)
Iter 0392 | Time 0.5025(0.5311) | Loss 2.843661(2.889067) | NFE Forward 56(51.0) | NFE Backward 38(41.6) | CNF Time 0.5000(0.5000)
Iter 0393 | Time 0.4489(0.5253) | Loss 2.945890(2.893045) | NFE Forward 50(50.9) | NFE Backward 38(41.4) | CNF Time 0.5000(0.5000)
Iter 0394 | Time 0.4393(0.5193) | Loss 2.867403(2.891250) | NFE Forward 56(51.3) | NFE Backward 38(41.1) | CNF Time 0.5000(0.5000)
Iter 0395 | Time 0.5245(0.5197) | Loss 2.831695(2.887081) | NFE Forward 50(51.2) | NFE Backward 44(41.3) | CNF Time 0.5000(0.5000)
Iter 0396 | Time 0.6199(0.5267) | Loss 2.684647(2.872911) | NFE Forward 50(51.1) | NFE Backward 44(41.5) | CNF Time 0.5000(0.5000)
Iter 0397 | Time 0.4339(0.5202) | Loss 2.736977(2.863395) | NFE Forward 50(51.0) | NFE Backward 38(41.3) | CNF Time 0.5000(0.5000)
Iter 0398 | Time 0.4753(0.5171) | Loss 3.083808(2.878824) | NFE Forward 56(51.4) | NFE Backward 38(41.0) | CNF Time 0.5000(0.5000)
Iter 0399 | Time 0.4325(0.5111) | Loss 2.815238(2.874373) | NFE Forward 50(51.3) | NFE Backward 44(41.3) | CNF Time 0.5000(0.5000)
Iter 0400 | Time 0.4409(0.5062) | Loss 2.908501(2.876762) | NFE Forward 56(51.6) | NFE Backward 38(41.0) | CNF Time 0.5000(0.5000)
[TEST] Iter 0400 | Test Loss 2.812366 | NFE 44
Iter 0401 | Time 0.5876(0.5119) | Loss 3.056694(2.889357) | NFE Forward 56(51.9) | NFE Backward 38(40.8) | CNF Time 0.5000(0.5000)
Iter 0402 | Time 0.5272(0.5130) | Loss 2.667996(2.873862) | NFE Forward 50(51.8) | NFE Backward 38(40.6) | CNF Time 0.5000(0.5000)
Iter 0403 | Time 0.7062(0.5265) | Loss 3.118664(2.890998) | NFE Forward 44(51.2) | NFE Backward 50(41.3) | CNF Time 0.5000(0.5000)
Iter 0404 | Time 0.7123(0.5395) | Loss 2.882653(2.890414) | NFE Forward 50(51.2) | NFE Backward 44(41.5) | CNF Time 0.5000(0.5000)
Iter 0405 | Time 0.8180(0.5590) | Loss 3.027123(2.899984) | NFE Forward 44(50.7) | NFE Backward 38(41.2) | CNF Time 0.5000(0.5000)
Iter 0406 | Time 0.5343(0.5573) | Loss 2.762736(2.890377) | NFE Forward 50(50.6) | NFE Backward 50(41.8) | CNF Time 0.5000(0.5000)
Iter 0407 | Time 0.5969(0.5601) | Loss 3.081166(2.903732) | NFE Forward 56(51.0) | NFE Backward 44(42.0) | CNF Time 0.5000(0.5000)
Iter 0408 | Time 0.5973(0.5627) | Loss 3.009181(2.911113) | NFE Forward 56(51.3) | NFE Backward 50(42.5) | CNF Time 0.5000(0.5000)
Iter 0409 | Time 0.4769(0.5567) | Loss 2.837173(2.905937) | NFE Forward 62(52.1) | NFE Backward 38(42.2) | CNF Time 0.5000(0.5000)
Iter 0410 | Time 0.6279(0.5616) | Loss 2.736650(2.894087) | NFE Forward 56(52.4) | NFE Backward 38(41.9) | CNF Time 0.5000(0.5000)
Iter 0411 | Time 0.5667(0.5620) | Loss 2.856023(2.891423) | NFE Forward 44(51.8) | NFE Backward 56(42.9) | CNF Time 0.5000(0.5000)
Iter 0412 | Time 0.5004(0.5577) | Loss 2.824805(2.886760) | NFE Forward 50(51.7) | NFE Backward 44(43.0) | CNF Time 0.5000(0.5000)
Iter 0413 | Time 0.7241(0.5693) | Loss 2.884692(2.886615) | NFE Forward 56(52.0) | NFE Backward 44(43.1) | CNF Time 0.5000(0.5000)
Iter 0414 | Time 1.2252(0.6152) | Loss 2.911721(2.888372) | NFE Forward 56(52.2) | NFE Backward 44(43.1) | CNF Time 0.5000(0.5000)
Iter 0415 | Time 1.0103(0.6429) | Loss 2.841052(2.885060) | NFE Forward 44(51.7) | NFE Backward 44(43.2) | CNF Time 0.5000(0.5000)
Iter 0416 | Time 0.4570(0.6299) | Loss 2.716424(2.873255) | NFE Forward 50(51.5) | NFE Backward 38(42.8) | CNF Time 0.5000(0.5000)
Iter 0417 | Time 0.6160(0.6289) | Loss 2.712745(2.862020) | NFE Forward 62(52.3) | NFE Backward 44(42.9) | CNF Time 0.5000(0.5000)
Iter 0418 | Time 0.5073(0.6204) | Loss 2.832726(2.859969) | NFE Forward 56(52.5) | NFE Backward 44(43.0) | CNF Time 0.5000(0.5000)
Iter 0419 | Time 0.4871(0.6111) | Loss 2.843350(2.858806) | NFE Forward 56(52.8) | NFE Backward 44(43.1) | CNF Time 0.5000(0.5000)
Iter 0420 | Time 0.5901(0.6096) | Loss 2.889367(2.860945) | NFE Forward 56(53.0) | NFE Backward 50(43.5) | CNF Time 0.5000(0.5000)
Iter 0421 | Time 0.5751(0.6072) | Loss 2.919118(2.865017) | NFE Forward 50(52.8) | NFE Backward 44(43.6) | CNF Time 0.5000(0.5000)
Iter 0422 | Time 0.5690(0.6045) | Loss 2.800242(2.860483) | NFE Forward 56(53.0) | NFE Backward 50(44.0) | CNF Time 0.5000(0.5000)
Iter 0423 | Time 0.5020(0.5973) | Loss 2.871634(2.861264) | NFE Forward 56(53.2) | NFE Backward 44(44.0) | CNF Time 0.5000(0.5000)
Iter 0424 | Time 0.4800(0.5891) | Loss 2.946967(2.867263) | NFE Forward 62(53.8) | NFE Backward 38(43.6) | CNF Time 0.5000(0.5000)
Iter 0425 | Time 0.5312(0.5851) | Loss 3.036946(2.879141) | NFE Forward 56(54.0) | NFE Backward 44(43.6) | CNF Time 0.5000(0.5000)
Iter 0426 | Time 0.5935(0.5857) | Loss 2.768270(2.871380) | NFE Forward 50(53.7) | NFE Backward 44(43.7) | CNF Time 0.5000(0.5000)
Iter 0427 | Time 0.4195(0.5740) | Loss 2.825210(2.868148) | NFE Forward 50(53.5) | NFE Backward 38(43.3) | CNF Time 0.5000(0.5000)
Iter 0428 | Time 0.6523(0.5795) | Loss 2.900282(2.870397) | NFE Forward 62(54.1) | NFE Backward 38(42.9) | CNF Time 0.5000(0.5000)
Iter 0429 | Time 0.4215(0.5685) | Loss 2.787644(2.864604) | NFE Forward 50(53.8) | NFE Backward 38(42.5) | CNF Time 0.5000(0.5000)
Iter 0430 | Time 0.5010(0.5637) | Loss 2.662193(2.850436) | NFE Forward 68(54.8) | NFE Backward 38(42.2) | CNF Time 0.5000(0.5000)
Iter 0431 | Time 0.4901(0.5586) | Loss 2.739490(2.842669) | NFE Forward 50(54.4) | NFE Backward 38(41.9) | CNF Time 0.5000(0.5000)
Iter 0432 | Time 0.4866(0.5535) | Loss 2.690832(2.832041) | NFE Forward 44(53.7) | NFE Backward 44(42.1) | CNF Time 0.5000(0.5000)
Iter 0433 | Time 0.4848(0.5487) | Loss 2.755750(2.826701) | NFE Forward 50(53.4) | NFE Backward 38(41.8) | CNF Time 0.5000(0.5000)
Iter 0434 | Time 0.4381(0.5410) | Loss 2.940908(2.834695) | NFE Forward 56(53.6) | NFE Backward 38(41.5) | CNF Time 0.5000(0.5000)
Iter 0435 | Time 0.4645(0.5356) | Loss 2.768167(2.830038) | NFE Forward 50(53.4) | NFE Backward 50(42.1) | CNF Time 0.5000(0.5000)
Iter 0436 | Time 0.4484(0.5295) | Loss 2.757695(2.824974) | NFE Forward 56(53.6) | NFE Backward 44(42.3) | CNF Time 0.5000(0.5000)
Iter 0437 | Time 0.5534(0.5312) | Loss 2.840401(2.826054) | NFE Forward 56(53.7) | NFE Backward 56(43.2) | CNF Time 0.5000(0.5000)
Iter 0438 | Time 0.5724(0.5341) | Loss 2.951601(2.834842) | NFE Forward 44(53.0) | NFE Backward 44(43.3) | CNF Time 0.5000(0.5000)
Iter 0439 | Time 0.5390(0.5344) | Loss 2.798487(2.832297) | NFE Forward 56(53.2) | NFE Backward 44(43.3) | CNF Time 0.5000(0.5000)
Iter 0440 | Time 0.6088(0.5396) | Loss 2.759176(2.827179) | NFE Forward 68(54.3) | NFE Backward 38(42.9) | CNF Time 0.5000(0.5000)
Iter 0441 | Time 0.6152(0.5449) | Loss 2.823076(2.826892) | NFE Forward 68(55.2) | NFE Backward 44(43.0) | CNF Time 0.5000(0.5000)
Iter 0442 | Time 0.4516(0.5384) | Loss 2.973777(2.837174) | NFE Forward 62(55.7) | NFE Backward 38(42.7) | CNF Time 0.5000(0.5000)
Iter 0443 | Time 0.5400(0.5385) | Loss 2.811300(2.835362) | NFE Forward 50(55.3) | NFE Backward 44(42.8) | CNF Time 0.5000(0.5000)
Iter 0444 | Time 0.5726(0.5409) | Loss 2.900606(2.839929) | NFE Forward 56(55.4) | NFE Backward 50(43.3) | CNF Time 0.5000(0.5000)
Iter 0445 | Time 0.4414(0.5339) | Loss 2.732910(2.832438) | NFE Forward 56(55.4) | NFE Backward 38(42.9) | CNF Time 0.5000(0.5000)
Iter 0446 | Time 0.5731(0.5367) | Loss 2.790351(2.829492) | NFE Forward 56(55.4) | NFE Backward 50(43.4) | CNF Time 0.5000(0.5000)
Iter 0447 | Time 0.5422(0.5371) | Loss 2.620033(2.814830) | NFE Forward 62(55.9) | NFE Backward 38(43.0) | CNF Time 0.5000(0.5000)
Iter 0448 | Time 0.5982(0.5413) | Loss 2.777951(2.812248) | NFE Forward 68(56.8) | NFE Backward 44(43.1) | CNF Time 0.5000(0.5000)
Iter 0449 | Time 0.7016(0.5526) | Loss 2.802897(2.811594) | NFE Forward 56(56.7) | NFE Backward 44(43.2) | CNF Time 0.5000(0.5000)
Iter 0450 | Time 0.7051(0.5632) | Loss 2.686786(2.802857) | NFE Forward 62(57.1) | NFE Backward 44(43.2) | CNF Time 0.5000(0.5000)
Iter 0451 | Time 0.6267(0.5677) | Loss 2.822670(2.804244) | NFE Forward 68(57.8) | NFE Backward 44(43.3) | CNF Time 0.5000(0.5000)
Iter 0452 | Time 0.4741(0.5611) | Loss 2.736828(2.799525) | NFE Forward 68(58.5) | NFE Backward 44(43.3) | CNF Time 0.5000(0.5000)
Iter 0453 | Time 0.5399(0.5596) | Loss 2.779264(2.798107) | NFE Forward 68(59.2) | NFE Backward 50(43.8) | CNF Time 0.5000(0.5000)
Iter 0454 | Time 0.5363(0.5580) | Loss 2.521737(2.778761) | NFE Forward 62(59.4) | NFE Backward 44(43.8) | CNF Time 0.5000(0.5000)
/Users/bjzhang/gitrepos/tap-sgm/ffjord-master/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":

    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(data='mnist', dims='64,64,64', strides='1,1,1,1', num_blocks=2, conv=True, layer_type='concat', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, imagesize=None, alpha=1e-06, time_length=1.0, train_T=True, num_epochs=1000, batch_size=200, batch_size_schedule='', test_batch_size=200, lr=0.001, warmup_iters=1000, weight_decay=0.0, spectral_norm_niter=10, add_noise=True, batch_norm=False, residual=False, autoencode=False, rademacher=True, spectral_norm=False, multiscale=True, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments/cnf', val_freq=1, log_freq=10)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
/work/bjzhang_umass_edu/generative_tmula/ffjord-master/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)


def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":
    print(torch.cuda.is_available())
    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(data='mnist', dims='64,64,64', strides='1,1,1,1', num_blocks=2, conv=True, layer_type='concat', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, imagesize=None, alpha=1e-06, time_length=1.0, train_T=True, num_epochs=1000, batch_size=200, batch_size_schedule='', test_batch_size=200, lr=0.001, warmup_iters=1000, weight_decay=0.0, spectral_norm_niter=10, add_noise=True, batch_norm=False, residual=False, autoencode=False, rademacher=True, spectral_norm=False, multiscale=True, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments/cnf', val_freq=1, log_freq=10)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 10.1440(10.1440) | Bit/dim 30.1535(30.1535) | Steps 406(406.00) | Grad Norm 242.6498(242.6498) | Total Time 10.00(10.00)
Iter 0010 | Time 2.8308(8.2833) | Bit/dim 27.2754(29.8102) | Steps 400(406.72) | Grad Norm 221.3472(239.9490) | Total Time 10.00(10.00)
Iter 0020 | Time 2.8386(6.8823) | Bit/dim 20.8741(28.2136) | Steps 400(406.01) | Grad Norm 163.7130(226.2318) | Total Time 10.00(10.00)
Iter 0030 | Time 2.8407(5.8202) | Bit/dim 14.8642(25.3412) | Steps 400(404.43) | Grad Norm 91.5182(198.8208) | Total Time 10.00(10.00)
Iter 0040 | Time 2.8299(5.0366) | Bit/dim 11.9199(22.0602) | Steps 400(403.27) | Grad Norm 32.9287(161.2598) | Total Time 10.00(10.00)
Iter 0050 | Time 2.8419(4.4589) | Bit/dim 10.1065(19.1134) | Steps 400(402.41) | Grad Norm 18.5271(124.8234) | Total Time 10.00(10.00)
Iter 0060 | Time 2.8434(4.0346) | Bit/dim 8.7235(16.5281) | Steps 400(401.78) | Grad Norm 13.8157(96.1344) | Total Time 10.00(10.00)
/work/bjzhang_umass_edu/generative_tmula/ffjord-master/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)

print(torch.cuda.is_available())



def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":
    print(torch.cuda.is_available())
    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(data='mnist', dims='64,64,64', strides='1,1,1,1', num_blocks=2, conv=True, layer_type='concat', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, imagesize=None, alpha=1e-06, time_length=1.0, train_T=True, num_epochs=1000, batch_size=200, batch_size_schedule='', test_batch_size=200, lr=0.001, warmup_iters=1000, weight_decay=0.0, spectral_norm_niter=10, add_noise=True, batch_norm=False, residual=False, autoencode=False, rademacher=True, spectral_norm=False, multiscale=True, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments/cnf', val_freq=1, log_freq=10)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 5.4338(5.4338) | Bit/dim 27.5431(27.5431) | Steps 406(406.00) | Grad Norm 217.1853(217.1853) | Total Time 10.00(10.00)
Iter 0010 | Time 2.8288(4.7695) | Bit/dim 25.2660(27.3015) | Steps 400(405.05) | Grad Norm 198.0794(215.3303) | Total Time 10.00(10.00)
Iter 0020 | Time 2.8428(4.2802) | Bit/dim 20.1004(26.0178) | Steps 400(404.33) | Grad Norm 150.3652(204.1677) | Total Time 10.00(10.00)
Iter 0030 | Time 2.8479(3.9021) | Bit/dim 15.0521(23.6800) | Steps 400(403.19) | Grad Norm 93.4240(181.3395) | Total Time 10.00(10.00)
/work/bjzhang_umass_edu/generative_tmula/ffjord-master/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)




def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":
    print(torch.cuda.is_available())
    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(data='mnist', dims='64,64,64', strides='1,1,1,1', num_blocks=2, conv=True, layer_type='concat', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, imagesize=None, alpha=1e-06, time_length=1.0, train_T=True, num_epochs=1000, batch_size=200, batch_size_schedule='', test_batch_size=200, lr=0.001, warmup_iters=1000, weight_decay=0.0, spectral_norm_niter=10, add_noise=True, batch_norm=False, residual=False, autoencode=False, rademacher=True, spectral_norm=False, multiscale=True, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments/cnf', val_freq=1, log_freq=10)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 5.5918(5.5918) | Bit/dim 26.3237(26.3237) | Steps 412(412.00) | Grad Norm 210.2658(210.2658) | Total Time 10.00(10.00)
Iter 0010 | Time 2.8356(4.8776) | Bit/dim 24.0213(26.0675) | Steps 400(409.26) | Grad Norm 192.2722(208.6441) | Total Time 10.00(10.00)
Iter 0020 | Time 2.8337(4.3507) | Bit/dim 18.7870(24.7523) | Steps 400(407.14) | Grad Norm 140.9946(197.0253) | Total Time 10.00(10.00)
Iter 0030 | Time 2.8373(3.9540) | Bit/dim 14.2482(22.4228) | Steps 400(405.26) | Grad Norm 75.9145(173.0385) | Total Time 10.00(10.00)
Iter 0040 | Time 2.8423(3.6614) | Bit/dim 11.5004(19.7844) | Steps 400(403.88) | Grad Norm 32.2529(140.5898) | Total Time 10.00(10.00)
Iter 0050 | Time 2.8402(3.4452) | Bit/dim 9.4141(17.3281) | Steps 400(402.86) | Grad Norm 17.4495(109.5292) | Total Time 10.00(10.00)
Iter 0060 | Time 2.8304(3.2867) | Bit/dim 8.3879(15.0819) | Steps 400(402.11) | Grad Norm 14.8458(84.7162) | Total Time 10.00(10.00)
Iter 0070 | Time 2.8373(3.1683) | Bit/dim 7.0016(13.0860) | Steps 400(401.56) | Grad Norm 15.7183(66.6109) | Total Time 10.00(10.00)
Iter 0080 | Time 2.8439(3.0830) | Bit/dim 5.6744(11.2852) | Steps 400(401.15) | Grad Norm 12.7096(52.8436) | Total Time 10.00(10.00)
Iter 0090 | Time 2.8417(3.0202) | Bit/dim 4.5466(9.6562) | Steps 400(400.85) | Grad Norm 10.7311(41.9993) | Total Time 10.00(10.00)
Iter 0100 | Time 2.8426(2.9736) | Bit/dim 3.6749(8.1923) | Steps 400(400.62) | Grad Norm 8.9554(33.5444) | Total Time 10.00(10.00)
Iter 0110 | Time 3.0510(2.9607) | Bit/dim 3.1301(6.9231) | Steps 412(401.99) | Grad Norm 6.6933(26.7710) | Total Time 10.00(10.00)
Iter 0120 | Time 3.2471(3.0023) | Bit/dim 2.7002(5.8546) | Steps 424(405.99) | Grad Norm 4.4111(21.1277) | Total Time 10.00(10.00)
Iter 0130 | Time 3.2462(3.0665) | Bit/dim 2.5087(4.9924) | Steps 424(410.72) | Grad Norm 2.7780(16.4807) | Total Time 10.00(10.00)
Iter 0140 | Time 3.3352(3.1337) | Bit/dim 2.3678(4.3177) | Steps 436(416.80) | Grad Norm 1.6763(12.6899) | Total Time 10.00(10.00)
Iter 0150 | Time 3.4946(3.2251) | Bit/dim 2.2981(3.7963) | Steps 442(423.28) | Grad Norm 1.2433(9.7366) | Total Time 10.00(10.00)
Iter 0160 | Time 3.4953(3.2951) | Bit/dim 2.2474(3.4001) | Steps 442(428.20) | Grad Norm 1.0346(7.4763) | Total Time 10.00(10.00)
Iter 0170 | Time 3.5032(3.3470) | Bit/dim 2.2508(3.0945) | Steps 442(431.82) | Grad Norm 0.8362(5.7581) | Total Time 10.00(10.00)
Iter 0180 | Time 3.4971(3.3863) | Bit/dim 2.1977(2.8674) | Steps 442(434.49) | Grad Norm 0.6792(4.4369) | Total Time 10.00(10.00)
Iter 0190 | Time 3.4922(3.4153) | Bit/dim 2.2218(2.6940) | Steps 442(436.46) | Grad Norm 0.6190(3.4365) | Total Time 10.00(10.00)
Iter 0200 | Time 3.4830(3.4363) | Bit/dim 2.1919(2.5609) | Steps 442(437.92) | Grad Norm 0.5507(2.6909) | Total Time 10.00(10.00)
Iter 0210 | Time 3.4977(3.4515) | Bit/dim 2.1879(2.4585) | Steps 442(438.99) | Grad Norm 0.5406(2.1355) | Total Time 10.00(10.00)
Iter 0220 | Time 3.5044(3.4644) | Bit/dim 2.1687(2.3794) | Steps 442(439.78) | Grad Norm 0.5876(1.7174) | Total Time 10.00(10.00)
Iter 0230 | Time 3.5008(3.4447) | Bit/dim 2.1366(2.3238) | Steps 442(439.27) | Grad Norm 0.4990(1.4096) | Total Time 10.00(10.00)
Iter 0240 | Time 3.3310(3.4215) | Bit/dim 2.1490(2.2772) | Steps 436(438.57) | Grad Norm 0.6330(1.1980) | Total Time 10.00(10.00)
Iter 0250 | Time 3.3380(3.3996) | Bit/dim 2.1375(2.2393) | Steps 436(437.89) | Grad Norm 0.5710(1.0300) | Total Time 10.00(10.00)
Iter 0260 | Time 3.3340(3.3834) | Bit/dim 2.1016(2.2074) | Steps 436(437.40) | Grad Norm 0.4371(0.8887) | Total Time 10.00(10.00)
Iter 0270 | Time 3.3426(3.3716) | Bit/dim 2.0946(2.1775) | Steps 436(437.03) | Grad Norm 0.4500(0.7763) | Total Time 10.00(10.00)
Iter 0280 | Time 3.3391(3.3627) | Bit/dim 2.1229(2.1578) | Steps 436(436.76) | Grad Norm 0.3942(0.7046) | Total Time 10.00(10.00)
Iter 0290 | Time 3.1632(3.3286) | Bit/dim 2.0715(2.1369) | Steps 430(435.58) | Grad Norm 0.4343(0.6376) | Total Time 10.00(10.00)
validating...
/work/bjzhang_umass_edu/generative_tmula/ffjord-master/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)




def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":
    print(torch.cuda.is_available())
    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss)

                loss = np.mean(losses.deatch().cpu().clone().numpy())
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(data='mnist', dims='64,64,64', strides='1,1,1,1', num_blocks=2, conv=True, layer_type='concat', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, imagesize=None, alpha=1e-06, time_length=1.0, train_T=True, num_epochs=1000, batch_size=200, batch_size_schedule='', test_batch_size=200, lr=0.001, warmup_iters=1000, weight_decay=0.0, spectral_norm_niter=10, add_noise=True, batch_norm=False, residual=False, autoencode=False, rademacher=True, spectral_norm=False, multiscale=True, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments/cnf', val_freq=1, log_freq=10)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 5.4613(5.4613) | Bit/dim 22.5767(22.5767) | Steps 406(406.00) | Grad Norm 167.5661(167.5661) | Total Time 10.00(10.00)
Iter 0010 | Time 2.8273(4.7779) | Bit/dim 20.5697(22.3357) | Steps 400(404.70) | Grad Norm 153.9431(166.1081) | Total Time 10.00(10.00)
Iter 0020 | Time 2.8387(4.2689) | Bit/dim 16.4983(21.2848) | Steps 400(403.46) | Grad Norm 108.6905(156.0066) | Total Time 10.00(10.00)
Iter 0030 | Time 2.8393(3.8931) | Bit/dim 13.1972(19.4923) | Steps 400(402.55) | Grad Norm 54.7065(135.3938) | Total Time 10.00(10.00)
Iter 0040 | Time 2.8344(3.6161) | Bit/dim 11.4015(17.5142) | Steps 400(401.88) | Grad Norm 23.4423(109.0724) | Total Time 10.00(10.00)
Iter 0050 | Time 2.8380(3.4109) | Bit/dim 9.5382(15.5977) | Steps 400(401.39) | Grad Norm 14.3444(85.0409) | Total Time 10.00(10.00)
Iter 0060 | Time 2.8481(3.2614) | Bit/dim 8.1862(13.8242) | Steps 400(401.02) | Grad Norm 14.5453(66.3394) | Total Time 10.00(10.00)
Iter 0070 | Time 3.0024(3.1609) | Bit/dim 6.9067(12.1579) | Steps 406(401.11) | Grad Norm 14.3415(52.8244) | Total Time 10.00(10.00)
Iter 0080 | Time 2.8413(3.0964) | Bit/dim 5.6517(10.5570) | Steps 400(401.55) | Grad Norm 11.9211(42.3190) | Total Time 10.00(10.00)
Iter 0090 | Time 2.8478(3.0299) | Bit/dim 4.3670(9.0419) | Steps 400(401.14) | Grad Norm 10.1886(34.0360) | Total Time 10.00(10.00)
Iter 0100 | Time 3.0454(2.9965) | Bit/dim 3.4084(7.6447) | Steps 412(402.20) | Grad Norm 7.8342(27.4302) | Total Time 10.00(10.00)
Iter 0110 | Time 3.0472(3.0085) | Bit/dim 2.8423(6.4362) | Steps 412(404.77) | Grad Norm 4.8124(21.7973) | Total Time 10.00(10.00)
Iter 0120 | Time 3.1380(3.0288) | Bit/dim 2.5605(5.4487) | Steps 424(407.88) | Grad Norm 2.9787(17.0358) | Total Time 10.00(10.00)
Iter 0130 | Time 3.2944(3.0797) | Bit/dim 2.3926(4.6673) | Steps 430(412.96) | Grad Norm 1.9194(13.1651) | Total Time 10.00(10.00)
Iter 0140 | Time 3.2995(3.1368) | Bit/dim 2.3555(4.0623) | Steps 430(417.44) | Grad Norm 1.4154(10.1352) | Total Time 10.00(10.00)
Iter 0150 | Time 3.6111(3.2381) | Bit/dim 2.2255(3.5963) | Steps 442(423.04) | Grad Norm 1.0375(7.7863) | Total Time 10.00(10.00)
Iter 0160 | Time 3.6174(3.3371) | Bit/dim 2.2388(3.2446) | Steps 442(428.02) | Grad Norm 0.8523(5.9838) | Total Time 10.00(10.00)
Iter 0170 | Time 3.6106(3.4093) | Bit/dim 2.2292(2.9781) | Steps 442(431.69) | Grad Norm 0.6864(4.6048) | Total Time 10.00(10.00)
