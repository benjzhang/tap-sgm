/work/bjzhang_umass_edu/generative_tmula/ffjord-master/train_cnf.py
import argparse
import os
import time
import numpy as np

import torch
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
import lib.multiscale_parallel as multiscale_parallel

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log

# go fast boi!!
torch.backends.cudnn.benchmark = True
SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams']
parser = argparse.ArgumentParser("Continuous Normalizing Flow")
parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church'], type=str, default="mnist")
parser.add_argument("--dims", type=str, default="8,32,32,8")
parser.add_argument("--strides", type=str, default="2,2,1,-2,-2")
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')

parser.add_argument("--conv", type=eval, default=True, choices=[True, False])
parser.add_argument(
    "--layer_type", type=str, default="ignore",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
parser.add_argument(
    "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu", "swish"]
)
parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument("--imagesize", type=int, default=None)
parser.add_argument("--alpha", type=float, default=1e-6)
parser.add_argument('--time_length', type=float, default=1.0)
parser.add_argument('--train_T', type=eval, default=True)

parser.add_argument("--num_epochs", type=int, default=1000)
parser.add_argument("--batch_size", type=int, default=200)
parser.add_argument(
    "--batch_size_schedule", type=str, default="", help="Increases the batchsize at every given epoch, dash separated."
)
parser.add_argument("--test_batch_size", type=int, default=200)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--warmup_iters", type=float, default=1000)
parser.add_argument("--weight_decay", type=float, default=0.0)
parser.add_argument("--spectral_norm_niter", type=int, default=10)

parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
parser.add_argument("--batch_norm", type=eval, default=False, choices=[True, False])
parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])
parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])

# Regularizations
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument("--time_penalty", type=float, default=0, help="Regularization on the end_time.")
parser.add_argument(
    "--max_grad_norm", type=float, default=1e10,
    help="Max norm of graidents (default is just stupidly high to avoid any clipping)"
)

parser.add_argument("--begin_epoch", type=int, default=1)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument("--save", type=str, default="experiments/cnf")
parser.add_argument("--val_freq", type=int, default=1)
parser.add_argument("--log_freq", type=int, default=10)

args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)




def add_noise(x):
    """
    [0, 1] -> [0, 255] -> add noise -> [0, 1]
    """
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
        x = x * 255 + noise
        x = x / 256
    return x


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_train_loader(train_set, epoch):
    if args.batch_size_schedule != "":
        epochs = [0] + list(map(int, args.batch_size_schedule.split("-")))
        n_passed = sum(np.array(epochs) <= epoch)
        current_batch_size = int(args.batch_size * n_passed)
    else:
        current_batch_size = args.batch_size
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True
    )
    logger.info("===> Using batch size {}. Total {} iterations/epoch.".format(current_batch_size, len(train_loader)))
    return train_loader


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root="./data", train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root="./data", split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root="./data", split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root="./data", train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ]), download=True
        )
        test_set = dset.CIFAR10(root="./data", train=False, transform=trans(im_size), download=True)
    elif args.data == 'celeba':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.CelebA(
            train=True, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.CelebA(
            train=False, transform=tforms.Compose([
                tforms.ToPILImage(),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
                tforms.ToTensor(),
                add_noise,
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    if not args.conv:
        data_shape = (im_dim * im_size * im_size,)

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True
    )
    return train_set, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    # Don't use data parallelize if batch size is small.
    # if x.shape[0] < 200:
    #     model = model.module

    z, delta_logp = model(x, zero)  # run model forward

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)

    return bits_per_dim


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))

    if args.multiscale:
        model = odenvp.ODENVP(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            nonlinearity=args.nonlinearity,
            alpha=args.alpha,
            cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
        )
    elif args.parallel:
        model = multiscale_parallel.MultiscaleParallelCNF(
            (args.batch_size, *data_shape),
            n_blocks=args.num_blocks,
            intermediate_dims=hidden_dims,
            alpha=args.alpha,
            time_length=args.time_length,
        )
    else:
        if args.autoencode:

            def build_cnf():
                autoencoder_diffeq = layers.AutoencoderDiffEqNet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.AutoencoderODEfunc(
                    autoencoder_diffeq=autoencoder_diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf
        else:

            def build_cnf():
                diffeq = layers.ODEnet(
                    hidden_dims=hidden_dims,
                    input_shape=data_shape,
                    strides=strides,
                    conv=args.conv,
                    layer_type=args.layer_type,
                    nonlinearity=args.nonlinearity,
                )
                odefunc = layers.ODEfunc(
                    diffeq=diffeq,
                    divergence_fn=args.divergence_fn,
                    residual=args.residual,
                    rademacher=args.rademacher,
                )
                cnf = layers.CNF(
                    odefunc=odefunc,
                    T=args.time_length,
                    train_T=args.train_T,
                    regularization_fns=regularization_fns,
                    solver=args.solver,
                )
                return cnf

        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]
        chain = chain + [build_cnf() for _ in range(args.num_blocks)]
        if args.batch_norm:
            chain.append(layers.MovingBatchNorm2d(data_shape[0]))
        model = layers.SequentialFlow(chain)
    return model


if __name__ == "__main__":
    print(torch.cuda.is_available())
    # get deivce
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_set, test_loader, data_shape = get_dataset(args)

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns)

    if args.spectral_norm: add_spectral_norm(model, logger)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    # optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    if torch.cuda.is_available():
        model = torch.nn.DataParallel(model).cuda()

    # For visualization.
    fixed_z = cvt(torch.randn(100, *data_shape))

    time_meter = utils.RunningAverageMeter(0.97)
    loss_meter = utils.RunningAverageMeter(0.97)
    steps_meter = utils.RunningAverageMeter(0.97)
    grad_meter = utils.RunningAverageMeter(0.97)
    tt_meter = utils.RunningAverageMeter(0.97)

    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)

    best_loss = float("inf")
    itr = 0
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        model.train()
        train_loader = get_train_loader(train_set, epoch)
        for _, (x, y) in enumerate(train_loader):
            start = time.time()
            update_lr(optimizer, itr)
            optimizer.zero_grad()

            if not args.conv:
                x = x.view(x.shape[0], -1)

            # cast data and move to device
            x = cvt(x)
            # compute loss
            loss = compute_bits_per_dim(x, model)
            if regularization_coeffs:
                reg_states = get_regularization(model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                )
                loss = loss + reg_loss
            total_time = count_total_time(model)
            loss = loss + total_time * args.time_penalty

            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            optimizer.step()

            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)

            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})".format(
                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,
                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg
                    )
                )
                if regularization_coeffs:
                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)
                logger.info(log_message)

            itr += 1

        # compute test loss
        model.eval()
        if epoch % args.val_freq == 0:
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                for (x, y) in test_loader:
                    if not args.conv:
                        x = x.view(x.shape[0], -1)
                    x = cvt(x)
                    loss = compute_bits_per_dim(x, model)
                    losses.append(loss.detach().cpu().clone().numpy())

                loss = np.mean(losses)
                logger.info("Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(epoch, time.time() - start, loss))
                if loss < best_loss:
                    best_loss = loss
                    utils.makedirs(args.save)
                    torch.save({
                        "args": args,
                        "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                        "optim_state_dict": optimizer.state_dict(),
                    }, os.path.join(args.save, "checkpt.pth"))

        # visualize samples and density
        with torch.no_grad():
            fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            utils.makedirs(os.path.dirname(fig_filename))
            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)
            save_image(generated_samples, fig_filename, nrow=10)

Namespace(data='mnist', dims='64,64,64', strides='1,1,1,1', num_blocks=2, conv=True, layer_type='concat', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, imagesize=None, alpha=1e-06, time_length=1.0, train_T=True, num_epochs=1000, batch_size=200, batch_size_schedule='', test_batch_size=200, lr=0.001, warmup_iters=1000, weight_decay=0.0, spectral_norm_niter=10, add_noise=True, batch_norm=False, residual=False, autoencode=False, rademacher=True, spectral_norm=False, multiscale=True, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments/cnf/mnist_test', val_freq=1, log_freq=10)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (1): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): SqueezeLayer()
        (3): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
    (2): StackedCNFLayers(
      (chain): ModuleList(
        (0): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 800646
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0000 | Time 19.0820(19.0820) | Bit/dim 21.4746(21.4746) | Steps 400(400.00) | Grad Norm 166.5108(166.5108) | Total Time 10.00(10.00)
Iter 0010 | Time 3.0435(14.8578) | Bit/dim 19.5585(21.2677) | Steps 400(400.16) | Grad Norm 147.9231(164.3208) | Total Time 10.00(10.00)
Iter 0020 | Time 3.0460(11.7481) | Bit/dim 15.8086(20.2582) | Steps 400(400.12) | Grad Norm 100.1890(153.2888) | Total Time 10.00(10.00)
Iter 0030 | Time 2.9061(9.4387) | Bit/dim 12.6674(18.5517) | Steps 400(400.09) | Grad Norm 47.4247(131.3010) | Total Time 10.00(10.00)
Iter 0040 | Time 2.9209(7.7564) | Bit/dim 10.7620(16.6920) | Steps 400(400.07) | Grad Norm 22.4293(104.9169) | Total Time 10.00(10.00)
Iter 0050 | Time 2.9266(6.5088) | Bit/dim 9.1522(14.8550) | Steps 400(400.05) | Grad Norm 14.9346(81.9331) | Total Time 10.00(10.00)
Iter 0060 | Time 2.9452(5.5821) | Bit/dim 7.7854(13.1448) | Steps 400(400.04) | Grad Norm 15.9626(64.4752) | Total Time 10.00(10.00)
Iter 0070 | Time 3.1102(4.9037) | Bit/dim 6.5081(11.5431) | Steps 400(400.03) | Grad Norm 13.8130(51.5423) | Total Time 10.00(10.00)
Iter 0080 | Time 2.9802(4.4142) | Bit/dim 5.3256(10.0226) | Steps 406(401.21) | Grad Norm 11.3233(41.1879) | Total Time 10.00(10.00)
Iter 0090 | Time 3.1479(4.0547) | Bit/dim 4.1987(8.5992) | Steps 406(402.89) | Grad Norm 9.7801(33.1014) | Total Time 10.00(10.00)
Iter 0100 | Time 3.3436(3.8512) | Bit/dim 3.3881(7.3132) | Steps 418(406.58) | Grad Norm 7.6397(26.6943) | Total Time 10.00(10.00)
Iter 0110 | Time 3.1810(3.6961) | Bit/dim 2.8263(6.1889) | Steps 418(409.58) | Grad Norm 4.9406(21.2525) | Total Time 10.00(10.00)
Iter 0120 | Time 3.2277(3.5991) | Bit/dim 2.5390(5.2603) | Steps 424(412.79) | Grad Norm 3.1175(16.6807) | Total Time 10.00(10.00)
Iter 0130 | Time 3.3991(3.5543) | Bit/dim 2.4382(4.5281) | Steps 430(416.89) | Grad Norm 1.9107(12.9137) | Total Time 10.00(10.00)
Iter 0140 | Time 3.7579(3.5728) | Bit/dim 2.3171(3.9572) | Steps 442(422.74) | Grad Norm 1.2801(9.9205) | Total Time 10.00(10.00)
Iter 0150 | Time 3.7732(3.5970) | Bit/dim 2.2516(3.5193) | Steps 442(427.80) | Grad Norm 1.0603(7.6107) | Total Time 10.00(10.00)
Iter 0160 | Time 3.6167(3.6030) | Bit/dim 2.2234(3.1826) | Steps 442(431.53) | Grad Norm 0.9637(5.8627) | Total Time 10.00(10.00)
Iter 0170 | Time 3.6362(3.6146) | Bit/dim 2.1976(2.9290) | Steps 448(434.63) | Grad Norm 0.7521(4.5306) | Total Time 10.00(10.00)
Iter 0180 | Time 3.7787(3.6448) | Bit/dim 2.1927(2.7371) | Steps 442(437.49) | Grad Norm 0.6127(3.5145) | Total Time 10.00(10.00)
Iter 0190 | Time 3.7523(3.6626) | Bit/dim 2.1726(2.5900) | Steps 442(438.95) | Grad Norm 0.5628(2.7494) | Total Time 10.00(10.00)
Iter 0200 | Time 3.4529(3.6401) | Bit/dim 2.1218(2.4758) | Steps 436(438.76) | Grad Norm 0.6281(2.1759) | Total Time 10.00(10.00)
Iter 0210 | Time 3.5949(3.6066) | Bit/dim 2.0971(2.3818) | Steps 436(438.51) | Grad Norm 0.5542(1.7475) | Total Time 10.00(10.00)
Iter 0220 | Time 3.5969(3.6000) | Bit/dim 2.0974(2.3147) | Steps 436(438.13) | Grad Norm 0.7003(1.4426) | Total Time 10.00(10.00)
Iter 0230 | Time 3.4494(3.5890) | Bit/dim 2.1146(2.2615) | Steps 436(437.57) | Grad Norm 0.5939(1.2072) | Total Time 10.00(10.00)
Iter 0240 | Time 3.6528(3.5750) | Bit/dim 2.1082(2.2190) | Steps 442(437.51) | Grad Norm 0.4576(1.0185) | Total Time 10.00(10.00)
Iter 0250 | Time 3.6626(3.6163) | Bit/dim 2.0647(2.1827) | Steps 448(439.70) | Grad Norm 0.4931(0.8777) | Total Time 10.00(10.00)
Iter 0260 | Time 3.6894(3.6621) | Bit/dim 2.0606(2.1500) | Steps 454(442.86) | Grad Norm 0.5955(0.7806) | Total Time 10.00(10.00)
Iter 0270 | Time 3.6843(3.6908) | Bit/dim 2.0788(2.1236) | Steps 454(445.79) | Grad Norm 0.5115(0.7073) | Total Time 10.00(10.00)
Iter 0280 | Time 3.6991(3.7090) | Bit/dim 2.0351(2.1026) | Steps 454(447.79) | Grad Norm 0.5730(0.6496) | Total Time 10.00(10.00)
Iter 0290 | Time 3.5602(3.7128) | Bit/dim 2.0508(2.0841) | Steps 448(449.07) | Grad Norm 0.5595(0.6151) | Total Time 10.00(10.00)
validating...
Epoch 0001 | Time 53.7036, Bit/dim 1.9899
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0300 | Time 3.6909(3.7075) | Bit/dim 2.0164(2.0635) | Steps 454(449.76) | Grad Norm 0.6318(0.6294) | Total Time 10.00(10.00)
Iter 0310 | Time 3.6961(3.7216) | Bit/dim 2.0061(2.0451) | Steps 448(450.39) | Grad Norm 0.4873(0.6263) | Total Time 10.00(10.00)
Iter 0320 | Time 3.6599(3.7391) | Bit/dim 1.9885(2.0265) | Steps 448(450.49) | Grad Norm 0.4585(0.5903) | Total Time 10.00(10.00)
Iter 0330 | Time 3.8170(3.7322) | Bit/dim 1.9541(2.0092) | Steps 448(449.84) | Grad Norm 0.5156(0.5648) | Total Time 10.00(10.00)
Iter 0340 | Time 3.6454(3.7273) | Bit/dim 1.9126(1.9903) | Steps 448(449.35) | Grad Norm 0.3431(0.5422) | Total Time 10.00(10.00)
Iter 0350 | Time 3.8445(3.7478) | Bit/dim 1.9142(1.9711) | Steps 454(449.85) | Grad Norm 0.5871(0.5474) | Total Time 10.00(10.00)
Iter 0360 | Time 3.6909(3.7499) | Bit/dim 1.9305(1.9558) | Steps 454(450.94) | Grad Norm 0.3110(0.5633) | Total Time 10.00(10.00)
Iter 0370 | Time 3.6859(3.7445) | Bit/dim 1.9191(1.9427) | Steps 454(451.74) | Grad Norm 0.3298(0.5124) | Total Time 10.00(10.00)
Iter 0380 | Time 4.0088(3.7818) | Bit/dim 1.8782(1.9321) | Steps 460(453.49) | Grad Norm 0.3186(0.4753) | Total Time 10.00(10.00)
Iter 0390 | Time 4.2351(3.8417) | Bit/dim 1.8869(1.9186) | Steps 472(456.41) | Grad Norm 0.5608(0.4830) | Total Time 10.00(10.00)
Iter 0400 | Time 4.2202(3.9655) | Bit/dim 1.8910(1.9063) | Steps 478(461.79) | Grad Norm 0.7986(0.5237) | Total Time 10.00(10.00)
Iter 0410 | Time 4.2149(4.0461) | Bit/dim 1.8633(1.9004) | Steps 478(466.05) | Grad Norm 0.5397(0.5301) | Total Time 10.00(10.00)
Iter 0420 | Time 4.4034(4.1256) | Bit/dim 1.8454(1.8900) | Steps 478(469.19) | Grad Norm 0.7610(0.5377) | Total Time 10.00(10.00)
Iter 0430 | Time 4.2167(4.1747) | Bit/dim 1.8515(1.8836) | Steps 478(471.50) | Grad Norm 0.4998(0.5268) | Total Time 10.00(10.00)
Iter 0440 | Time 4.3989(4.2264) | Bit/dim 1.8759(1.8783) | Steps 478(473.21) | Grad Norm 0.5241(0.5247) | Total Time 10.00(10.00)
Iter 0450 | Time 4.3843(4.2667) | Bit/dim 1.8224(1.8713) | Steps 478(474.47) | Grad Norm 0.6755(0.5300) | Total Time 10.00(10.00)
Iter 0460 | Time 4.4493(4.3025) | Bit/dim 1.8307(1.8646) | Steps 484(476.24) | Grad Norm 0.8342(0.5490) | Total Time 10.00(10.00)
Iter 0470 | Time 4.2637(4.3119) | Bit/dim 1.8775(1.8594) | Steps 484(478.27) | Grad Norm 0.7127(0.5548) | Total Time 10.00(10.00)
Iter 0480 | Time 4.2696(4.3268) | Bit/dim 1.7875(1.8501) | Steps 484(479.78) | Grad Norm 0.3392(0.5566) | Total Time 10.00(10.00)
Iter 0490 | Time 4.4472(4.3394) | Bit/dim 1.8197(1.8401) | Steps 484(480.89) | Grad Norm 1.0785(0.5850) | Total Time 10.00(10.00)
Iter 0500 | Time 4.4329(4.3809) | Bit/dim 1.7796(1.8322) | Steps 490(483.48) | Grad Norm 0.7644(0.6613) | Total Time 10.00(10.00)
Iter 0510 | Time 4.7185(4.4509) | Bit/dim 1.7967(1.8241) | Steps 502(488.07) | Grad Norm 0.4852(0.6674) | Total Time 10.00(10.00)
Iter 0520 | Time 4.5204(4.4998) | Bit/dim 1.8284(1.8178) | Steps 502(491.73) | Grad Norm 0.8145(0.7104) | Total Time 10.00(10.00)
Iter 0530 | Time 4.7138(4.5402) | Bit/dim 1.7622(1.8118) | Steps 502(494.43) | Grad Norm 1.7192(0.8582) | Total Time 10.00(10.00)
Iter 0540 | Time 4.7075(4.5587) | Bit/dim 1.7399(1.8002) | Steps 502(496.25) | Grad Norm 0.6953(0.8769) | Total Time 10.00(10.00)
Iter 0550 | Time 4.5229(4.5830) | Bit/dim 1.8226(1.7945) | Steps 502(497.76) | Grad Norm 0.6139(0.8905) | Total Time 10.00(10.00)
Iter 0560 | Time 4.3666(4.5827) | Bit/dim 1.7684(1.7850) | Steps 496(498.19) | Grad Norm 0.5497(0.9640) | Total Time 10.00(10.00)
Iter 0570 | Time 4.5564(4.5597) | Bit/dim 1.7388(1.7765) | Steps 496(497.45) | Grad Norm 3.4337(1.1262) | Total Time 10.00(10.00)
Iter 0580 | Time 4.3557(4.5220) | Bit/dim 1.7712(1.7684) | Steps 496(496.93) | Grad Norm 5.9561(2.2179) | Total Time 10.00(10.00)
Iter 0590 | Time 4.3562(4.5052) | Bit/dim 1.7197(1.7572) | Steps 496(496.37) | Grad Norm 3.4747(2.4477) | Total Time 10.00(10.00)
validating...
Epoch 0002 | Time 61.4787, Bit/dim 1.6766
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0600 | Time 4.5329(4.5007) | Bit/dim 1.7131(1.7417) | Steps 496(496.27) | Grad Norm 3.3425(2.5641) | Total Time 10.00(10.00)
Iter 0610 | Time 4.5555(4.4928) | Bit/dim 1.6145(1.7200) | Steps 496(496.03) | Grad Norm 0.8743(3.2659) | Total Time 10.00(10.00)
Iter 0620 | Time 4.3070(4.4739) | Bit/dim 1.5744(1.6931) | Steps 490(494.45) | Grad Norm 1.6921(3.6218) | Total Time 10.00(10.00)
Iter 0630 | Time 4.5882(4.4622) | Bit/dim 1.6991(1.6741) | Steps 502(493.79) | Grad Norm 21.3380(6.0791) | Total Time 10.00(10.00)
Iter 0640 | Time 4.5929(4.4523) | Bit/dim 1.5494(1.6533) | Steps 502(493.65) | Grad Norm 4.8476(8.0900) | Total Time 10.00(10.00)
Iter 0650 | Time 4.4030(4.4642) | Bit/dim 1.5311(1.6263) | Steps 502(495.40) | Grad Norm 2.8541(7.6503) | Total Time 10.00(10.00)
Iter 0660 | Time 4.2453(4.4651) | Bit/dim 1.4858(1.5951) | Steps 496(496.32) | Grad Norm 2.3313(6.6170) | Total Time 10.00(10.00)
Iter 0670 | Time 4.2450(4.4414) | Bit/dim 1.4738(1.5692) | Steps 496(496.38) | Grad Norm 2.5237(5.8709) | Total Time 10.00(10.00)
Iter 0680 | Time 4.2570(4.4404) | Bit/dim 1.5190(1.5556) | Steps 472(495.79) | Grad Norm 15.3230(8.1339) | Total Time 10.00(10.00)
Iter 0690 | Time 4.4763(4.4501) | Bit/dim 1.4281(1.5299) | Steps 502(498.08) | Grad Norm 1.8341(8.0539) | Total Time 10.00(10.00)
Iter 0700 | Time 4.4730(4.4480) | Bit/dim 1.4460(1.5114) | Steps 502(499.84) | Grad Norm 9.4010(7.7142) | Total Time 10.00(10.00)
Iter 0710 | Time 4.4951(4.4760) | Bit/dim 1.4823(1.5011) | Steps 514(502.13) | Grad Norm 23.7600(9.5164) | Total Time 10.00(10.00)
Iter 0720 | Time 4.4479(4.4911) | Bit/dim 1.4812(1.4880) | Steps 508(503.14) | Grad Norm 11.4649(10.2150) | Total Time 10.00(10.00)
Iter 0730 | Time 4.2944(4.4923) | Bit/dim 1.4434(1.4738) | Steps 502(503.93) | Grad Norm 2.5077(9.5296) | Total Time 10.00(10.00)
Iter 0740 | Time 4.5506(4.5088) | Bit/dim 1.5432(1.4642) | Steps 496(504.41) | Grad Norm 31.3760(10.2043) | Total Time 10.00(10.00)
Iter 0750 | Time 4.4492(4.5356) | Bit/dim 1.4188(1.4591) | Steps 508(506.52) | Grad Norm 1.8686(11.0675) | Total Time 10.00(10.00)
Iter 0760 | Time 4.4795(4.5417) | Bit/dim 1.4271(1.4524) | Steps 514(507.07) | Grad Norm 14.5428(10.4204) | Total Time 10.00(10.00)
Iter 0770 | Time 4.4775(4.5325) | Bit/dim 1.4264(1.4437) | Steps 502(507.22) | Grad Norm 16.0253(10.7681) | Total Time 10.00(10.00)
Iter 0780 | Time 4.4985(4.5457) | Bit/dim 1.4245(1.4387) | Steps 514(507.79) | Grad Norm 11.7348(11.1902) | Total Time 10.00(10.00)
Iter 0790 | Time 4.7058(4.5536) | Bit/dim 1.4394(1.4328) | Steps 526(508.84) | Grad Norm 22.3097(11.7310) | Total Time 10.00(10.00)
Iter 0800 | Time 4.6961(4.5560) | Bit/dim 1.4454(1.4257) | Steps 526(509.29) | Grad Norm 15.9473(11.8964) | Total Time 10.00(10.00)
Iter 0810 | Time 4.2907(4.5487) | Bit/dim 1.3878(1.4199) | Steps 502(508.34) | Grad Norm 13.8934(11.1077) | Total Time 10.00(10.00)
Iter 0820 | Time 4.8439(4.5626) | Bit/dim 1.3699(1.4113) | Steps 520(508.54) | Grad Norm 16.9795(10.8198) | Total Time 10.00(10.00)
Iter 0830 | Time 4.8480(4.5749) | Bit/dim 1.4286(1.4134) | Steps 520(509.50) | Grad Norm 16.9565(12.3665) | Total Time 10.00(10.00)
Iter 0840 | Time 4.8428(4.5629) | Bit/dim 1.4191(1.4088) | Steps 520(509.45) | Grad Norm 13.0352(12.3268) | Total Time 10.00(10.00)
Iter 0850 | Time 4.2857(4.5555) | Bit/dim 1.3755(1.4058) | Steps 502(509.07) | Grad Norm 1.5430(11.5973) | Total Time 10.00(10.00)
Iter 0860 | Time 4.4632(4.5608) | Bit/dim 1.3661(1.3978) | Steps 502(509.27) | Grad Norm 17.7197(11.6833) | Total Time 10.00(10.00)
Iter 0870 | Time 4.4742(4.5852) | Bit/dim 1.4016(1.3975) | Steps 502(510.12) | Grad Norm 16.7798(12.7213) | Total Time 10.00(10.00)
Iter 0880 | Time 4.6368(4.5728) | Bit/dim 1.3776(1.3906) | Steps 508(509.87) | Grad Norm 1.5452(11.9930) | Total Time 10.00(10.00)
Iter 0890 | Time 4.9077(4.5879) | Bit/dim 1.3857(1.3862) | Steps 526(510.50) | Grad Norm 22.0984(12.3805) | Total Time 10.00(10.00)
validating...
Epoch 0003 | Time 69.4876, Bit/dim 1.3773
===> Using batch size 200. Total 300 iterations/epoch.
Iter 0900 | Time 4.6481(4.5851) | Bit/dim 1.3891(1.3843) | Steps 520(511.22) | Grad Norm 18.2241(12.4617) | Total Time 10.00(10.00)
Iter 0910 | Time 4.6878(4.5859) | Bit/dim 1.3763(1.3832) | Steps 520(510.89) | Grad Norm 17.3080(13.0130) | Total Time 10.00(10.00)
Iter 0920 | Time 4.8993(4.5771) | Bit/dim 1.4042(1.3799) | Steps 520(510.46) | Grad Norm 16.7827(12.7417) | Total Time 10.00(10.00)
Iter 0930 | Time 4.6718(4.5694) | Bit/dim 1.3502(1.3788) | Steps 508(509.97) | Grad Norm 3.4650(12.4456) | Total Time 10.00(10.00)
Iter 0940 | Time 4.3256(4.5530) | Bit/dim 1.3420(1.3751) | Steps 502(509.26) | Grad Norm 18.5886(12.0262) | Total Time 10.00(10.00)
Iter 0950 | Time 4.5234(4.5791) | Bit/dim 1.5012(1.3794) | Steps 502(511.87) | Grad Norm 31.3612(13.9390) | Total Time 10.00(10.00)
Iter 0960 | Time 4.9248(4.6171) | Bit/dim 1.5118(1.4051) | Steps 526(514.06) | Grad Norm 32.3873(16.1918) | Total Time 10.00(10.00)
Iter 0970 | Time 4.5121(4.5978) | Bit/dim 1.4062(1.4090) | Steps 502(513.10) | Grad Norm 14.2253(15.6392) | Total Time 10.00(10.00)
Iter 0980 | Time 4.3116(4.5654) | Bit/dim 1.4048(1.3984) | Steps 502(510.80) | Grad Norm 4.3721(13.1279) | Total Time 10.00(10.00)
Iter 0990 | Time 4.4725(4.5434) | Bit/dim 1.3528(1.3868) | Steps 508(509.14) | Grad Norm 4.6300(10.6983) | Total Time 10.00(10.00)
Iter 1000 | Time 4.5123(4.5337) | Bit/dim 1.3306(1.3752) | Steps 502(507.74) | Grad Norm 5.4189(8.8908) | Total Time 10.00(10.00)
Iter 1010 | Time 4.4921(4.5261) | Bit/dim 1.3470(1.3627) | Steps 508(507.34) | Grad Norm 3.0173(7.7673) | Total Time 10.00(10.00)
Iter 1020 | Time 4.4799(4.5283) | Bit/dim 1.3099(1.3536) | Steps 508(507.06) | Grad Norm 4.8368(7.4018) | Total Time 10.00(10.00)
Iter 1030 | Time 4.3207(4.5357) | Bit/dim 1.3531(1.3469) | Steps 502(508.94) | Grad Norm 18.5316(8.7135) | Total Time 10.00(10.00)
Iter 1040 | Time 4.9190(4.5523) | Bit/dim 1.3308(1.3415) | Steps 538(510.65) | Grad Norm 17.0862(9.2874) | Total Time 10.00(10.00)
Iter 1050 | Time 4.5065(4.5651) | Bit/dim 1.3963(1.3387) | Steps 502(511.98) | Grad Norm 26.4622(10.6289) | Total Time 10.00(10.00)
Iter 1060 | Time 4.5157(4.5892) | Bit/dim 1.4849(1.3631) | Steps 502(514.05) | Grad Norm 26.1789(14.2141) | Total Time 10.00(10.00)
Iter 1070 | Time 4.3235(4.5838) | Bit/dim 1.3424(1.3774) | Steps 502(514.35) | Grad Norm 6.9176(14.4453) | Total Time 10.00(10.00)
Iter 1080 | Time 4.3119(4.5707) | Bit/dim 1.3414(1.3718) | Steps 508(514.19) | Grad Norm 6.3629(12.8608) | Total Time 10.00(10.00)
Iter 1090 | Time 4.5026(4.5521) | Bit/dim 1.2945(1.3603) | Steps 514(513.51) | Grad Norm 2.0824(10.8175) | Total Time 10.00(10.00)
Iter 1100 | Time 4.7070(4.5578) | Bit/dim 1.3399(1.3503) | Steps 514(513.71) | Grad Norm 4.7288(9.0194) | Total Time 10.00(10.00)
Iter 1110 | Time 4.5698(4.5618) | Bit/dim 1.3308(1.3376) | Steps 520(513.96) | Grad Norm 6.7180(8.2533) | Total Time 10.00(10.00)
Iter 1120 | Time 4.6901(4.5789) | Bit/dim 1.3040(1.3281) | Steps 508(513.66) | Grad Norm 3.5654(7.5743) | Total Time 10.00(10.00)
Iter 1130 | Time 4.7617(4.5851) | Bit/dim 1.3118(1.3226) | Steps 520(513.75) | Grad Norm 4.5811(6.8193) | Total Time 10.00(10.00)
Iter 1140 | Time 4.6496(4.6131) | Bit/dim 1.2788(1.3167) | Steps 532(514.38) | Grad Norm 14.6848(7.6808) | Total Time 10.00(10.00)
Iter 1150 | Time 4.5475(4.6260) | Bit/dim 1.3190(1.3126) | Steps 520(514.90) | Grad Norm 15.4365(8.2373) | Total Time 10.00(10.00)
Iter 1160 | Time 4.5088(4.6330) | Bit/dim 1.4001(1.3242) | Steps 502(515.99) | Grad Norm 18.6180(11.5466) | Total Time 10.00(10.00)
Iter 1170 | Time 5.0364(4.6682) | Bit/dim 1.4025(1.3561) | Steps 538(517.19) | Grad Norm 9.7806(13.7778) | Total Time 10.00(10.00)
Iter 1180 | Time 4.5508(4.6597) | Bit/dim 1.3678(1.3583) | Steps 508(517.58) | Grad Norm 12.8886(14.0164) | Total Time 10.00(10.00)
Iter 1190 | Time 4.8038(4.6495) | Bit/dim 1.2916(1.3446) | Steps 526(517.96) | Grad Norm 9.1148(12.5793) | Total Time 10.00(10.00)
validating...
Epoch 0004 | Time 71.1580, Bit/dim 1.2976
===> Using batch size 200. Total 300 iterations/epoch.
Iter 1200 | Time 4.8015(4.6379) | Bit/dim 1.2987(1.3344) | Steps 526(517.77) | Grad Norm 6.8280(10.8064) | Total Time 10.00(10.00)
Iter 1210 | Time 4.5799(4.6363) | Bit/dim 1.2855(1.3223) | Steps 526(519.20) | Grad Norm 5.4980(9.0379) | Total Time 10.00(10.00)
Iter 1220 | Time 4.5768(4.6372) | Bit/dim 1.2931(1.3118) | Steps 520(518.79) | Grad Norm 2.7968(7.5449) | Total Time 10.00(10.00)
Iter 1230 | Time 4.5498(4.6441) | Bit/dim 1.2606(1.3041) | Steps 520(519.11) | Grad Norm 5.4154(6.4243) | Total Time 10.00(10.00)
Iter 1240 | Time 4.6840(4.6479) | Bit/dim 1.2466(1.2972) | Steps 520(519.34) | Grad Norm 6.8657(5.8809) | Total Time 10.00(10.00)
Iter 1250 | Time 4.4496(4.6530) | Bit/dim 1.3107(1.2930) | Steps 508(518.99) | Grad Norm 11.3490(6.5991) | Total Time 10.00(10.00)
Iter 1260 | Time 4.7706(4.6627) | Bit/dim 1.2775(1.2906) | Steps 520(518.94) | Grad Norm 1.7305(7.0414) | Total Time 10.00(10.00)
Iter 1270 | Time 4.5679(4.6623) | Bit/dim 1.2662(1.2905) | Steps 520(518.89) | Grad Norm 2.9995(7.8251) | Total Time 10.00(10.00)
Iter 1280 | Time 4.4249(4.6568) | Bit/dim 1.4886(1.3077) | Steps 502(517.67) | Grad Norm 17.6664(11.0287) | Total Time 10.00(10.00)
Iter 1290 | Time 5.0513(4.6911) | Bit/dim 1.4825(1.3438) | Steps 538(520.26) | Grad Norm 10.4934(12.9270) | Total Time 10.00(10.00)
Iter 1300 | Time 5.2636(4.7293) | Bit/dim 1.3584(1.3544) | Steps 562(524.35) | Grad Norm 13.6659(13.7764) | Total Time 10.00(10.00)
Iter 1310 | Time 4.4071(4.7348) | Bit/dim 1.2969(1.3443) | Steps 526(528.09) | Grad Norm 10.3573(13.1280) | Total Time 10.00(10.00)
Iter 1320 | Time 4.4886(4.7155) | Bit/dim 1.3082(1.3333) | Steps 514(528.54) | Grad Norm 3.3812(11.7059) | Total Time 10.00(10.00)
Iter 1330 | Time 4.5744(4.7095) | Bit/dim 1.2636(1.3217) | Steps 520(527.05) | Grad Norm 3.6575(9.8601) | Total Time 10.00(10.00)
Iter 1340 | Time 4.7750(4.6930) | Bit/dim 1.2860(1.3095) | Steps 520(525.20) | Grad Norm 1.3495(7.8438) | Total Time 10.00(10.00)
Iter 1350 | Time 4.5706(4.6761) | Bit/dim 1.2558(1.2985) | Steps 520(523.83) | Grad Norm 1.2680(6.1933) | Total Time 10.00(10.00)
Iter 1360 | Time 4.5447(4.6639) | Bit/dim 1.2694(1.2885) | Steps 520(522.83) | Grad Norm 1.6749(4.9283) | Total Time 10.00(10.00)
Iter 1370 | Time 4.7758(4.6549) | Bit/dim 1.2764(1.2844) | Steps 520(522.09) | Grad Norm 3.2486(4.2097) | Total Time 10.00(10.00)
Iter 1380 | Time 4.7887(4.6606) | Bit/dim 1.2496(1.2774) | Steps 520(521.54) | Grad Norm 1.6946(3.9016) | Total Time 10.00(10.00)
Iter 1390 | Time 4.5792(4.6672) | Bit/dim 1.2728(1.2715) | Steps 520(521.44) | Grad Norm 2.9202(4.7149) | Total Time 10.00(10.00)
Iter 1400 | Time 4.7409(4.6782) | Bit/dim 1.2964(1.2726) | Steps 526(521.69) | Grad Norm 19.5114(6.9320) | Total Time 10.00(10.00)
Iter 1410 | Time 5.1324(4.6728) | Bit/dim 1.4591(1.2941) | Steps 562(521.53) | Grad Norm 33.5498(10.2388) | Total Time 10.00(10.00)
Iter 1420 | Time 4.5130(4.6733) | Bit/dim 1.3859(1.3094) | Steps 502(521.44) | Grad Norm 19.6965(12.2942) | Total Time 10.00(10.00)
Iter 1430 | Time 4.2593(4.6967) | Bit/dim 1.3516(1.3265) | Steps 484(521.31) | Grad Norm 18.1911(13.4896) | Total Time 10.00(10.00)
Iter 1440 | Time 4.9406(4.7117) | Bit/dim 1.3027(1.3219) | Steps 526(521.71) | Grad Norm 15.6434(13.1076) | Total Time 10.00(10.00)
Iter 1450 | Time 4.7775(4.6992) | Bit/dim 1.2650(1.3115) | Steps 520(520.82) | Grad Norm 4.7457(12.0959) | Total Time 10.00(10.00)
Iter 1460 | Time 4.7834(4.6909) | Bit/dim 1.2333(1.2969) | Steps 520(520.61) | Grad Norm 6.7777(10.5900) | Total Time 10.00(10.00)
Iter 1470 | Time 4.5573(4.6742) | Bit/dim 1.2355(1.2848) | Steps 520(520.45) | Grad Norm 3.3954(8.8030) | Total Time 10.00(10.00)
Iter 1480 | Time 4.7658(4.6758) | Bit/dim 1.2402(1.2749) | Steps 520(520.33) | Grad Norm 1.7919(7.1493) | Total Time 10.00(10.00)
Iter 1490 | Time 4.5460(4.6601) | Bit/dim 1.2288(1.2642) | Steps 520(520.24) | Grad Norm 0.7818(5.6422) | Total Time 10.00(10.00)
validating...
Epoch 0005 | Time 76.1469, Bit/dim 1.2434
===> Using batch size 200. Total 300 iterations/epoch.
Iter 1500 | Time 4.7718(4.6572) | Bit/dim 1.2414(1.2605) | Steps 520(520.18) | Grad Norm 2.6897(4.7678) | Total Time 10.00(10.00)
Iter 1510 | Time 4.5763(4.6567) | Bit/dim 1.2523(1.2563) | Steps 520(520.13) | Grad Norm 0.9093(4.1122) | Total Time 10.00(10.00)
Iter 1520 | Time 4.7970(4.6618) | Bit/dim 1.2583(1.2519) | Steps 520(520.10) | Grad Norm 7.4634(3.9485) | Total Time 10.00(10.00)
Iter 1530 | Time 4.9307(4.6932) | Bit/dim 1.2345(1.2526) | Steps 526(520.67) | Grad Norm 11.0677(6.0725) | Total Time 10.00(10.00)
Iter 1540 | Time 4.7150(4.7094) | Bit/dim 1.2593(1.2513) | Steps 526(521.47) | Grad Norm 12.1468(7.5363) | Total Time 10.00(10.00)
Iter 1550 | Time 4.7209(4.7212) | Bit/dim 1.2480(1.2488) | Steps 526(521.89) | Grad Norm 10.3122(7.6833) | Total Time 10.00(10.00)
Iter 1560 | Time 4.9519(4.7291) | Bit/dim 1.2612(1.2497) | Steps 526(522.33) | Grad Norm 17.9797(8.9544) | Total Time 10.00(10.00)
Iter 1570 | Time 4.8398(4.7212) | Bit/dim 1.3812(1.2763) | Steps 538(521.89) | Grad Norm 9.4076(11.7529) | Total Time 10.00(10.00)
Iter 1580 | Time 4.9151(4.7294) | Bit/dim 1.3285(1.2933) | Steps 526(522.55) | Grad Norm 21.8725(13.3433) | Total Time 10.00(10.00)
Iter 1590 | Time 4.6015(4.7203) | Bit/dim 1.2623(1.2937) | Steps 514(521.19) | Grad Norm 8.5228(13.7687) | Total Time 10.00(10.00)
Iter 1600 | Time 4.6095(4.7102) | Bit/dim 1.2958(1.2916) | Steps 514(520.96) | Grad Norm 9.4010(13.4621) | Total Time 10.00(10.00)
Iter 1610 | Time 4.7327(4.7178) | Bit/dim 1.2504(1.2847) | Steps 526(521.35) | Grad Norm 10.9328(11.9844) | Total Time 10.00(10.00)
Iter 1620 | Time 4.9002(4.7248) | Bit/dim 1.2334(1.2744) | Steps 526(521.34) | Grad Norm 10.6155(10.5269) | Total Time 10.00(10.00)
Iter 1630 | Time 4.5870(4.7215) | Bit/dim 1.2429(1.2647) | Steps 520(521.13) | Grad Norm 1.2484(8.9222) | Total Time 10.00(10.00)
Iter 1640 | Time 4.7606(4.7191) | Bit/dim 1.2251(1.2548) | Steps 520(520.84) | Grad Norm 4.4794(7.3454) | Total Time 10.00(10.00)
Iter 1650 | Time 4.6053(4.7060) | Bit/dim 1.2405(1.2480) | Steps 520(520.62) | Grad Norm 4.9029(6.4057) | Total Time 10.00(10.00)
Iter 1660 | Time 4.7842(4.7039) | Bit/dim 1.2279(1.2457) | Steps 520(520.45) | Grad Norm 4.5671(5.5211) | Total Time 10.00(10.00)
Iter 1670 | Time 4.5752(4.6944) | Bit/dim 1.1960(1.2412) | Steps 520(520.34) | Grad Norm 3.4693(4.6998) | Total Time 10.00(10.00)
Iter 1680 | Time 4.5699(4.6985) | Bit/dim 1.2403(1.2382) | Steps 520(520.68) | Grad Norm 2.1613(4.7491) | Total Time 10.00(10.00)
Iter 1690 | Time 4.5709(4.6997) | Bit/dim 1.2045(1.2345) | Steps 520(520.81) | Grad Norm 3.7033(5.0911) | Total Time 10.00(10.00)
Iter 1700 | Time 4.5619(4.7028) | Bit/dim 1.2095(1.2326) | Steps 520(521.24) | Grad Norm 5.6362(6.1190) | Total Time 10.00(10.00)
Iter 1710 | Time 4.9310(4.7240) | Bit/dim 1.2360(1.2290) | Steps 526(522.04) | Grad Norm 3.4524(6.8045) | Total Time 10.00(10.00)
Iter 1720 | Time 4.9450(4.7368) | Bit/dim 1.2707(1.2290) | Steps 526(522.63) | Grad Norm 21.7489(7.8247) | Total Time 10.00(10.00)
Iter 1730 | Time 4.1577(4.7331) | Bit/dim 1.4934(1.2662) | Steps 472(522.17) | Grad Norm 20.0123(11.5173) | Total Time 10.00(10.00)
Iter 1740 | Time 4.3254(4.7667) | Bit/dim 1.4777(1.2987) | Steps 502(523.47) | Grad Norm 9.9928(13.2063) | Total Time 10.00(10.00)
Iter 1750 | Time 4.8840(4.7493) | Bit/dim 1.2574(1.3143) | Steps 526(521.78) | Grad Norm 16.8341(12.7203) | Total Time 10.00(10.00)
Iter 1760 | Time 4.7503(4.7556) | Bit/dim 1.2481(1.3027) | Steps 520(521.89) | Grad Norm 6.8220(11.6393) | Total Time 10.00(10.00)
Iter 1770 | Time 4.5606(4.7218) | Bit/dim 1.2311(1.2874) | Steps 520(521.40) | Grad Norm 6.7111(10.3648) | Total Time 10.00(10.00)
Iter 1780 | Time 4.7717(4.7081) | Bit/dim 1.2282(1.2748) | Steps 520(521.03) | Grad Norm 5.4099(8.8236) | Total Time 10.00(10.00)
Iter 1790 | Time 4.5587(4.6985) | Bit/dim 1.2298(1.2630) | Steps 520(520.76) | Grad Norm 4.7960(7.5557) | Total Time 10.00(10.00)
validating...
Epoch 0006 | Time 73.5558, Bit/dim 1.2158
===> Using batch size 200. Total 300 iterations/epoch.
Iter 1800 | Time 4.7806(4.6905) | Bit/dim 1.2092(1.2508) | Steps 520(520.56) | Grad Norm 5.9679(6.4025) | Total Time 10.00(10.00)
Iter 1810 | Time 4.7737(4.6862) | Bit/dim 1.2103(1.2421) | Steps 520(520.75) | Grad Norm 9.6598(6.3423) | Total Time 10.00(10.00)
Iter 1820 | Time 4.9439(4.7198) | Bit/dim 1.2288(1.2392) | Steps 526(522.01) | Grad Norm 14.5178(8.3266) | Total Time 10.00(10.00)
Iter 1830 | Time 4.5723(4.7245) | Bit/dim 1.2290(1.2363) | Steps 520(522.66) | Grad Norm 2.1070(8.4807) | Total Time 10.00(10.00)
Iter 1840 | Time 4.7360(4.7332) | Bit/dim 1.2231(1.2323) | Steps 526(522.62) | Grad Norm 13.3217(8.2336) | Total Time 10.00(10.00)
Iter 1850 | Time 5.1531(4.7630) | Bit/dim 1.2509(1.2377) | Steps 538(524.01) | Grad Norm 30.7199(10.8134) | Total Time 10.00(10.00)
Iter 1860 | Time 4.9244(4.7867) | Bit/dim 1.2779(1.2529) | Steps 526(524.68) | Grad Norm 19.1344(12.9397) | Total Time 10.00(10.00)
Iter 1870 | Time 4.5659(4.7781) | Bit/dim 1.2174(1.2577) | Steps 520(524.16) | Grad Norm 7.6889(13.6485) | Total Time 10.00(10.00)
Iter 1880 | Time 4.7564(4.8045) | Bit/dim 1.2339(1.2528) | Steps 520(524.56) | Grad Norm 4.8227(13.1873) | Total Time 10.00(10.00)
Iter 1890 | Time 4.5771(4.7758) | Bit/dim 1.2195(1.2451) | Steps 520(523.80) | Grad Norm 8.9027(11.6519) | Total Time 10.00(10.00)
Iter 1900 | Time 4.7751(4.7492) | Bit/dim 1.1967(1.2380) | Steps 520(522.80) | Grad Norm 3.7737(9.6766) | Total Time 10.00(10.00)
Iter 1910 | Time 4.5566(4.7215) | Bit/dim 1.2232(1.2312) | Steps 520(522.07) | Grad Norm 5.9149(7.9753) | Total Time 10.00(10.00)
Iter 1920 | Time 4.7566(4.7018) | Bit/dim 1.2409(1.2258) | Steps 520(521.52) | Grad Norm 2.8363(6.7639) | Total Time 10.00(10.00)
Iter 1930 | Time 4.7705(4.6862) | Bit/dim 1.1934(1.2218) | Steps 520(521.12) | Grad Norm 1.7568(5.6437) | Total Time 10.00(10.00)
Iter 1940 | Time 4.7698(4.6828) | Bit/dim 1.2211(1.2192) | Steps 520(520.83) | Grad Norm 2.4028(4.6033) | Total Time 10.00(10.00)
Iter 1950 | Time 4.5502(4.6687) | Bit/dim 1.2316(1.2190) | Steps 520(520.61) | Grad Norm 1.4968(4.2591) | Total Time 10.00(10.00)
Iter 1960 | Time 4.5755(4.6627) | Bit/dim 1.2430(1.2173) | Steps 520(520.61) | Grad Norm 3.1906(3.9865) | Total Time 10.00(10.00)
Iter 1970 | Time 4.5638(4.6579) | Bit/dim 1.1863(1.2118) | Steps 520(520.62) | Grad Norm 3.9754(4.1630) | Total Time 10.00(10.00)
Iter 1980 | Time 5.0842(4.6677) | Bit/dim 1.2059(1.2123) | Steps 532(520.68) | Grad Norm 11.8057(4.2119) | Total Time 10.00(10.00)
Iter 1990 | Time 5.1997(4.7084) | Bit/dim 1.2737(1.2128) | Steps 544(522.09) | Grad Norm 36.3267(6.5615) | Total Time 10.00(10.00)
Iter 2000 | Time 5.0288(4.7419) | Bit/dim 1.2505(1.2348) | Steps 538(523.69) | Grad Norm 9.2985(8.5993) | Total Time 10.00(10.00)
Iter 2010 | Time 4.7722(4.7538) | Bit/dim 1.2761(1.2466) | Steps 532(523.17) | Grad Norm 10.2952(10.5949) | Total Time 10.00(10.00)
Iter 2020 | Time 4.3360(4.7622) | Bit/dim 1.3869(1.2864) | Steps 508(524.07) | Grad Norm 9.4512(11.8127) | Total Time 10.00(10.00)
Iter 2030 | Time 4.5045(4.7412) | Bit/dim 1.2710(1.2817) | Steps 514(523.68) | Grad Norm 11.4363(10.8016) | Total Time 10.00(10.00)
Iter 2040 | Time 4.4283(4.6983) | Bit/dim 1.2137(1.2663) | Steps 502(521.76) | Grad Norm 3.3944(9.4399) | Total Time 10.00(10.00)
Iter 2050 | Time 4.4116(4.6637) | Bit/dim 1.2176(1.2512) | Steps 514(519.93) | Grad Norm 2.6285(7.6573) | Total Time 10.00(10.00)
Iter 2060 | Time 4.5464(4.6526) | Bit/dim 1.1756(1.2380) | Steps 520(519.81) | Grad Norm 1.7756(6.1557) | Total Time 10.00(10.00)
Iter 2070 | Time 4.5717(4.6553) | Bit/dim 1.1582(1.2250) | Steps 520(519.86) | Grad Norm 1.4515(5.0228) | Total Time 10.00(10.00)
Iter 2080 | Time 4.7509(4.6612) | Bit/dim 1.1908(1.2152) | Steps 520(519.90) | Grad Norm 1.1787(4.2137) | Total Time 10.00(10.00)
Iter 2090 | Time 5.1014(4.6741) | Bit/dim 1.2203(1.2104) | Steps 532(520.28) | Grad Norm 5.7763(3.9568) | Total Time 10.00(10.00)
validating...
Epoch 0007 | Time 75.6840, Bit/dim 1.1872
===> Using batch size 200. Total 300 iterations/epoch.
Iter 2100 | Time 4.5810(4.6891) | Bit/dim 1.1928(1.2052) | Steps 520(520.39) | Grad Norm 5.3548(4.3699) | Total Time 10.00(10.00)
Iter 2110 | Time 5.1525(4.7330) | Bit/dim 1.2147(1.2059) | Steps 550(521.65) | Grad Norm 25.0241(6.3598) | Total Time 10.00(10.00)
Iter 2120 | Time 4.9294(4.7550) | Bit/dim 1.1806(1.2101) | Steps 526(521.93) | Grad Norm 3.1946(7.4184) | Total Time 10.00(10.00)
Iter 2130 | Time 4.5238(4.7304) | Bit/dim 1.2068(1.2067) | Steps 514(520.96) | Grad Norm 2.3580(6.3798) | Total Time 10.00(10.00)
Iter 2140 | Time 4.5544(4.6960) | Bit/dim 1.1785(1.2044) | Steps 520(520.06) | Grad Norm 2.3599(5.2998) | Total Time 10.00(10.00)
Iter 2150 | Time 4.8910(4.7285) | Bit/dim 1.1982(1.2003) | Steps 532(520.09) | Grad Norm 7.4386(5.9310) | Total Time 10.00(10.00)
Iter 2160 | Time 4.5932(4.7160) | Bit/dim 1.1832(1.1982) | Steps 508(519.40) | Grad Norm 9.1912(6.5247) | Total Time 10.00(10.00)
Iter 2170 | Time 5.1317(4.7570) | Bit/dim 1.2684(1.2050) | Steps 550(521.22) | Grad Norm 20.1001(9.1014) | Total Time 10.00(10.00)
Iter 2180 | Time 4.5566(4.7900) | Bit/dim 1.3071(1.2425) | Steps 520(521.58) | Grad Norm 6.4351(10.7093) | Total Time 10.00(10.00)
Iter 2190 | Time 4.8965(4.8023) | Bit/dim 1.2092(1.2497) | Steps 538(523.00) | Grad Norm 6.5358(11.0621) | Total Time 10.00(10.00)
Iter 2200 | Time 4.2851(4.7755) | Bit/dim 1.3036(1.2526) | Steps 490(523.42) | Grad Norm 15.5896(12.4714) | Total Time 10.00(10.00)
Iter 2210 | Time 4.7139(4.7593) | Bit/dim 1.2655(1.2666) | Steps 526(523.20) | Grad Norm 7.3177(13.3374) | Total Time 10.00(10.00)
Iter 2220 | Time 5.0915(4.7272) | Bit/dim 1.2903(1.2677) | Steps 544(520.83) | Grad Norm 24.8490(13.6712) | Total Time 10.00(10.00)
Iter 2230 | Time 5.3449(4.7319) | Bit/dim 1.2767(1.2635) | Steps 550(519.76) | Grad Norm 22.9123(13.8273) | Total Time 10.00(10.00)
Iter 2240 | Time 4.5915(4.7171) | Bit/dim 1.2026(1.2509) | Steps 508(518.78) | Grad Norm 9.9583(12.9946) | Total Time 10.00(10.00)
Iter 2250 | Time 4.5710(4.7053) | Bit/dim 1.2151(1.2394) | Steps 520(518.48) | Grad Norm 11.0915(11.9132) | Total Time 10.00(10.00)
Iter 2260 | Time 4.7564(4.6928) | Bit/dim 1.1734(1.2237) | Steps 520(517.94) | Grad Norm 2.0939(10.1237) | Total Time 10.00(10.00)
Iter 2270 | Time 4.6841(4.6794) | Bit/dim 1.1946(1.2155) | Steps 520(517.25) | Grad Norm 2.0361(8.4697) | Total Time 10.00(10.00)
Iter 2280 | Time 4.5670(4.6698) | Bit/dim 1.1998(1.2089) | Steps 520(517.97) | Grad Norm 4.3257(7.1124) | Total Time 10.00(10.00)
Iter 2290 | Time 4.5672(4.6619) | Bit/dim 1.1825(1.2010) | Steps 520(518.36) | Grad Norm 4.5856(6.1133) | Total Time 10.00(10.00)
Iter 2300 | Time 4.7716(4.6605) | Bit/dim 1.1384(1.1929) | Steps 520(517.96) | Grad Norm 1.6802(5.3914) | Total Time 10.00(10.00)
Iter 2310 | Time 4.6615(4.6691) | Bit/dim 1.1967(1.1909) | Steps 520(518.33) | Grad Norm 5.9162(5.1618) | Total Time 10.00(10.00)
Iter 2320 | Time 4.6294(4.6574) | Bit/dim 1.1788(1.1896) | Steps 502(517.18) | Grad Norm 3.7097(4.9044) | Total Time 10.00(10.00)
Iter 2330 | Time 4.7701(4.6536) | Bit/dim 1.1789(1.1869) | Steps 520(517.00) | Grad Norm 3.8450(4.4090) | Total Time 10.00(10.00)
Iter 2340 | Time 4.5255(4.6339) | Bit/dim 1.1310(1.1846) | Steps 514(515.48) | Grad Norm 1.5001(3.8813) | Total Time 10.00(10.00)
Iter 2350 | Time 4.7906(4.6261) | Bit/dim 1.1797(1.1836) | Steps 508(513.37) | Grad Norm 8.8593(4.0077) | Total Time 10.00(10.00)
Iter 2360 | Time 4.7207(4.6656) | Bit/dim 1.1350(1.1821) | Steps 514(515.02) | Grad Norm 5.1544(5.3595) | Total Time 10.00(10.00)
Iter 2370 | Time 4.6709(4.6583) | Bit/dim 1.2113(1.1822) | Steps 532(514.13) | Grad Norm 4.0933(5.9104) | Total Time 10.00(10.00)
Iter 2380 | Time 5.6450(4.7678) | Bit/dim 1.4636(1.2312) | Steps 574(517.96) | Grad Norm 10.3222(9.4584) | Total Time 10.00(10.00)
Iter 2390 | Time 4.7182(4.7825) | Bit/dim 1.2393(1.2488) | Steps 526(521.51) | Grad Norm 4.9744(9.1434) | Total Time 10.00(10.00)
validating...
Epoch 0008 | Time 70.4204, Bit/dim 1.2018
===> Using batch size 200. Total 300 iterations/epoch.
Iter 2400 | Time 4.8785(4.7735) | Bit/dim 1.1961(1.2406) | Steps 532(522.49) | Grad Norm 10.0187(8.8538) | Total Time 10.00(10.00)
Iter 2410 | Time 4.7687(4.7451) | Bit/dim 1.1841(1.2284) | Steps 520(521.69) | Grad Norm 2.7725(7.5050) | Total Time 10.00(10.00)
Iter 2420 | Time 4.6186(4.7200) | Bit/dim 1.2056(1.2142) | Steps 502(518.55) | Grad Norm 2.9861(6.5096) | Total Time 10.00(10.00)
Iter 2430 | Time 4.5802(4.6965) | Bit/dim 1.2133(1.2061) | Steps 496(515.88) | Grad Norm 2.3349(5.6215) | Total Time 10.00(10.00)
Iter 2440 | Time 4.5945(4.6450) | Bit/dim 1.1808(1.1983) | Steps 508(511.65) | Grad Norm 7.2517(4.8819) | Total Time 10.00(10.00)
Iter 2450 | Time 4.3914(4.6237) | Bit/dim 1.1714(1.1934) | Steps 502(509.95) | Grad Norm 6.5012(4.3609) | Total Time 10.00(10.00)
Iter 2460 | Time 4.5451(4.6542) | Bit/dim 1.2279(1.1943) | Steps 502(511.14) | Grad Norm 14.1522(6.8832) | Total Time 10.00(10.00)
Iter 2470 | Time 5.1639(4.7447) | Bit/dim 1.4908(1.2444) | Steps 526(513.81) | Grad Norm 5.7925(9.4619) | Total Time 10.00(10.00)
Iter 2480 | Time 4.7089(4.7663) | Bit/dim 1.2966(1.2756) | Steps 532(518.30) | Grad Norm 6.6348(8.7293) | Total Time 10.00(10.00)
Iter 2490 | Time 5.1312(4.7421) | Bit/dim 1.2213(1.2630) | Steps 538(519.48) | Grad Norm 7.2795(8.2187) | Total Time 10.00(10.00)
Iter 2500 | Time 4.5735(4.6929) | Bit/dim 1.1766(1.2462) | Steps 520(516.04) | Grad Norm 2.1434(7.3022) | Total Time 10.00(10.00)
Iter 2510 | Time 5.0962(4.6783) | Bit/dim 1.1824(1.2270) | Steps 532(514.85) | Grad Norm 9.8619(6.8518) | Total Time 10.00(10.00)
Iter 2520 | Time 5.4019(4.6892) | Bit/dim 1.3291(1.2275) | Steps 556(514.30) | Grad Norm 44.9018(9.7691) | Total Time 10.00(10.00)
Iter 2530 | Time 5.1890(4.6987) | Bit/dim 1.2541(1.2436) | Steps 544(514.17) | Grad Norm 6.3903(11.1655) | Total Time 10.00(10.00)
Iter 2540 | Time 4.4114(4.6701) | Bit/dim 1.2533(1.2503) | Steps 490(512.27) | Grad Norm 9.1597(11.9034) | Total Time 10.00(10.00)
Iter 2550 | Time 4.4287(4.6854) | Bit/dim 1.2819(1.2532) | Steps 490(513.00) | Grad Norm 14.1442(12.6425) | Total Time 10.00(10.00)
Iter 2560 | Time 4.0553(4.6522) | Bit/dim 1.2053(1.2442) | Steps 484(511.20) | Grad Norm 12.2900(12.5057) | Total Time 10.00(10.00)
Iter 2570 | Time 4.5038(4.6642) | Bit/dim 1.1919(1.2339) | Steps 514(512.03) | Grad Norm 13.2769(11.9901) | Total Time 10.00(10.00)
Iter 2580 | Time 4.4201(4.6442) | Bit/dim 1.1685(1.2200) | Steps 502(511.45) | Grad Norm 5.2169(10.3181) | Total Time 10.00(10.00)
Iter 2590 | Time 4.4102(4.6128) | Bit/dim 1.1800(1.2064) | Steps 502(510.00) | Grad Norm 3.8133(8.7092) | Total Time 10.00(10.00)
Iter 2600 | Time 4.4302(4.5917) | Bit/dim 1.1800(1.1981) | Steps 502(509.67) | Grad Norm 1.3910(6.9285) | Total Time 10.00(10.00)
Iter 2610 | Time 4.4583(4.5754) | Bit/dim 1.1512(1.1885) | Steps 508(508.29) | Grad Norm 1.6339(5.5685) | Total Time 10.00(10.00)
Iter 2620 | Time 4.3589(4.5737) | Bit/dim 1.1905(1.1817) | Steps 496(507.74) | Grad Norm 1.2789(4.5783) | Total Time 10.00(10.00)
Iter 2630 | Time 4.6269(4.5735) | Bit/dim 1.1699(1.1778) | Steps 502(507.50) | Grad Norm 0.8640(3.8781) | Total Time 10.00(10.00)
Iter 2640 | Time 4.7679(4.5686) | Bit/dim 1.1663(1.1768) | Steps 520(506.91) | Grad Norm 3.9099(3.5550) | Total Time 10.00(10.00)
Iter 2650 | Time 4.5744(4.5683) | Bit/dim 1.1565(1.1732) | Steps 520(506.15) | Grad Norm 4.8849(3.5301) | Total Time 10.00(10.00)
Iter 2660 | Time 4.6212(4.5859) | Bit/dim 1.1451(1.1701) | Steps 502(506.39) | Grad Norm 6.7406(3.9528) | Total Time 10.00(10.00)
Iter 2670 | Time 4.4029(4.5805) | Bit/dim 1.1522(1.1690) | Steps 502(505.74) | Grad Norm 3.6832(4.3616) | Total Time 10.00(10.00)
Iter 2680 | Time 5.0501(4.6145) | Bit/dim 1.1519(1.1690) | Steps 526(506.82) | Grad Norm 9.1104(5.5147) | Total Time 10.00(10.00)
Iter 2690 | Time 4.5489(4.6239) | Bit/dim 1.1848(1.1702) | Steps 520(507.95) | Grad Norm 2.4105(6.7276) | Total Time 10.00(10.00)
validating...
Epoch 0009 | Time 73.0200, Bit/dim 1.2372
===> Using batch size 200. Total 300 iterations/epoch.
Iter 2700 | Time 4.7798(4.6802) | Bit/dim 1.2706(1.2028) | Steps 520(510.77) | Grad Norm 5.5935(8.9077) | Total Time 10.00(10.00)
Iter 2710 | Time 4.6056(4.6611) | Bit/dim 1.2224(1.2082) | Steps 502(510.43) | Grad Norm 8.1304(8.6303) | Total Time 10.00(10.00)
Iter 2720 | Time 4.6260(4.6462) | Bit/dim 1.2336(1.2179) | Steps 508(512.08) | Grad Norm 12.7746(10.4226) | Total Time 10.00(10.00)
Iter 2730 | Time 5.1246(4.6457) | Bit/dim 1.2669(1.2258) | Steps 550(512.52) | Grad Norm 30.8279(12.6045) | Total Time 10.00(10.00)
Iter 2740 | Time 4.5436(4.6351) | Bit/dim 1.1954(1.2200) | Steps 502(511.40) | Grad Norm 9.9822(12.1610) | Total Time 10.00(10.00)
Iter 2750 | Time 4.5353(4.6584) | Bit/dim 1.1663(1.2086) | Steps 502(512.64) | Grad Norm 11.2510(12.1297) | Total Time 10.00(10.00)
Iter 2760 | Time 4.7041(4.7001) | Bit/dim 1.1591(1.2002) | Steps 508(513.29) | Grad Norm 10.8231(11.6884) | Total Time 10.00(10.00)
Iter 2770 | Time 4.7263(4.6983) | Bit/dim 1.1507(1.1918) | Steps 502(512.07) | Grad Norm 7.4165(10.6683) | Total Time 10.00(10.00)
Iter 2780 | Time 5.1804(4.7357) | Bit/dim 1.1882(1.1888) | Steps 544(513.55) | Grad Norm 19.8899(11.4952) | Total Time 10.00(10.00)
Iter 2790 | Time 4.7981(4.7469) | Bit/dim 1.1615(1.1801) | Steps 508(513.82) | Grad Norm 4.8433(10.6451) | Total Time 10.00(10.00)
Iter 2800 | Time 4.8933(4.7425) | Bit/dim 1.2172(1.1772) | Steps 508(513.42) | Grad Norm 13.2644(10.7662) | Total Time 10.00(10.00)
Iter 2810 | Time 5.1254(4.7528) | Bit/dim 1.2249(1.1866) | Steps 550(515.62) | Grad Norm 26.7993(12.3394) | Total Time 10.00(10.00)
Iter 2820 | Time 5.0223(4.7287) | Bit/dim 1.2387(1.2122) | Steps 538(513.72) | Grad Norm 5.5408(12.7229) | Total Time 10.00(10.00)
Iter 2830 | Time 5.0848(4.7120) | Bit/dim 1.2172(1.2156) | Steps 532(512.66) | Grad Norm 28.8154(12.5588) | Total Time 10.00(10.00)
Iter 2840 | Time 4.5793(4.6809) | Bit/dim 1.2426(1.2166) | Steps 496(511.68) | Grad Norm 15.0899(13.1313) | Total Time 10.00(10.00)
Iter 2850 | Time 4.1712(4.6592) | Bit/dim 1.1646(1.2117) | Steps 484(510.39) | Grad Norm 13.8278(13.3700) | Total Time 10.00(10.00)
Iter 2860 | Time 4.7301(4.6716) | Bit/dim 1.1791(1.2021) | Steps 526(511.21) | Grad Norm 12.5180(12.8023) | Total Time 10.00(10.00)
Iter 2870 | Time 4.5714(4.6562) | Bit/dim 1.1537(1.1920) | Steps 496(509.56) | Grad Norm 5.1814(10.8501) | Total Time 10.00(10.00)
Iter 2880 | Time 4.3824(4.6137) | Bit/dim 1.1586(1.1847) | Steps 496(507.68) | Grad Norm 3.8930(8.9367) | Total Time 10.00(10.00)
Iter 2890 | Time 4.3571(4.5889) | Bit/dim 1.1702(1.1809) | Steps 496(505.80) | Grad Norm 1.4209(7.0830) | Total Time 10.00(10.00)
Iter 2900 | Time 4.4761(4.5734) | Bit/dim 1.1349(1.1723) | Steps 508(504.82) | Grad Norm 5.4575(6.0153) | Total Time 10.00(10.00)
Iter 2910 | Time 4.7339(4.5855) | Bit/dim 1.1556(1.1683) | Steps 502(505.21) | Grad Norm 11.5078(6.0667) | Total Time 10.00(10.00)
Iter 2920 | Time 4.2527(4.6356) | Bit/dim 1.2044(1.1713) | Steps 484(508.16) | Grad Norm 18.8299(8.3877) | Total Time 10.00(10.00)
Iter 2930 | Time 3.9927(4.6347) | Bit/dim 1.2291(1.1904) | Steps 472(508.84) | Grad Norm 13.1079(10.0850) | Total Time 10.00(10.00)
Iter 2940 | Time 4.1144(4.6045) | Bit/dim 1.2336(1.2031) | Steps 466(507.09) | Grad Norm 13.5450(11.2103) | Total Time 10.00(10.00)
Iter 2950 | Time 4.8516(4.6082) | Bit/dim 1.1769(1.2015) | Steps 532(508.87) | Grad Norm 15.3308(11.1132) | Total Time 10.00(10.00)
Iter 2960 | Time 4.4149(4.6154) | Bit/dim 1.1542(1.1932) | Steps 502(509.36) | Grad Norm 4.2423(10.4184) | Total Time 10.00(10.00)
Iter 2970 | Time 4.4605(4.5964) | Bit/dim 1.1804(1.1852) | Steps 508(507.10) | Grad Norm 2.8365(8.8110) | Total Time 10.00(10.00)
Iter 2980 | Time 4.6686(4.5778) | Bit/dim 1.1774(1.1770) | Steps 508(505.77) | Grad Norm 2.8797(7.2921) | Total Time 10.00(10.00)
Iter 2990 | Time 4.7360(4.5592) | Bit/dim 1.1677(1.1712) | Steps 502(504.47) | Grad Norm 5.1126(6.1347) | Total Time 10.00(10.00)
validating...
Epoch 0010 | Time 65.0468, Bit/dim 1.1467
===> Using batch size 200. Total 300 iterations/epoch.
Iter 3000 | Time 4.3889(4.5412) | Bit/dim 1.1561(1.1661) | Steps 502(503.20) | Grad Norm 4.2095(5.3009) | Total Time 10.00(10.00)
Iter 3010 | Time 4.7994(4.5301) | Bit/dim 1.1800(1.1633) | Steps 508(502.47) | Grad Norm 4.7234(4.9286) | Total Time 10.00(10.00)
Iter 3020 | Time 4.7987(4.5524) | Bit/dim 1.0994(1.1581) | Steps 508(503.02) | Grad Norm 4.7140(5.0629) | Total Time 10.00(10.00)
Iter 3030 | Time 4.6179(4.5439) | Bit/dim 1.1293(1.1562) | Steps 502(502.11) | Grad Norm 3.6909(4.4011) | Total Time 10.00(10.00)
Iter 3040 | Time 4.5414(4.5489) | Bit/dim 1.1462(1.1528) | Steps 502(502.40) | Grad Norm 1.4529(4.3566) | Total Time 10.00(10.00)
Iter 3050 | Time 4.5993(4.6146) | Bit/dim 1.1549(1.1522) | Steps 508(505.01) | Grad Norm 4.4015(5.6270) | Total Time 10.00(10.00)
Iter 3060 | Time 5.1571(4.6499) | Bit/dim 1.2857(1.1706) | Steps 556(508.11) | Grad Norm 35.4289(8.7466) | Total Time 10.00(10.00)
Iter 3070 | Time 4.8120(4.6294) | Bit/dim 1.2349(1.1940) | Steps 526(508.24) | Grad Norm 44.5321(11.1794) | Total Time 10.00(10.00)
Iter 3080 | Time 4.3574(4.6116) | Bit/dim 1.2776(1.2299) | Steps 496(508.91) | Grad Norm 11.1231(11.0385) | Total Time 10.00(10.00)
Iter 3090 | Time 4.1942(4.6148) | Bit/dim 1.3137(1.2305) | Steps 478(508.65) | Grad Norm 13.2902(11.4283) | Total Time 10.00(10.00)
Iter 3100 | Time 4.2035(4.5990) | Bit/dim 1.2750(1.2333) | Steps 478(507.64) | Grad Norm 11.4844(12.7532) | Total Time 10.00(10.00)
Iter 3110 | Time 5.0804(4.5805) | Bit/dim 1.2302(1.2340) | Steps 544(507.13) | Grad Norm 25.5344(12.7206) | Total Time 10.00(10.00)
Iter 3120 | Time 5.0463(4.5686) | Bit/dim 1.1746(1.2227) | Steps 538(506.70) | Grad Norm 22.0006(12.1771) | Total Time 10.00(10.00)
Iter 3130 | Time 4.4212(4.5880) | Bit/dim 1.1907(1.2141) | Steps 490(506.93) | Grad Norm 9.3932(12.0631) | Total Time 10.00(10.00)
Iter 3140 | Time 4.4391(4.5757) | Bit/dim 1.1406(1.2014) | Steps 502(505.45) | Grad Norm 9.3444(10.7931) | Total Time 10.00(10.00)
Iter 3150 | Time 4.4249(4.5686) | Bit/dim 1.1726(1.1901) | Steps 502(504.53) | Grad Norm 5.2221(9.3368) | Total Time 10.00(10.00)
Iter 3160 | Time 4.6192(4.5716) | Bit/dim 1.1432(1.1826) | Steps 502(503.51) | Grad Norm 2.3356(7.6827) | Total Time 10.00(10.00)
Iter 3170 | Time 4.6000(4.5519) | Bit/dim 1.1495(1.1741) | Steps 496(502.19) | Grad Norm 3.4187(6.2629) | Total Time 10.00(10.00)
Iter 3180 | Time 4.3682(4.5371) | Bit/dim 1.1563(1.1655) | Steps 496(501.34) | Grad Norm 1.4113(5.1391) | Total Time 10.00(10.00)
Iter 3190 | Time 4.6650(4.5313) | Bit/dim 1.1621(1.1595) | Steps 508(501.07) | Grad Norm 3.7385(4.4610) | Total Time 10.00(10.00)
Iter 3200 | Time 4.5887(4.5246) | Bit/dim 1.1503(1.1563) | Steps 496(500.54) | Grad Norm 1.7437(3.8774) | Total Time 10.00(10.00)
Iter 3210 | Time 4.6333(4.5150) | Bit/dim 1.1764(1.1550) | Steps 502(500.33) | Grad Norm 0.6778(3.1803) | Total Time 10.00(10.00)
Iter 3220 | Time 4.3875(4.4981) | Bit/dim 1.1336(1.1521) | Steps 496(499.67) | Grad Norm 1.4344(2.9825) | Total Time 10.00(10.00)
Iter 3230 | Time 4.5951(4.5208) | Bit/dim 1.1510(1.1504) | Steps 502(500.14) | Grad Norm 1.7500(3.3356) | Total Time 10.00(10.00)
Iter 3240 | Time 4.7141(4.5295) | Bit/dim 1.1433(1.1503) | Steps 514(500.73) | Grad Norm 11.3824(4.1237) | Total Time 10.00(10.00)
Iter 3250 | Time 4.6425(4.5725) | Bit/dim 1.1278(1.1572) | Steps 532(503.98) | Grad Norm 4.7489(6.1156) | Total Time 10.00(10.00)
Iter 3260 | Time 4.5169(4.5698) | Bit/dim 1.1528(1.1576) | Steps 502(503.99) | Grad Norm 10.9411(6.4942) | Total Time 10.00(10.00)
Iter 3270 | Time 5.2485(4.6194) | Bit/dim 1.2548(1.1574) | Steps 550(506.10) | Grad Norm 47.3305(8.3240) | Total Time 10.00(10.00)
Iter 3280 | Time 4.3462(4.5993) | Bit/dim 1.2398(1.1952) | Steps 496(506.38) | Grad Norm 9.7628(9.4202) | Total Time 10.00(10.00)
Iter 3290 | Time 4.7723(4.6262) | Bit/dim 1.2523(1.2059) | Steps 550(512.34) | Grad Norm 28.7194(10.9098) | Total Time 10.00(10.00)
validating...
Epoch 0011 | Time 66.3895, Bit/dim 1.2260
===> Using batch size 200. Total 300 iterations/epoch.
Iter 3300 | Time 4.2561(4.5987) | Bit/dim 1.2241(1.2139) | Steps 484(512.13) | Grad Norm 14.0634(12.4778) | Total Time 10.00(10.00)
Iter 3310 | Time 4.9405(4.5885) | Bit/dim 1.2554(1.2214) | Steps 538(510.54) | Grad Norm 34.6786(12.7963) | Total Time 10.00(10.00)
Iter 3320 | Time 4.4735(4.5526) | Bit/dim 1.1653(1.2164) | Steps 508(508.00) | Grad Norm 11.5780(12.2096) | Total Time 10.00(10.00)
Iter 3330 | Time 4.7316(4.5493) | Bit/dim 1.1681(1.2040) | Steps 502(506.46) | Grad Norm 9.0343(11.5054) | Total Time 10.00(10.00)
Iter 3340 | Time 4.6793(4.5777) | Bit/dim 1.1490(1.1901) | Steps 508(506.27) | Grad Norm 5.8545(10.3011) | Total Time 10.00(10.00)
Iter 3350 | Time 4.4798(4.5660) | Bit/dim 1.1470(1.1772) | Steps 508(505.04) | Grad Norm 7.2582(8.8365) | Total Time 10.00(10.00)
Iter 3360 | Time 4.6295(4.5466) | Bit/dim 1.1194(1.1684) | Steps 502(503.77) | Grad Norm 1.7621(7.3712) | Total Time 10.00(10.00)
Iter 3370 | Time 4.5763(4.5319) | Bit/dim 1.1523(1.1619) | Steps 496(502.81) | Grad Norm 1.4668(5.9288) | Total Time 10.00(10.00)
Iter 3380 | Time 4.4158(4.5190) | Bit/dim 1.1073(1.1551) | Steps 502(501.98) | Grad Norm 2.8320(5.1750) | Total Time 10.00(10.00)
Iter 3390 | Time 4.4268(4.5215) | Bit/dim 1.0980(1.1497) | Steps 502(502.03) | Grad Norm 1.8467(4.4640) | Total Time 10.00(10.00)
Iter 3400 | Time 4.5877(4.5207) | Bit/dim 1.1128(1.1466) | Steps 496(501.39) | Grad Norm 3.1049(3.8201) | Total Time 10.00(10.00)
Iter 3410 | Time 4.7494(4.5155) | Bit/dim 1.1442(1.1447) | Steps 502(501.79) | Grad Norm 5.9518(3.8181) | Total Time 10.00(10.00)
Iter 3420 | Time 4.3790(4.5172) | Bit/dim 1.1370(1.1442) | Steps 496(501.54) | Grad Norm 3.6110(4.0543) | Total Time 10.00(10.00)
Iter 3430 | Time 4.3615(4.5565) | Bit/dim 1.1361(1.1439) | Steps 496(502.59) | Grad Norm 3.6506(4.5814) | Total Time 10.00(10.00)
Iter 3440 | Time 4.5566(4.5528) | Bit/dim 1.1724(1.1452) | Steps 502(502.00) | Grad Norm 11.7190(4.7880) | Total Time 10.00(10.00)
Iter 3450 | Time 4.8689(4.5576) | Bit/dim 1.2771(1.1737) | Steps 538(503.49) | Grad Norm 7.3693(7.2370) | Total Time 10.00(10.00)
Iter 3460 | Time 4.7849(4.5526) | Bit/dim 1.2203(1.1966) | Steps 520(502.76) | Grad Norm 5.4535(8.6205) | Total Time 10.00(10.00)
Iter 3470 | Time 4.1754(4.5356) | Bit/dim 1.1833(1.2120) | Steps 484(501.46) | Grad Norm 7.7707(10.4436) | Total Time 10.00(10.00)
Iter 3480 | Time 4.9125(4.5377) | Bit/dim 1.1811(1.2092) | Steps 526(502.37) | Grad Norm 10.4123(11.3872) | Total Time 10.00(10.00)
Iter 3490 | Time 5.2006(4.5752) | Bit/dim 1.2159(1.2068) | Steps 544(504.70) | Grad Norm 28.6741(12.4968) | Total Time 10.00(10.00)
Iter 3500 | Time 5.3009(4.5709) | Bit/dim 1.2053(1.2089) | Steps 556(505.43) | Grad Norm 27.1033(12.9075) | Total Time 10.00(10.00)
Iter 3510 | Time 5.1209(4.5980) | Bit/dim 1.1852(1.1992) | Steps 550(507.21) | Grad Norm 22.7041(12.5098) | Total Time 10.00(10.00)
Iter 3520 | Time 4.4142(4.6056) | Bit/dim 1.1658(1.1927) | Steps 490(506.67) | Grad Norm 10.2259(12.1963) | Total Time 10.00(10.00)
Iter 3530 | Time 4.2841(4.5833) | Bit/dim 1.1801(1.1885) | Steps 490(506.86) | Grad Norm 12.1766(12.1860) | Total Time 10.00(10.00)
Iter 3540 | Time 4.6893(4.5862) | Bit/dim 1.1882(1.1812) | Steps 508(506.93) | Grad Norm 6.2598(11.3969) | Total Time 10.00(10.00)
Iter 3550 | Time 4.7369(4.5938) | Bit/dim 1.1377(1.1745) | Steps 502(506.25) | Grad Norm 5.9257(10.2633) | Total Time 10.00(10.00)
Iter 3560 | Time 4.7422(4.5933) | Bit/dim 1.1540(1.1671) | Steps 502(506.28) | Grad Norm 7.6755(9.4351) | Total Time 10.00(10.00)
Iter 3570 | Time 4.5576(4.6123) | Bit/dim 1.1060(1.1587) | Steps 502(506.44) | Grad Norm 7.3782(8.9737) | Total Time 10.00(10.00)
Iter 3580 | Time 4.3791(4.6046) | Bit/dim 1.1467(1.1528) | Steps 496(506.26) | Grad Norm 3.0457(8.1442) | Total Time 10.00(10.00)
Iter 3590 | Time 4.6365(4.6115) | Bit/dim 1.1217(1.1503) | Steps 502(506.16) | Grad Norm 4.7810(8.2501) | Total Time 10.00(10.00)
validating...
Epoch 0012 | Time 65.0903, Bit/dim 1.1315
===> Using batch size 200. Total 300 iterations/epoch.
Iter 3600 | Time 4.6254(4.6267) | Bit/dim 1.1467(1.1485) | Steps 502(506.34) | Grad Norm 6.9407(8.3205) | Total Time 10.00(10.00)
Iter 3610 | Time 4.5505(4.5933) | Bit/dim 1.1608(1.1444) | Steps 502(504.61) | Grad Norm 6.0591(7.2098) | Total Time 10.00(10.00)
Iter 3620 | Time 4.4115(4.5680) | Bit/dim 1.1670(1.1432) | Steps 502(503.16) | Grad Norm 0.8323(6.1405) | Total Time 10.00(10.00)
Iter 3630 | Time 4.4243(4.5393) | Bit/dim 1.1178(1.1421) | Steps 502(501.61) | Grad Norm 4.8220(5.3274) | Total Time 10.00(10.00)
Iter 3640 | Time 4.3344(4.5386) | Bit/dim 1.1046(1.1360) | Steps 490(501.00) | Grad Norm 4.3797(4.9668) | Total Time 10.00(10.00)
Iter 3650 | Time 4.4344(4.5380) | Bit/dim 1.1193(1.1354) | Steps 502(500.81) | Grad Norm 2.4608(4.4423) | Total Time 10.00(10.00)
Iter 3660 | Time 4.3851(4.5249) | Bit/dim 1.1132(1.1348) | Steps 496(499.71) | Grad Norm 4.3813(3.9436) | Total Time 10.00(10.00)
Iter 3670 | Time 4.3322(4.5259) | Bit/dim 1.1082(1.1317) | Steps 490(499.94) | Grad Norm 1.0670(3.8454) | Total Time 10.00(10.00)
Iter 3680 | Time 4.8772(4.5607) | Bit/dim 1.1205(1.1321) | Steps 520(501.09) | Grad Norm 16.3575(4.8386) | Total Time 10.00(10.00)
Iter 3690 | Time 4.6311(4.5710) | Bit/dim 1.2079(1.1711) | Steps 514(502.98) | Grad Norm 7.1292(7.3711) | Total Time 10.00(10.00)
Iter 3700 | Time 4.5330(4.5928) | Bit/dim 1.1871(1.1826) | Steps 490(503.28) | Grad Norm 8.7040(8.3740) | Total Time 10.00(10.00)
Iter 3710 | Time 4.0835(4.5751) | Bit/dim 1.2236(1.1886) | Steps 490(502.58) | Grad Norm 13.9450(10.5789) | Total Time 10.00(10.00)
Iter 3720 | Time 5.1468(4.5767) | Bit/dim 1.2211(1.1979) | Steps 550(505.60) | Grad Norm 28.6873(11.9532) | Total Time 10.00(10.00)
Iter 3730 | Time 4.9064(4.5573) | Bit/dim 1.1735(1.1942) | Steps 532(506.63) | Grad Norm 26.3433(11.8914) | Total Time 10.00(10.00)
Iter 3740 | Time 4.7536(4.5676) | Bit/dim 1.1668(1.1850) | Steps 508(506.48) | Grad Norm 7.6668(11.1496) | Total Time 10.00(10.00)
Iter 3750 | Time 4.3555(4.5478) | Bit/dim 1.1605(1.1756) | Steps 496(505.19) | Grad Norm 6.5757(10.0464) | Total Time 10.00(10.00)
Iter 3760 | Time 4.5483(4.5602) | Bit/dim 1.1367(1.1662) | Steps 502(505.20) | Grad Norm 5.3963(9.3050) | Total Time 10.00(10.00)
Iter 3770 | Time 4.8864(4.5859) | Bit/dim 1.1473(1.1582) | Steps 520(505.78) | Grad Norm 14.7471(9.1445) | Total Time 10.00(10.00)
Iter 3780 | Time 4.7297(4.6061) | Bit/dim 1.1595(1.1540) | Steps 502(506.07) | Grad Norm 8.7886(9.6548) | Total Time 10.00(10.00)
Iter 3790 | Time 4.5565(4.6176) | Bit/dim 1.1273(1.1496) | Steps 502(506.57) | Grad Norm 4.8090(9.6781) | Total Time 10.00(10.00)
Iter 3800 | Time 4.0718(4.5889) | Bit/dim 1.2016(1.1528) | Steps 484(505.13) | Grad Norm 12.8321(10.6219) | Total Time 10.00(10.00)
Iter 3810 | Time 4.0620(4.5514) | Bit/dim 1.1874(1.1663) | Steps 466(502.92) | Grad Norm 9.7551(11.7386) | Total Time 10.00(10.00)
Iter 3820 | Time 4.4744(4.5719) | Bit/dim 1.1116(1.1612) | Steps 496(504.28) | Grad Norm 5.9478(11.1647) | Total Time 10.00(10.00)
Iter 3830 | Time 4.6968(4.5964) | Bit/dim 1.1174(1.1565) | Steps 496(504.98) | Grad Norm 7.3388(10.7462) | Total Time 10.00(10.00)
