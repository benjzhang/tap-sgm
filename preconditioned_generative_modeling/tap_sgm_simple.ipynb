{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transport-accelerated preconditioners for score-based generative modeling with SDEs (Simple examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import lib.toy_data as toy_data\n",
    "import numpy as np\n",
    "\n",
    "import lib.layers as layers\n",
    "\n",
    "import argparse\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--finalT'], dest='finalT', nargs=None, const=None, default=0.5, type=<class 'float'>, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('TAP-SGM')\n",
    "parser.add_argument(\n",
    "    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],\n",
    "    type=str, default='moons'\n",
    ")\n",
    "\n",
    "\n",
    "# parser.add_argument('--depth', help='number of coupling layers', type=int, default=8)\n",
    "parser.add_argument('--niters', type=int, default=100001)\n",
    "parser.add_argument('--batch_size', type=int, default=256)\n",
    "parser.add_argument('--lr', type=float, default=1e-3)\n",
    "parser.add_argument('--save', type=str, default='experiments/cnf')\n",
    "parser.add_argument('--tint',type = float, default = 0.02)\n",
    "parser.add_argument('--finalT',type = float, default = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args('')\n",
    "\n",
    "learning_rate = args.lr  # learning rate for training neural network\n",
    "batch_size = args.batch_size  # batch size during training of neural network\n",
    "epochs = args.niters   # Number of training epochs for the neural network\n",
    "tint = args.tint #  put more points in the initial training\n",
    "T = args.finalT    # Forward simulation time in the forward SDE\n",
    "dataset = args.data # Dataset choice, see toy_data for full options of toy datasets ('checkerboard','8gaussians','2spirals','swissroll','moons',etc.)\n",
    "saveloc = args.save # save folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preconditioning map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_model():\n",
    "\n",
    "    if preconditionerargs.nf:\n",
    "        chain = []\n",
    "        for i in range(preconditionerargs.depth):\n",
    "            chain.append(layers.PlanarFlow(2))\n",
    "        return layers.SequentialFlow(chain)\n",
    "    else:\n",
    "        chain = []\n",
    "        for i in range(preconditionerargs.depth):\n",
    "            if preconditionerargs.glow: chain.append(layers.BruteForceLayer(2))\n",
    "            chain.append(layers.CouplingLayer(2, swap=i % 2 == 0))\n",
    "        return layers.SequentialFlow(chain)\n",
    "    \n",
    "checkpointlocation = 'experiments/cnf/'+dataset+'_depth3/checkpt.pth' \n",
    "checkpoint = torch.load(checkpointlocation)\n",
    "preconditionerargs = checkpoint['args']\n",
    "model = construct_model()\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initialize the neural net that models the score function. A key part of this is that we train adaptive Fourier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bespoke activation function\n",
    "class cosineact(nn.Module):\n",
    "    def __init__(self,weights = 1):\n",
    "        super().__init__()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self,input):\n",
    "        evaluation = torch.cos(self.weights * input)\n",
    "        return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenoisingModel(\n",
      "  (fc1): Linear(in_features=3, out_features=32, bias=True)\n",
      "  (activation1): cosineact()\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (activation2): GELU(approximate='none')\n",
      "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (activation3): GELU(approximate='none')\n",
      "  (fc4): Linear(in_features=3, out_features=32, bias=True)\n",
      "  (activation4): GELU(approximate='none')\n",
      "  (fc5): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (activation5): GELU(approximate='none')\n",
      "  (fc6): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (activation6): GELU(approximate='none')\n",
      "  (fc7): Linear(in_features=3, out_features=32, bias=True)\n",
      "  (activation7): GELU(approximate='none')\n",
      "  (fc8): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (activation8): GELU(approximate='none')\n",
      "  (fc9): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Score Model construction\n",
    "\n",
    "class DenoisingModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_units=32):\n",
    "        super(DenoisingModel, self).__init__()\n",
    "        # hidden_units = 32\n",
    "        \n",
    "        # data and timestep\n",
    "        self.fc1 = nn.Linear(3, int(hidden_units), bias=True)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        self.activation1 = cosineact()\n",
    "        self.fc2 = nn.Linear(int(hidden_units), int(hidden_units), bias=True)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        self.activation2 = nn.GELU()\n",
    "        self.fc3 = nn.Linear(int(hidden_units), 3, bias=True)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        self.activation3 = nn.GELU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(3, int(hidden_units), bias=True)\n",
    "        nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        self.activation4 = nn.GELU()\n",
    "        self.fc5 = nn.Linear(int(hidden_units), int(hidden_units), bias=True)\n",
    "        nn.init.xavier_uniform_(self.fc5.weight)\n",
    "        self.activation5 = nn.GELU()\n",
    "        self.fc6 = nn.Linear(int(hidden_units), 3, bias=True)\n",
    "        nn.init.xavier_uniform_(self.fc6.weight)\n",
    "        self.activation6 = nn.GELU()\n",
    "        \n",
    "        self.fc7 = nn.Linear(3, int(hidden_units), bias=True)\n",
    "        nn.init.xavier_uniform_(self.fc7.weight)\n",
    "        self.activation7 = nn.GELU()\n",
    "        self.fc8 = nn.Linear(int(hidden_units), int(hidden_units), bias=True)\n",
    "        nn.init.xavier_uniform_(self.fc8.weight)\n",
    "        self.activation8 = nn.GELU()\n",
    "        self.fc9 = nn.Linear(int(hidden_units), 2, bias=True)\n",
    "        nn.init.xavier_uniform_(self.fc9.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.activation4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.activation5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.activation6(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.activation7(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.activation8(x)\n",
    "        x = self.fc9(x)\n",
    "        \n",
    "        return x\n",
    "      \n",
    "scorenet = DenoisingModel()\n",
    "print(scorenet)\n",
    "optimizer = optim.Adam(scorenet.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss functions. These loss functions assume that the forward process is a standard OU process dx = -x/2 dt + dW. The choice of \\lambda(t) in the SGM objective function is equal to 1 (the constant in front of the dW term). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -- we use the denoising diffusions objective function\n",
    "# Scorenet is the score model, samples are the training samples, Tmin/Tmax are the time interval that is being trained on, and eps is so that Tmin is not sampled. \n",
    "\n",
    "def time_dsm_score_estimator(scorenet,samples,Tmin,Tmax,eps):\n",
    "\n",
    "    t = torch.rand(samples.shape[0]) * (Tmax - Tmin - eps) + eps + Tmin # sample uniformly from time interval\n",
    "\n",
    "    # Add noise to the training samples\n",
    "    sigmas = torch.sqrt(1 - torch.exp(-t))\n",
    "    sigmas = sigmas.view(samples.shape[0],*([1]*len(samples.shape[1:])))\n",
    "    noise = torch.randn_like(samples) * sigmas\n",
    "    tenlarge = t.repeat(2,1).T\n",
    "    perturbed_samples = samples * torch.exp(-0.5 * tenlarge) + noise\n",
    "\n",
    "    # Evaluate score and marginal score on the perturbed samples\n",
    "    target = - 1/ (sigmas ** 2) * (noise)\n",
    "    score_eval_samples = torch.cat((t.reshape(-1,1),perturbed_samples),1)\n",
    "    scores = scorenet(score_eval_samples)\n",
    "\n",
    "    # Evaluate the loss function \n",
    "    target = target.view(target.shape[0],-1)\n",
    "    scores = scores.view(scores.shape[0],-1)\n",
    "    loss = 0.5 * ((scores-target) ** 2).sum(dim = -1) \n",
    "\n",
    "    return loss.mean(dim = 0)\n",
    "\n",
    "\n",
    "# Loss function\n",
    "# This is for if you have a specific mesh for the time interval you would like the network to train on. \n",
    "def deterministic_time_dsm_score_estimator(scorenet,samples,t):\n",
    "\n",
    "    loss = 0\n",
    "    for ii in range(len(t)-1):\n",
    "\n",
    "        # Add noise to the training samples\n",
    "        sigmas = torch.sqrt(1 - torch.exp(-t[ii]))\n",
    "        noise = torch.randn_like(samples) * sigmas\n",
    "        perturbed_samples = samples * torch.exp(-0.5 * t[ii]) + noise\n",
    "\n",
    "        # Evaluate score and marginal score on perturbed samples\n",
    "        target = - 1/ (sigmas ** 2) * (noise)\n",
    "        score_eval_samples = torch.cat((t[ii].repeat(perturbed_samples.shape[0],1),perturbed_samples),1)\n",
    "        scores = scorenet(score_eval_samples)\n",
    "\n",
    "        # Evaluate loss function at this particular t[ii]\n",
    "        target = target.view(target.shape[0],-1)\n",
    "        scores = scores.view(scores.shape[0],-1)\n",
    "        loss_vec = 0.5 * ((scores-target) ** 2).sum(dim = -1) \n",
    "        loss = loss + (t[ii+1]-t[ii])*loss_vec.mean(dim = 0)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the score network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'randn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Training the score network\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      4\u001b[0m     \u001b[39m# sample toy_data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     p_samples \u001b[39m=\u001b[39m toy_data\u001b[39m.\u001b[39;49minf_train_gen(dataset, batch_size)\n\u001b[1;32m      6\u001b[0m     samples \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(p_samples)\u001b[39m.\u001b[39mto(dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m      8\u001b[0m     half \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(batch_size\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/gitrepos/tap-sgm/preconditioned_generative_modeling/lib/toy_data.py:69\u001b[0m, in \u001b[0;36minf_train_gen\u001b[0;34m(data, rng, batch_size)\u001b[0m\n\u001b[1;32m     67\u001b[0m dataset \u001b[39m=\u001b[39m []\n\u001b[1;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[0;32m---> 69\u001b[0m     point \u001b[39m=\u001b[39m rng\u001b[39m.\u001b[39;49mrandn(\u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[1;32m     70\u001b[0m     idx \u001b[39m=\u001b[39m rng\u001b[39m.\u001b[39mrandint(\u001b[39m8\u001b[39m)\n\u001b[1;32m     71\u001b[0m     center \u001b[39m=\u001b[39m centers[idx]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'randn'"
     ]
    }
   ],
   "source": [
    "# Training the score network\n",
    "\n",
    "for step in range(epochs):\n",
    "    # sample toy_data\n",
    "    p_samples = toy_data.inf_train_gen(dataset, batch_size = batch_size)\n",
    "    samples = torch.tensor(p_samples).to(dtype = torch.float32)\n",
    "\n",
    "    half = int(batch_size/2)\n",
    "    samples1 = samples[0:half,:]\n",
    "    samples2 = samples[half:-1,:]\n",
    "    # evaluate loss function and gradient\n",
    "    # loss = time_dsm_score_estimator(scorenet,samples,0,T,eps = 0.0005)\n",
    "    loss = time_dsm_score_estimator(scorenet,samples1,0,tint,0.0001) + time_dsm_score_estimator(scorenet,samples2,tint,T,0.0001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update score network\n",
    "    optimizer.step()\n",
    "\n",
    "    if not step%100:\n",
    "        print(loss,step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SDE simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the solving the OU process exactly given deterministic initial conditions\n",
    "def ou_dynamics(init, T):\n",
    "    init = init * torch.exp(- 0.5 * T) + torch.sqrt(1-torch.exp(-T)) * torch.randn_like(init)\n",
    "    return init\n",
    "\n",
    "def reverse_sde(score, init,T,lr=0.001):\n",
    "    step = int(T/lr) \n",
    "    for i in range(step,-1,-1):\n",
    "        current_lr = lr\n",
    "        evalpoint = torch.cat(((torch.tensor(lr*i)).repeat(init.shape[0],1),init),1)\n",
    "        init = init + current_lr  * (init/2 + score(evalpoint).detach() )\n",
    "        init = init + torch.randn_like(init) * np.sqrt(current_lr)\n",
    "    return init\n",
    "\n",
    "\n",
    "# The following is the deterministic ODE flow that can also sample from the target distribution\n",
    "\n",
    "def reverse_ode_flow(score,init,T,lr = 0.001):\n",
    "    step = int(T/lr)\n",
    "    for i in range(step,-1,-1):\n",
    "        current_lr = lr\n",
    "        evalpoint = torch.cat(((torch.tensor(lr*i)).repeat(init.shape[0],1),init),1)\n",
    "        init = init + current_lr  * (init/2 + 1/2 * score(evalpoint).detach() )\n",
    "    return init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample using the score network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising the normal distribution \n",
    "samples_lang = torch.randn(10000, 2) # * (right_bound - left_bound) + left_bound\n",
    "samples_lang = reverse_sde(scorenet, samples_lang,torch.tensor(T)).detach().numpy()\n",
    "\n",
    "# Denoising samples from the training data\n",
    "samples = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = 10000))\n",
    "samples_lang_noisedtraining = samples * torch.exp(-0.5 * torch.tensor(T)) + torch.sqrt(1-torch.exp(-torch.tensor(T))) * torch.randn_like(samples)\n",
    "samples_lang_noisedtraining =reverse_sde(scorenet, samples_lang_noisedtraining.to(dtype=torch.float32),torch.tensor(T)).detach().numpy()\n",
    "\n",
    "# Deterministically evolving the normal distribution \n",
    "samples_deterministic = torch.randn(10000,2)\n",
    "samples_deterministic = reverse_ode_flow(scorenet,samples_deterministic,torch.tensor(T)).detach().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postprocessing with the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_lang_target = model(torch.tensor(samples_lang),None,reverse = True).detach().numpy()\n",
    "samples_lang_noisedtraining_target = model(torch.tensor(samples_lang_noisedtraining),None,reverse = True).detach().numpy()\n",
    "samples_deterministic_target = model(torch.tensor(samples_deterministic),None,reverse = True).detach().numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make and save plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ou_dynamics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m plt\u001b[39m.\u001b[39mclf()\n\u001b[1;32m     41\u001b[0m pushforward_samples_true \u001b[39m=\u001b[39m model(samples_true,\u001b[39mNone\u001b[39;00m,reverse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m---> 42\u001b[0m pushforward_samples_atT \u001b[39m=\u001b[39m ou_dynamics(torch\u001b[39m.\u001b[39mtensor(pushforward_samples_true),torch\u001b[39m.\u001b[39mtensor(T))\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     43\u001b[0m plt\u001b[39m.\u001b[39mscatter(pushforward_samples_atT[:,\u001b[39m0\u001b[39m],pushforward_samples_atT[:,\u001b[39m1\u001b[39m],s \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m)\n\u001b[1;32m     44\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39msquare\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ou_dynamics' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "savedir = saveloc + '/' + dataset + '_depth3_tapsgm/'\n",
    "\n",
    "# Check whether the specified path exists or not\n",
    "isExist = os.path.exists(savedir)\n",
    "if not isExist:\n",
    "\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(savedir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "p_samples = toy_data.inf_train_gen(dataset, batch_size = 10000)\n",
    "samples_true = torch.tensor(p_samples).to(dtype = torch.float32)\n",
    "plt.scatter(samples_true[:,0],samples_true[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('True samples')\n",
    "\n",
    "\n",
    "savename = savedir + 'true_samples.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "pushforward_samples_true = model(samples_true,None,reverse = False).detach().numpy()\n",
    "plt.scatter(pushforward_samples_true[:,0],pushforward_samples_true[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('True pushforward samples')\n",
    "\n",
    "savename = savedir + 'true_pushforward.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "pushforward_samples_true = model(samples_true,None,reverse = False).detach().numpy()\n",
    "pushforward_samples_atT = ou_dynamics(torch.tensor(pushforward_samples_true),torch.tensor(T)).detach().numpy()\n",
    "plt.scatter(pushforward_samples_atT[:,0],pushforward_samples_atT[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('True pushforward samples at T')\n",
    "\n",
    "savename = savedir + 'true_pushforward_atT.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(samples_lang[:,0],samples_lang[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('Reverse SDE pushforward')\n",
    "savename = savedir + 'reversesde_pushforward.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(samples_lang_target[:,0],samples_lang_target[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('Denoising normal distribution')\n",
    "savename = savedir + 'reversesde_target.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(samples_lang_noisedtraining[:,0],samples_lang_noisedtraining[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('Denoising noised training samples')\n",
    "savename = savedir + 'reversesde_noisedtraining_target.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(samples_deterministic_target[:,0],samples_deterministic_target[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('Denoising normal with ODE flow')\n",
    "savename = savedir + 'deterministic_ode_target.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "\n",
    "samples_nf = model(torch.randn(1000,2),None,reverse = True).detach().numpy()\n",
    "plt.clf()\n",
    "plt.scatter(samples_nf[:,0],samples_nf[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('Map only')\n",
    "savename = savedir + 'map_only.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "savename = savedir + 'scorenet'\n",
    "torch.save(scorenet,savename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "particlesystems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcff6d82c76e93f8bf1a2a5a5959262afe2e0fbcc55797eb73e33c922868f9ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
