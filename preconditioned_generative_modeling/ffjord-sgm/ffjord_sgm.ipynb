{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transport-accelerated preconditioners for score-based generative modeling with SDEs (TAP-SGM with FFJORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import lib.toy_data as toy_data\n",
    "import numpy as np\n",
    "\n",
    "import lib.layers as layers\n",
    "\n",
    "import argparse\n",
    "from train_misc import build_model_tabular\n",
    "from train_misc import create_regularization_fns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--gpu'], dest='gpu', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('FFJORD-TAP-SGM')\n",
    "parser.add_argument(\n",
    "    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],\n",
    "    type=str, default='moons'\n",
    ")\n",
    "\n",
    "\n",
    "parser.add_argument('--depth', help='number of hidden layers of score network', type=int, default=7)\n",
    "parser.add_argument('--dims', type = int,default = 64)\n",
    "parser.add_argument('--hiddenunits',type = int, default = 32)\n",
    "parser.add_argument('--niters', type=int, default=100001)\n",
    "parser.add_argument('--batch_size', type=int, default=256)\n",
    "parser.add_argument('--lr', type=float, default=1e-3)\n",
    "parser.add_argument('--save', type=str, default='experiments/ffjord')\n",
    "parser.add_argument('--tint',type = float, default = 0.05)\n",
    "parser.add_argument('--finalT',type = float, default = 0.5)\n",
    "parser.add_argument('--rff',type = bool, default = False)\n",
    "parser.add_argument('--dt',type = float,default = 0.001)\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args('')\n",
    "\n",
    "\n",
    "learning_rate = args.lr  # learning rate for training neural network\n",
    "batch_size = args.batch_size  # batch size during training of neural network\n",
    "epochs = args.niters   # Number of training epochs for the neural network\n",
    "tint = args.tint #  put more points in the initial training\n",
    "T = args.finalT    # Forward simulation time in the forward SDE\n",
    "dataset = args.data # Dataset choice, see toy_data for full options of toy datasets ('checkerboard','8gaussians','2spirals','swissroll','moons',etc.)\n",
    "saveloc = args.save # save folder\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preconditioning map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_location = '../../ffjord-master/experiments/ffjord/'+dataset + str(args.dims)+'/checkpt.pth'\n",
    "checkpoint = torch.load(model_location)\n",
    "preconditionerargs = checkpoint['args']\n",
    "device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "regularization_fns, regularization_coeffs = create_regularization_fns(preconditionerargs)\n",
    "model = build_model_tabular(preconditionerargs, 2, regularization_fns)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initialize the neural net that models the score function. A key part of this is that we train adaptive Fourier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bespoke activation function\n",
    "class cosineact(nn.Module):\n",
    "    def __init__(self,weights = 1):\n",
    "        super().__init__()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self,input):\n",
    "        evaluation = torch.cos(self.weights * input)\n",
    "        return evaluation\n",
    "\n",
    "\n",
    "# Random Fourier features\n",
    "if args.rff:\n",
    "    B = 2 * torch.pi * torch.randn(2,int(args.hiddenunits))\n",
    "    c = 2 * torch.pi * torch.rand(1,int(args.hiddenunits))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=33, out_features=32, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (3): GELU(approximate='none')\n",
      "  (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (5): GELU(approximate='none')\n",
      "  (6): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Score Model construction\n",
    "\n",
    "\n",
    "def construct_score_model(depth,hidden_units):\n",
    "    chain = []\n",
    "    chain.append(nn.Linear(int(2)+1,int(hidden_units),bias =True))\n",
    "    chain.append(nn.GELU())\n",
    "\n",
    "    for ii in range(depth-1):\n",
    "        chain.append(nn.Linear(int(hidden_units),int(hidden_units),bias = True))\n",
    "        chain.append(nn.GELU())\n",
    "    chain.append(nn.Linear(int(hidden_units),2,bias = True))    \n",
    "\n",
    "    return nn.Sequential(*chain)\n",
    "\n",
    "\n",
    "def construct_FF_score_model(depth,hidden_units):\n",
    "    chain = []\n",
    "    chain.append(nn.Linear(int(hidden_units)+1,int(hidden_units),bias =True))\n",
    "    chain.append(nn.GELU())\n",
    "\n",
    "    for ii in range(depth-1):\n",
    "        chain.append(nn.Linear(int(hidden_units),int(hidden_units),bias = True))\n",
    "        chain.append(nn.GELU())\n",
    "    chain.append(nn.Linear(int(hidden_units),2,bias = True))    \n",
    "\n",
    "    return nn.Sequential(*chain)\n",
    "\n",
    "args.rff = True\n",
    "args.depth = 3\n",
    "if args.rff:\n",
    "    scorenet = construct_FF_score_model(args.depth,args.hiddenunits)\n",
    "else:\n",
    "    scorenet = construct_score_model(args.depth,args.hiddenunits)\n",
    "\n",
    "\n",
    "print(scorenet)\n",
    "optimizer = optim.Adam(scorenet.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss functions. These loss functions assume that the forward process is a standard OU process dx = -x/2 dt + dW. The choice of \\lambda(t) in the SGM objective function is equal to 1 (the constant in front of the dW term). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -- we use the denoising diffusions objective function\n",
    "# Scorenet is the score model, samples are the training samples, Tmin/Tmax are the time interval that is being trained on, and eps is so that Tmin is not sampled. \n",
    "\n",
    "def time_dsm_score_estimator(scorenet,samples,Tmin,Tmax,eps):\n",
    "\n",
    "    t = torch.rand(samples.shape[0]) * (Tmax - Tmin - eps) + eps + Tmin # sample uniformly from time interval\n",
    "\n",
    "    # Add noise to the training samples\n",
    "    sigmas = torch.sqrt(1 - torch.exp(-t))\n",
    "    sigmas = sigmas.view(samples.shape[0],*([1]*len(samples.shape[1:])))\n",
    "    noise = torch.randn_like(samples) * sigmas\n",
    "    tenlarge = t.repeat(2,1).T\n",
    "    perturbed_samples = samples * torch.exp(-0.5 * tenlarge) + noise\n",
    "\n",
    "    # Evaluate score and marginal score on the perturbed samples\n",
    "    target = - 1/ (sigmas ** 2) * (noise)\n",
    "    score_eval_samples = torch.cat((t.reshape(-1,1),perturbed_samples),1)\n",
    "    scores = -perturbed_samples + scorenet(score_eval_samples)\n",
    "\n",
    "    # Evaluate the loss function \n",
    "    target = target.view(target.shape[0],-1)\n",
    "    scores = scores.view(scores.shape[0],-1)\n",
    "    loss = 0.5 * ((scores-target) ** 2).sum(dim = -1) \n",
    "\n",
    "    return loss.mean(dim = 0)\n",
    "\n",
    "\n",
    "def FF_time_dsm_score_estimator(scorenet,samples,Tmin,Tmax,eps,B,c):\n",
    "\n",
    "    t = torch.rand(samples.shape[0]) * (Tmax - Tmin - eps) + eps + Tmin\n",
    "\n",
    "\n",
    "    sigmas = torch.sqrt(1 - torch.exp(-t))\n",
    "    sigmas = sigmas.view(samples.shape[0],*([1]*len(samples.shape[1:])))\n",
    "    noise = torch.randn_like(samples) * sigmas\n",
    "    tenlarge = t.repeat(2,1).T\n",
    "    perturbed_samples = samples * torch.exp(-0.5 * tenlarge) + noise\n",
    "    target = - 1/ (sigmas ** 2) * (noise)\n",
    "\n",
    "    # checkbounds = (torch.abs(samples[:,0])<3).type(torch.uint8) * (torch.abs(samples[:,1])<3)\n",
    "\n",
    "    # FFperturbed_samples = torch.multiply( checkbounds.reshape(-1,1) ,(torch.matmul(perturbed_samples,B) + c))\n",
    "    FFperturbed_samples = torch.matmul(perturbed_samples,B) + c\n",
    "    # perturbed_samples = torch.cat( (torch.cos(FFperturbed_samples), torch.sin(FFperturbed_samples)),1 )\n",
    "\n",
    "    FFperturbed_samples = torch.cos(FFperturbed_samples)\n",
    "\n",
    "    score_eval_samples = torch.cat((t.reshape(-1,1),FFperturbed_samples),1)\n",
    "    scores = -perturbed_samples + scorenet(score_eval_samples)\n",
    "\n",
    "    target = target.view(target.shape[0],-1)\n",
    "    scores = scores.view(scores.shape[0],-1)\n",
    "    loss = 0.5 * ((scores-target) ** 2).sum(dim = -1) \n",
    "\n",
    "    return loss.mean(dim = 0)\n",
    "\n",
    "\n",
    "# Loss function\n",
    "# This is for if you have a specific mesh for the time interval you would like the network to train on. \n",
    "def deterministic_time_dsm_score_estimator(scorenet,samples,t):\n",
    "\n",
    "    loss = 0\n",
    "    for ii in range(len(t)-1):\n",
    "\n",
    "        # Add noise to the training samples\n",
    "        sigmas = torch.sqrt(1 - torch.exp(-t[ii]))\n",
    "        noise = torch.randn_like(samples) * sigmas\n",
    "        perturbed_samples = samples * torch.exp(-0.5 * t[ii]) + noise\n",
    "\n",
    "        # Evaluate score and marginal score on perturbed samples\n",
    "        target = - 1/ (sigmas ** 2) * (noise)\n",
    "        score_eval_samples = torch.cat((t[ii].repeat(perturbed_samples.shape[0],1),perturbed_samples),1)\n",
    "        scores = scorenet(score_eval_samples)\n",
    "\n",
    "        # Evaluate loss function at this particular t[ii]\n",
    "        target = target.view(target.shape[0],-1)\n",
    "        scores = scores.view(scores.shape[0],-1)\n",
    "        loss_vec = 0.5 * ((scores-target) ** 2).sum(dim = -1) \n",
    "        loss = loss + (t[ii+1]-t[ii])*loss_vec.mean(dim = 0)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the score network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bjzhang/gitrepos/tap-sgm/preconditioned_generative_modeling/ffjord-sgm/lib/layers/odefunc.py:288: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m p_samples \u001b[39m=\u001b[39m toy_data\u001b[39m.\u001b[39minf_train_gen(dataset, batch_size \u001b[39m=\u001b[39m\u001b[39m1000000\u001b[39m)\n\u001b[1;32m      3\u001b[0m training_samples \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(p_samples)\u001b[39m.\u001b[39mto(dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m----> 4\u001b[0m training_samples \u001b[39m=\u001b[39m model(training_samples,\u001b[39mNone\u001b[39;49;00m,reverse \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/gitrepos/tap-sgm/preconditioned_generative_modeling/ffjord-sgm/lib/layers/container.py:21\u001b[0m, in \u001b[0;36mSequentialFlow.forward\u001b[0;34m(self, x, logpx, reverse, inds)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m logpx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m inds:\n\u001b[0;32m---> 21\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchain[i](x, reverse\u001b[39m=\u001b[39;49mreverse)\n\u001b[1;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m     23\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/gitrepos/tap-sgm/preconditioned_generative_modeling/ffjord-sgm/lib/layers/cnf.py:53\u001b[0m, in \u001b[0;36mCNF.forward\u001b[0;34m(self, z, logpz, integration_times, reverse)\u001b[0m\n\u001b[1;32m     50\u001b[0m reg_states \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(torch\u001b[39m.\u001b[39mtensor(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(z) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnreg))\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m---> 53\u001b[0m     state_t \u001b[39m=\u001b[39m odeint(\n\u001b[1;32m     54\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49modefunc,\n\u001b[1;32m     55\u001b[0m         (z, _logpz) \u001b[39m+\u001b[39;49m reg_states,\n\u001b[1;32m     56\u001b[0m         integration_times\u001b[39m.\u001b[39;49mto(z),\n\u001b[1;32m     57\u001b[0m         atol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matol,\n\u001b[1;32m     58\u001b[0m         rtol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrtol,\n\u001b[1;32m     59\u001b[0m         method\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msolver,\n\u001b[1;32m     60\u001b[0m         options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msolver_options,\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     state_t \u001b[39m=\u001b[39m odeint(\n\u001b[1;32m     64\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39modefunc,\n\u001b[1;32m     65\u001b[0m         (z, _logpz),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_solver,\n\u001b[1;32m     70\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torchdiffeq/_impl/adjoint.py:198\u001b[0m, in \u001b[0;36modeint_adjoint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, adjoint_params)\u001b[0m\n\u001b[1;32m    195\u001b[0m state_norm \u001b[39m=\u001b[39m options[\u001b[39m\"\u001b[39m\u001b[39mnorm\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    196\u001b[0m handle_adjoint_norm_(adjoint_options, shapes, state_norm)\n\u001b[0;32m--> 198\u001b[0m ans \u001b[39m=\u001b[39m OdeintAdjointMethod\u001b[39m.\u001b[39;49mapply(shapes, func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol,\n\u001b[1;32m    199\u001b[0m                                 adjoint_method, adjoint_options, t\u001b[39m.\u001b[39;49mrequires_grad, \u001b[39m*\u001b[39;49madjoint_params)\n\u001b[1;32m    201\u001b[0m \u001b[39mif\u001b[39;00m event_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     solution \u001b[39m=\u001b[39m ans\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torchdiffeq/_impl/adjoint.py:25\u001b[0m, in \u001b[0;36mOdeintAdjointMethod.forward\u001b[0;34m(ctx, shapes, func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, t_requires_grad, *adjoint_params)\u001b[0m\n\u001b[1;32m     22\u001b[0m ctx\u001b[39m.\u001b[39mevent_mode \u001b[39m=\u001b[39m event_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m     ans \u001b[39m=\u001b[39m odeint(func, y0, t, rtol\u001b[39m=\u001b[39;49mrtol, atol\u001b[39m=\u001b[39;49matol, method\u001b[39m=\u001b[39;49mmethod, options\u001b[39m=\u001b[39;49moptions, event_fn\u001b[39m=\u001b[39;49mevent_fn)\n\u001b[1;32m     27\u001b[0m     \u001b[39mif\u001b[39;00m event_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         y \u001b[39m=\u001b[39m ans\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torchdiffeq/_impl/odeint.py:77\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     74\u001b[0m solver \u001b[39m=\u001b[39m SOLVERS[method](func\u001b[39m=\u001b[39mfunc, y0\u001b[39m=\u001b[39my0, rtol\u001b[39m=\u001b[39mrtol, atol\u001b[39m=\u001b[39matol, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m event_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     solution \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39;49mintegrate(t)\n\u001b[1;32m     78\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     event_t, solution \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39mintegrate_until_event(t[\u001b[39m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torchdiffeq/_impl/solvers.py:28\u001b[0m, in \u001b[0;36mAdaptiveStepsizeODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     26\u001b[0m solution[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my0\n\u001b[1;32m     27\u001b[0m t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m---> 28\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_before_integrate(t)\n\u001b[1;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(t)):\n\u001b[1;32m     30\u001b[0m     solution[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_advance(t[i])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torchdiffeq/_impl/rk_common.py:163\u001b[0m, in \u001b[0;36mRKAdaptiveStepsizeODESolver._before_integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    161\u001b[0m f0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(t[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my0)\n\u001b[1;32m    162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_step \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     first_step \u001b[39m=\u001b[39m _select_initial_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, t[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my0, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morder \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrtol, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matol,\n\u001b[1;32m    164\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm, f0\u001b[39m=\u001b[39;49mf0)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     first_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_step\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py:62\u001b[0m, in \u001b[0;36m_select_initial_step\u001b[0;34m(func, t0, y0, order, rtol, atol, norm, f0)\u001b[0m\n\u001b[1;32m     59\u001b[0m     h0 \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m \u001b[39m*\u001b[39m d0 \u001b[39m/\u001b[39m d1\n\u001b[1;32m     61\u001b[0m y1 \u001b[39m=\u001b[39m y0 \u001b[39m+\u001b[39m h0 \u001b[39m*\u001b[39m f0\n\u001b[0;32m---> 62\u001b[0m f1 \u001b[39m=\u001b[39m func(t0 \u001b[39m+\u001b[39;49m h0, y1)\n\u001b[1;32m     64\u001b[0m d2 \u001b[39m=\u001b[39m norm((f1 \u001b[39m-\u001b[39m f0) \u001b[39m/\u001b[39m scale) \u001b[39m/\u001b[39m h0\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m d1 \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1e-15\u001b[39m \u001b[39mand\u001b[39;00m d2 \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1e-15\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py:189\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39m# Do nothing.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_func(t, y)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py:189\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39m# Do nothing.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_func(t, y)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py:138\u001b[0m, in \u001b[0;36m_TupleFunc.forward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, t, y):\n\u001b[0;32m--> 138\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_func(t, _flat_to_shape(y, (), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshapes))\n\u001b[1;32m    139\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat([f_\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m f_ \u001b[39min\u001b[39;00m f])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/gitrepos/tap-sgm/preconditioned_generative_modeling/ffjord-sgm/lib/layers/wrappers/cnf_regularization.py:22\u001b[0m, in \u001b[0;36mRegularizedODEfunc.forward\u001b[0;34m(self, t, state)\u001b[0m\n\u001b[1;32m     20\u001b[0m x\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m logp\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 22\u001b[0m dstate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49modefunc(t, (x, logp))\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(state) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m     24\u001b[0m     dx, dlogp \u001b[39m=\u001b[39m dstate[:\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/gitrepos/tap-sgm/preconditioned_generative_modeling/ffjord-sgm/lib/layers/odefunc.py:308\u001b[0m, in \u001b[0;36mODEfunc.forward\u001b[0;34m(self, t, states)\u001b[0m\n\u001b[1;32m    306\u001b[0m         divergence \u001b[39m=\u001b[39m divergence_bf(dy, y)\u001b[39m.\u001b[39mview(batchsize, \u001b[39m1\u001b[39m)\n\u001b[1;32m    307\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m         divergence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdivergence_fn(dy, y, e\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_e)\u001b[39m.\u001b[39mview(batchsize, \u001b[39m1\u001b[39m)\n\u001b[1;32m    309\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresidual:\n\u001b[1;32m    310\u001b[0m     dy \u001b[39m=\u001b[39m dy \u001b[39m-\u001b[39m y\n",
      "File \u001b[0;32m~/gitrepos/tap-sgm/preconditioned_generative_modeling/ffjord-sgm/lib/layers/odefunc.py:16\u001b[0m, in \u001b[0;36mdivergence_bf\u001b[0;34m(dx, y, **unused_kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m sum_diag \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[0;32m---> 16\u001b[0m     sum_diag \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(dx[:, i]\u001b[39m.\u001b[39;49msum(), y, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcontiguous()[:, i]\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m sum_diag\u001b[39m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torch/autograd/__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    301\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    302\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Generate training data\n",
    "p_samples = toy_data.inf_train_gen(dataset, batch_size =1000000)\n",
    "training_samples = torch.tensor(p_samples).to(dtype = torch.float32)\n",
    "training_samples = model(training_samples,None,reverse = False).detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(70.5844, grad_fn=<AddBackward0>) 0\n",
      "tensor(140.7869, grad_fn=<AddBackward0>) 100\n",
      "tensor(159.3622, grad_fn=<AddBackward0>) 200\n",
      "tensor(147.2463, grad_fn=<AddBackward0>) 300\n",
      "tensor(174.9357, grad_fn=<AddBackward0>) 400\n",
      "tensor(172.3721, grad_fn=<AddBackward0>) 500\n",
      "tensor(93.2151, grad_fn=<AddBackward0>) 600\n",
      "tensor(143.0519, grad_fn=<AddBackward0>) 700\n",
      "tensor(116.6231, grad_fn=<AddBackward0>) 800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# loss = FF_time_dsm_score_estimator(scorenet,samples1,0,tint,0.001,B,c) + FF_time_dsm_score_estimator(scorenet,samples2,tint,T,0.001,B,c)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 37\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     39\u001b[0m \u001b[39m# Update score network\u001b[39;00m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/particlesystems/lib/python3.9/site-packages/torch/_tensor.py:428\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[39m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_tensor_str\u001b[39m.\u001b[39m_str(\u001b[39mself\u001b[39m, tensor_contents\u001b[39m=\u001b[39mtensor_contents)\n\u001b[0;32m--> 428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\n\u001b[1;32m    429\u001b[0m     \u001b[39mself\u001b[39m, gradient\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, retain_graph\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, create_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, inputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    430\u001b[0m ):\n\u001b[1;32m    431\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Computes the gradient of current tensor w.r.t. graph leaves.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[39m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[39m            used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the score network\n",
    "\n",
    "if args.rff:\n",
    "    for step in range(epochs):\n",
    "\n",
    "            randind1 = torch.randint(0,10000,(128,))\n",
    "            randind2 = torch.randint(0,10000,(128,))\n",
    "            samples1 = training_samples[randind1,:]\n",
    "            samples2 = training_samples[randind2,:]\n",
    "\n",
    "            # evaluate loss function and gradient\n",
    "            # loss = time_dsm_score_estimator(scorenet,samples1,0,tint,0.001) + time_dsm_score_estimator(scorenet,samples2,tint,T,0.001)\n",
    "            loss = FF_time_dsm_score_estimator(scorenet,samples1,0,tint,args.dt/10,B,c) + FF_time_dsm_score_estimator(scorenet,samples2,tint,T,args.dt/10,B,c)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update score network\n",
    "            optimizer.step()\n",
    "\n",
    "            if not step%100:\n",
    "                print(loss,step)\n",
    "\n",
    "else:\n",
    "    for step in range(epochs):\n",
    "\n",
    "        randind1 = torch.randint(0,10000,(128,))\n",
    "        randind2 = torch.randint(0,10000,(128,))\n",
    "        samples1 = training_samples[randind1,:]\n",
    "        samples2 = training_samples[randind2,:]\n",
    "\n",
    "        # evaluate loss function and gradient\n",
    "        loss = time_dsm_score_estimator(scorenet,samples1,0,tint,args.dt/10) + time_dsm_score_estimator(scorenet,samples2,tint,T,args.dt/10)\n",
    "        # loss = FF_time_dsm_score_estimator(scorenet,samples1,0,tint,0.001,B,c) + FF_time_dsm_score_estimator(scorenet,samples2,tint,T,0.001,B,c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update score network\n",
    "        optimizer.step()\n",
    "\n",
    "        if not step%100:\n",
    "            print(loss,step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SDE simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the solving the OU process exactly given deterministic initial conditions\n",
    "def ou_dynamics(init, T):\n",
    "    init = init * torch.exp(- 0.5 * T) + torch.sqrt(1-torch.exp(-T)) * torch.randn_like(init)\n",
    "    return init\n",
    "\n",
    "def reverse_sde(score, init,T,lr=args.dt):\n",
    "    step = int(T/lr) \n",
    "    for i in range(step,-1,-1):\n",
    "        current_lr = lr\n",
    "        evalpoint = torch.cat(((torch.tensor(lr*i)).repeat(init.shape[0],1),init),1)\n",
    "        init = init + current_lr  * (init/2 + score(evalpoint).detach() )\n",
    "        init = init + torch.randn_like(init) * np.sqrt(current_lr)\n",
    "    return init\n",
    "\n",
    "\n",
    "# The following is the deterministic ODE flow that can also sample from the target distribution\n",
    "\n",
    "def reverse_ode_flow(score,init,T,lr = args.dt):\n",
    "    step = int(T/lr)\n",
    "    for i in range(step,-1,-1):\n",
    "        current_lr = lr\n",
    "        evalpoint = torch.cat(((torch.tensor(lr*i)).repeat(init.shape[0],1),init),1)\n",
    "        init = init + current_lr  * (init/2 + 1/2 * score(evalpoint).detach() )\n",
    "    return init\n",
    "\n",
    "\n",
    "\n",
    "def FF_reverse_sde(score, init,T,B,c,lr=args.dt):\n",
    "    step = int(T/lr) \n",
    "    for i in range(step,-1,-1):\n",
    "        current_lr = lr\n",
    "\n",
    "\n",
    "        FFinit = torch.matmul(init,B) + c\n",
    "        # checkbounds = (torch.abs(init[:,0])<3).type(torch.uint8) * (torch.abs(init[:,1])<3)\n",
    "        # FFinit = torch.multiply( checkbounds.reshape(-1,1) , torch.cos(FFinit) )\n",
    "        FFinit = torch.cos(FFinit) \n",
    "        # FFinit = torch.cat( (torch.cos(FFinit), torch.sin(FFinit)),1 )\n",
    "        evalpoint = torch.cat(((torch.tensor(lr*i)).repeat(FFinit.shape[0],1),FFinit),1)\n",
    "\n",
    "\n",
    "        init = init + current_lr  * (init/2 +( -init + score(evalpoint).detach() ))\n",
    "        init = init + torch.randn_like(init) * np.sqrt(current_lr)\n",
    "    return init\n",
    "\n",
    "\n",
    "def FF_reverse_ode_flow(score,init,T,B,c,lr = args.dt):\n",
    "    step = int(T/lr)\n",
    "    for i in range(step,-1,-1):\n",
    "        current_lr = lr\n",
    "\n",
    "        FFinit = torch.matmul(init,B) + c\n",
    "        # checkbounds = (torch.abs(init[:,0])<3).type(torch.uint8) * (torch.abs(init[:,1])<3)\n",
    "        FFinit = torch.cos(FFinit)\n",
    "        # FFinit = torch.multiply( checkbounds.reshape(-1,1) , torch.cos(FFinit) )\n",
    "        evalpoint = torch.cat(((torch.tensor(lr*i)).repeat(FFinit.shape[0],1),FFinit),1)\n",
    "        init = init + current_lr  * (init/2 + 1/2 *( - init +  score(evalpoint).detach()) )\n",
    "    return init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample using the score network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.rff:\n",
    "\n",
    "    # Denoising the normal distribution \n",
    "    samples_lang = torch.randn(10000, 2) # * (right_bound - left_bound) + left_bound\n",
    "    # samples_lang = reverse_sde(scorenet, samples_lang,torch.tensor(T)).detach().numpy()\n",
    "    samples_lang = FF_reverse_sde(scorenet, samples_lang,torch.tensor(T),B,c).detach().numpy()\n",
    "\n",
    "    # Denoising samples from the training data\n",
    "    samples = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = 10000))\n",
    "    samples = model(samples.to(dtype = torch.float32),None,reverse = False)\n",
    "    samples_lang_noisedtraining = samples * torch.exp(-0.5 * torch.tensor(T)) + torch.sqrt(1-torch.exp(-torch.tensor(T))) * torch.randn_like(samples)\n",
    "    # samples_lang_noisedtraining =reverse_sde(scorenet, samples_lang_noisedtraining.to(dtype=torch.float32),torch.tensor(T)).detach().numpy()\n",
    "    samples_lang_noisedtraining =FF_reverse_sde(scorenet, samples_lang_noisedtraining.to(dtype=torch.float32),torch.tensor(T),B,c).detach().numpy()\n",
    "\n",
    "    # Deterministically evolving the normal distribution \n",
    "    samples_deterministic = torch.randn(10000,2)\n",
    "    # samples_deterministic = reverse_ode_flow(scorenet,samples_deterministic,torch.tensor(T)).detach().numpy()\n",
    "    samples_deterministic = FF_reverse_ode_flow(scorenet,samples_deterministic,torch.tensor(T),B,c).detach().numpy()\n",
    "\n",
    "else:\n",
    "        # Denoising the normal distribution \n",
    "    samples_lang = torch.randn(10000, 2) # * (right_bound - left_bound) + left_bound\n",
    "    samples_lang = reverse_sde(scorenet, samples_lang,torch.tensor(T)).detach().numpy()\n",
    "\n",
    "    # Denoising samples from the training data\n",
    "    samples = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = 10000))\n",
    "    samples = model(samples.to(dtype = torch.float32),None,reverse = False)\n",
    "    samples_lang_noisedtraining = samples * torch.exp(-0.5 * torch.tensor(T)) + torch.sqrt(1-torch.exp(-torch.tensor(T))) * torch.randn_like(samples)\n",
    "    samples_lang_noisedtraining =reverse_sde(scorenet, samples_lang_noisedtraining.to(dtype=torch.float32),torch.tensor(T)).detach().numpy()\n",
    "\n",
    "    # Deterministically evolving the normal distribution \n",
    "    samples_deterministic = torch.randn(10000,2)\n",
    "    samples_deterministic = reverse_ode_flow(scorenet,samples_deterministic,torch.tensor(T)).detach().numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postprocessing with the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_lang_target = model(torch.tensor(samples_lang),None,reverse = True).detach().numpy()\n",
    "samples_lang_noisedtraining_target = model(torch.tensor(samples_lang_noisedtraining),None,reverse = True).detach().numpy()\n",
    "samples_deterministic_target = model(torch.tensor(samples_deterministic),None,reverse = True).detach().numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make and save plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.rff:\n",
    "    savedir = saveloc + '/' + dataset +'_dims' + str(args.dims) + '_scoredepth' + str(args.depth)+ '_T' + str(args.finalT) +'_FF/'\n",
    "else:\n",
    "    savedir = saveloc + '/' + dataset +'_dims' + str(args.dims) + '_scoredepth' + str(args.depth)+ '_T' + str(args.finalT) + '/'\n",
    "\n",
    "# Check whether the specified path exists or not\n",
    "isExist = os.path.exists(savedir)\n",
    "if not isExist:\n",
    "\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(savedir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "p_samples = toy_data.inf_train_gen(dataset, batch_size = 10000)\n",
    "samples_true = torch.tensor(p_samples).to(dtype = torch.float32)\n",
    "plt.scatter(samples_true[:,0],samples_true[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('True samples')\n",
    "\n",
    "\n",
    "savename = savedir + 'true_samples.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "pushforward_samples_true = model(samples_true,None,reverse = False).detach().numpy()\n",
    "plt.scatter(pushforward_samples_true[:,0],pushforward_samples_true[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('True pushforward samples')\n",
    "\n",
    "savename = savedir + 'true_pushforward.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "# plt.clf()\n",
    "# pushforward_samples_true = model(samples_true,None,reverse = False).detach().numpy()\n",
    "# pushforward_samples_atT = ou_dynamics(torch.tensor(pushforward_samples_true),torch.tensor(T)).detach().numpy()\n",
    "# plt.scatter(pushforward_samples_atT[:,0],pushforward_samples_atT[:,1],s = 0.1)\n",
    "# plt.axis('square')\n",
    "# plt.title('True pushforward samples at T')\n",
    "\n",
    "# savename = savedir + 'true_pushforward_atT.png'\n",
    "# plt.savefig(savename)\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "fig,axs = plt.subplots(1,6)\n",
    "samples_nf = model(samples_true,None,reverse = False)\n",
    "\n",
    "for jj in range(6):\n",
    "\n",
    "    samples_nf_out = ou_dynamics(samples_nf,torch.tensor(T/6*jj)).detach().numpy()\n",
    "    axs[jj].set_box_aspect(1)\n",
    "    axs[jj].set_xlim([-6, 6])\n",
    "    axs[jj].set_ylim([-6, 6])\n",
    "    axs[jj].scatter(samples_nf_out[:,0],samples_nf_out[:,1],s = 0.1)\n",
    "\n",
    "# plt.title('True pushforward samples at T')\n",
    "savename = savedir + 'true_pushforward_atT.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(samples_lang[:,0],samples_lang[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('Reverse SDE pushforward')\n",
    "savename = savedir + 'reversesde_pushforward.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(samples_lang_target[:,0],samples_lang_target[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('Denoising normal distribution')\n",
    "savename = savedir + 'reversesde_target.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(samples_lang_noisedtraining_target[:,0],samples_lang_noisedtraining_target[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('Denoising noised training samples')\n",
    "savename = savedir + 'reversesde_noisedtraining_target.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(samples_deterministic_target[:,0],samples_deterministic_target[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('Denoising normal with ODE flow')\n",
    "savename = savedir + 'deterministic_ode_target.png'\n",
    "plt.savefig(savename)\n",
    "\n",
    "\n",
    "\n",
    "samples_nf = model(torch.randn(10000,2),None,reverse = True).detach().numpy()\n",
    "plt.clf()\n",
    "plt.scatter(samples_nf[:,0],samples_nf[:,1],s = 0.1)\n",
    "plt.axis('square')\n",
    "plt.title('Map only')\n",
    "savename = savedir + 'map_only.png'\n",
    "plt.savefig(savename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(scorenet,savename + 'scorenet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "particlesystems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcff6d82c76e93f8bf1a2a5a5959262afe2e0fbcc55797eb73e33c922868f9ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
